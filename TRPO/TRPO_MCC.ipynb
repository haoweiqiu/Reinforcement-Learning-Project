{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "gamma = 0.99\n",
    "goal_score = 0\n",
    "log_interval = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_kl = 0.01\n",
    "\n",
    "def flat_grad(grads):\n",
    "    grad_flatten = []\n",
    "    for grad in grads:\n",
    "        grad_flatten.append(grad.view(-1))\n",
    "    grad_flatten = torch.cat(grad_flatten)\n",
    "    return grad_flatten\n",
    "\n",
    "def flat_hessian(hessians):\n",
    "    hessians_flatten = []\n",
    "    for hessian in hessians:\n",
    "        hessians_flatten.append(hessian.contiguous().view(-1))\n",
    "    hessians_flatten = torch.cat(hessians_flatten).data\n",
    "    return hessians_flatten\n",
    "\n",
    "def flat_params(model):\n",
    "    params = []\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "    params_flatten = torch.cat(params)\n",
    "    return params_flatten\n",
    "\n",
    "def update_model(model, new_params):\n",
    "    index = 0\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    for params in model.parameters():\n",
    "        params_length = len(params.view(-1))\n",
    "        new_param = new_params[index: index + params_length]\n",
    "        new_param = new_param.view(params.size())\n",
    "        params.data.copy_(new_param)\n",
    "        index += params_length\n",
    "\n",
    "def kl_divergence(policy, old_policy):\n",
    "    kl = old_policy * torch.log(old_policy / policy)\n",
    "\n",
    "    kl = kl.sum(1, keepdim=True)\n",
    "    return kl\n",
    "\n",
    "def fisher_vector_product(net, states, p, cg_damp=0.1):\n",
    "    policy = net(states)\n",
    "    old_policy = net(states).detach()\n",
    "    kl = kl_divergence(policy, old_policy)\n",
    "    kl = kl.mean()\n",
    "    kl_grad = torch.autograd.grad(kl, net.parameters(), create_graph=True) # create_graph is True if we need higher order derivative products\n",
    "    kl_grad = flat_grad(kl_grad)\n",
    "\n",
    "    kl_grad_p = (kl_grad * p.detach()).sum()\n",
    "    kl_hessian_p = torch.autograd.grad(kl_grad_p, net.parameters())\n",
    "    kl_hessian_p = flat_hessian(kl_hessian_p)\n",
    "\n",
    "    return kl_hessian_p + cg_damp * p.detach()\n",
    "\n",
    "\n",
    "def conjugate_gradient(net, states, loss_grad, n_step=10, residual_tol=1e-10):\n",
    "    x = torch.zeros(loss_grad.size())\n",
    "    r = loss_grad.clone()\n",
    "    p = loss_grad.clone()\n",
    "    r_dot_r = torch.dot(r, r)\n",
    "\n",
    "    for i in range(n_step):\n",
    "        A_dot_p = fisher_vector_product(net, states, p)\n",
    "        alpha = r_dot_r / torch.dot(p, A_dot_p)\n",
    "        x += alpha * p\n",
    "        r -= alpha * A_dot_p\n",
    "        new_r_dot_r = torch.dot(r,r)\n",
    "        betta = new_r_dot_r / r_dot_r\n",
    "        p = r + betta * p\n",
    "        r_dot_r = new_r_dot_r\n",
    "        if r_dot_r < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "class TRPO(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(TRPO, self).__init__()\n",
    "        self.t = 0\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.fc_1 = nn.Linear(num_inputs, 128)\n",
    "        self.fc_2 = nn.Linear(128, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        x = torch.relu(self.fc_1(input))\n",
    "        logits = self.fc_2(x) / 0.1\n",
    "        logits = logits  - logits.max(dim=-1, keepdim=True).values\n",
    "        policy = F.softmax(logits  , dim=-1)\n",
    "        return policy\n",
    "\n",
    "    @classmethod\n",
    "    def train_model(cls, net, transitions):\n",
    "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        masks = torch.Tensor(masks)\n",
    "\n",
    "        returns = torch.zeros_like(rewards)\n",
    "\n",
    "        running_return = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
    "            returns[t] = running_return\n",
    "\n",
    "        policy = net(states)\n",
    "        policy = policy.view(-1, net.num_outputs)\n",
    "        policy_action = (policy * actions.detach()).sum(dim=1)\n",
    "\n",
    "        old_policy = net(states).detach()\n",
    "        old_policy = old_policy.view(-1, net.num_outputs)\n",
    "        old_policy_action = (old_policy * actions.detach()).sum(dim=1)\n",
    "\n",
    "        surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
    "\n",
    "        surrogate_loss_grad = torch.autograd.grad(surrogate_loss, net.parameters())\n",
    "        surrogate_loss_grad = flat_grad(surrogate_loss_grad)\n",
    "\n",
    "        step_dir = conjugate_gradient(net, states, surrogate_loss_grad.data)\n",
    "\n",
    "        params = flat_params(net)\n",
    "        shs = (step_dir * fisher_vector_product(net, states, step_dir)).sum(0, keepdim=True)\n",
    "        step_size = torch.sqrt((2 * max_kl) / shs)[0]\n",
    "        #print('step_size', step_size)\n",
    "        full_step = step_size * step_dir\n",
    "\n",
    "        fraction = 1.0\n",
    "        for _ in range(10):\n",
    "            new_params = params + fraction * full_step\n",
    "            update_model(net, new_params)\n",
    "            policy = net(states)\n",
    "            policy = policy.view(-1, net.num_outputs)\n",
    "            policy_action = (policy * actions.detach()).sum(dim=1)\n",
    "            entropy = -(policy * policy.log()).sum(1, keepdim=True).mean()\n",
    "            beta = .9\n",
    "            surrogate_loss = ((policy_action / old_policy_action) * returns).mean() - beta * entropy\n",
    "\n",
    "            kl = kl_divergence(policy, old_policy)\n",
    "            kl = kl.mean()\n",
    "\n",
    "            if kl < max_kl:\n",
    "                break\n",
    "            fraction = fraction * 0.5\n",
    "\n",
    "        return -surrogate_loss\n",
    "\n",
    "    def get_action(self, input):\n",
    "        policy = self.forward(input)\n",
    "        policy = policy[0].data.numpy()\n",
    "        #print(policy)\n",
    "        # Check if policy contains NaN values\n",
    "        if np.isnan(policy).any():\n",
    "          print(\"Policy contains NaN values:\", policy)\n",
    "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
    "        #print(action)\n",
    "        return action\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = deque()\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
    "\n",
    "    def sample(self):\n",
    "        memory = self.memory\n",
    "        return Transition(*zip(*memory))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "def main():\n",
    "    env = gym.make('MountainCar-v0')  # Change the environment here\n",
    "    env._max_episode_steps = 2000\n",
    "    max_step = env._max_episode_steps\n",
    "    ep = 1000\n",
    "    print(env._max_episode_steps)\n",
    "    #env.seed(500)\n",
    "    torch.manual_seed(500)\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n  # Update the number of actions\n",
    "    print('state size:', num_inputs)\n",
    "    print('action size:', num_actions)\n",
    "\n",
    "    net = TRPO(num_inputs, num_actions)\n",
    "\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Create lists to hold scores and episodes\n",
    "    scores = []\n",
    "    episodes = []\n",
    "\n",
    "    for e in range(ep):\n",
    "        done = False\n",
    "        memory = Memory()\n",
    "        running_score = 0\n",
    "        steps = 0\n",
    "        score = 0\n",
    "        state, _ = env.reset()\n",
    "        state = torch.Tensor(state).to(device)\n",
    "        state = state.unsqueeze(0)\n",
    "\n",
    "        while not done and steps < max_step:\n",
    "            steps += 1\n",
    "\n",
    "            action = net.get_action(state)\n",
    "            next_state, reward, done, info, _ = env.step(action)  # No need to wrap action in a list\n",
    "            #print(reward, steps)\n",
    "\n",
    "            next_state = torch.Tensor(next_state)\n",
    "            next_state = next_state.unsqueeze(0)\n",
    "\n",
    "            mask = 0 if done else 1\n",
    "            reward = reward if not done or score == 499 else -1\n",
    "\n",
    "            action_one_hot = torch.zeros(num_actions)  # Update the size of action_one_hot\n",
    "            action_one_hot[action] = 1\n",
    "            memory.push(state, next_state, action_one_hot, reward, mask)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "        loss = TRPO.train_model(net, memory.sample())\n",
    "\n",
    "        score = score #if score == 500.0 else score + 1\n",
    "        running_score = score #0.99 * running_score + 0.01 * score\n",
    "\n",
    "        # Append score and episode number to lists\n",
    "        scores.append(score)\n",
    "        episodes.append(e)\n",
    "\n",
    "        if e % log_interval == 0:\n",
    "            print('{} episode | score: {:.2f}'.format(e, running_score))\n",
    "\n",
    "        if running_score > goal_score:\n",
    "            break\n",
    "\n",
    "    # After all episodes, plot the scores\n",
    "    plt.plot(episodes, scores)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
