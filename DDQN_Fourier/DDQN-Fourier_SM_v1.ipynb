{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('Swimmer-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import deque\n",
    "import random\n",
    "import itertools\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_rewards_data(rewards_data, filename='rewards_data.pkl'):\n",
    "    \"\"\"Save the rewards data to a pickle file.\"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(rewards_data, file)\n",
    "    print(f'Rewards data saved to {filename}')\n",
    "\n",
    "\n",
    "def load_rewards_data(filename='rewards_data.pkl'):\n",
    "    \"\"\"Load the rewards data from a pickle file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            rewards_data = pickle.load(file)\n",
    "        print(f'Rewards data loaded from {filename}')\n",
    "        return rewards_data\n",
    "    except FileNotFoundError:\n",
    "        print(f'Error: The file {filename} does not exist. Please check the file path and try again.')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred while loading the data: {e}')\n",
    "        return None\n",
    "\n",
    "class Fourier_Basis:\n",
    "  def __init__(self, order, k):\n",
    "    #self.env = env\n",
    "    self.order = [order]*k\n",
    "    self.coefficients = np.array([])\n",
    "\n",
    "  def get_coefficients(self):\n",
    "    prods = [range(0, i+1) for i in self.order]\n",
    "    #print(prods)\n",
    "    coeffs = [v for v in itertools.product(*prods)]\n",
    "    self.coefficients = np.array(coeffs)\n",
    "    return self.coefficients\n",
    "  \n",
    "  def value(self, state):\n",
    "    self.get_coefficients()\n",
    "    return np.cos(np.pi*np.dot(self.coefficients, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_features, output_values):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_features, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32, out_features=output_values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = functional.selu(self.fc1(x))\n",
    "        x = functional.selu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DDQN:\n",
    "    def __init__(self, env, order, discretizer, steps_per_dim, use_cuda=True, learning_rate=1e-5, gamma=0.99, memory_len=10000, start_epsilon=1):\n",
    "        self.device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "        self.env = env\n",
    "        self.discrete_actions = discretize_action(env)\n",
    "        self.discretizer = discretizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.u_state = env.observation_space.high\n",
    "        self.l_state = env.observation_space.low\n",
    "        self.d_state = self.u_state - self.l_state\n",
    "        self.n_features = len(env.observation_space.high)\n",
    "        self.steps_per_dimension = steps_per_dim\n",
    "        self.action_list = self.discrete_actions.discretize_action_space(self.steps_per_dimension)\n",
    "        print(self.action_list)\n",
    "\n",
    "        self.epsilon = start_epsilon\n",
    "        self.min_epsilon = 0.01\n",
    "        self.epsilon_decay = 0.09 / 5e3\n",
    "        self.memory_len = memory_len\n",
    "        self.memory = deque(maxlen=self.memory_len)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        input_features = (order + 1)**env.observation_space.shape[0]\n",
    "        output_values = len(self.action_list)\n",
    "        self.policy_net = Model(input_features, output_values).to(self.device)\n",
    "        self.target_net = Model(input_features, output_values).to(self.device)\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "    def get_states_tensor(self, sample, states_idx):\n",
    "        sample_len = len(sample)\n",
    "        states_tensor = torch.empty((sample_len, self.n_features), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "        features_range = range(self.n_features)\n",
    "        for i in range(sample_len):\n",
    "            for j in features_range:\n",
    "                states_tensor[i, j] = sample[i][states_idx][j].item()\n",
    "\n",
    "        return states_tensor\n",
    "\n",
    "    def normalize_state(self, state):\n",
    "        if type(state) == tuple:\n",
    "            state = state[0]\n",
    "        state[0] /= 1.8\n",
    "        state[1] /= 0.14\n",
    "        \n",
    "\n",
    "    def state_reward(self, state, env_reward):\n",
    "        return env_reward - (abs(state[0]) + abs(state[1])) / 2.5\n",
    "\n",
    "    def get_action(self, state, e=0.001):\n",
    "        if random.random() < e:\n",
    "            return np.random.randint(len(self.action_list))\n",
    "        else:\n",
    "            encoded_state = (state - self.l_state) / self.d_state\n",
    "            state_cos = self.discretizer.value(encoded_state)\n",
    "            encoded_state = torch.tensor(state_cos, dtype=torch.float32, device=self.device)\n",
    "            return self.policy_net(encoded_state).argmax().item()\n",
    "\n",
    "    def fit(self, model, inputs, labels):\n",
    "        inputs = inputs.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        train_ds = TensorDataset(inputs, labels)\n",
    "        train_dl = DataLoader(train_ds, batch_size=5)\n",
    "\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=self.learning_rate)\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y in train_dl:\n",
    "            out = model(x)\n",
    "            loss = self.criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        return total_loss / len(train_dl)\n",
    "\n",
    "    def optimize_model(self, train_batch_size=100):\n",
    "        if len(self.memory) < train_batch_size:\n",
    "            return\n",
    "        train_sample = random.sample(self.memory, train_batch_size)\n",
    "\n",
    "        state = torch.tensor([s[0] for s in train_sample], dtype=torch.float32, device=device)\n",
    "        next_state = torch.tensor([s[3] for s in train_sample], dtype=torch.float32, device=device)\n",
    "\n",
    "        q_estimates = self.policy_net(state).detach()\n",
    "        next_state_q_estimates = self.target_net(next_state).detach()\n",
    "        next_actions = self.policy_net(next_state).argmax(dim=1)\n",
    "\n",
    "        for i in range(len(train_sample)):\n",
    "            next_action = next_actions[i].item()\n",
    "            q_estimates[i][train_sample[i][1]] = (train_sample[i][2] +\n",
    "                                                  self.gamma * next_state_q_estimates[i][next_action].item())\n",
    "\n",
    "        self.fit(self.policy_net, state, q_estimates)\n",
    "\n",
    "    def train_one_episode(self):\n",
    "        current_state = self.env.reset()\n",
    "        #self.normalize_state(current_state)\n",
    "        done = False\n",
    "        score = 0\n",
    "        reward = 0\n",
    "        step = 0\n",
    "        while not done and step < 999:\n",
    "            if type(current_state) == tuple:\n",
    "                current_state = current_state[0]\n",
    "            action = self.get_action(current_state, self.epsilon)\n",
    "            #print(self.action_list[action])\n",
    "            next_state, env_reward, done, info,  _ = self.env.step(self.action_list[action])\n",
    "            #self.normalize_state(next_state)\n",
    "            encoded_current_state = self.discretizer.value((current_state - self.l_state) / self.d_state)\n",
    "            encoded_next_state = self.discretizer.value((next_state - self.l_state) / self.d_state)\n",
    "            self.memory.append((encoded_current_state, action, env_reward, encoded_next_state))\n",
    "            current_state = next_state\n",
    "            score += env_reward\n",
    "            reward += self.state_reward(next_state, env_reward)\n",
    "\n",
    "            self.optimize_model(100)\n",
    "            step += 1\n",
    "\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "        return score, reward\n",
    "\n",
    "    def test(self):\n",
    "        state = self.env.reset()\n",
    "        #self.normalize_state(state)\n",
    "        done = False\n",
    "        score = 0\n",
    "        reward = 0\n",
    "        step = 0\n",
    "        while not done and step < 999:\n",
    "            if type(state) == tuple:\n",
    "                state = state[0]\n",
    "            action = self.get_action(state, 0)  # Using 0 to bypass random action selection\n",
    "            state, env_reward, done, info,  _ = self.env.step(self.action_list[action])\n",
    "            #self.normalize_state(state)\n",
    "            score += env_reward\n",
    "            reward += self.state_reward(state, env_reward)\n",
    "            step += 1\n",
    "\n",
    "        return score, reward\n",
    "\n",
    "class discretize_action:\n",
    "    def __init__(self, env) -> None:\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def discretize_action_space(self, steps_per_dimension):\n",
    "        \"\"\"\n",
    "        Discretizes the action space.\n",
    "\n",
    "        Parameters:\n",
    "        - low: The lower bound of the action space.\n",
    "        - high: The higher bound of the action space.\n",
    "        - steps_per_dimension: Number of discrete steps per dimension.\n",
    "\n",
    "        Returns:\n",
    "        - A list of discretized actions.\n",
    "        \"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        # Generate a range of values for each dimension\n",
    "        ranges = [np.linspace(low[i], high[i], steps_per_dimension) for i in range(len(low))]\n",
    "        \n",
    "        # Create a meshgrid of all possible combinations\n",
    "        mesh = np.meshgrid(*ranges)\n",
    "        \n",
    "        # Reshape the meshgrid to create a list of actions\n",
    "        self.actions = np.vstack([m.flatten() for m in mesh]).T\n",
    "        \n",
    "        return self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Swimmer-v4\")\n",
    "obs = env.reset()\n",
    "\n",
    "order = 5\n",
    "k = env.observation_space.shape[0]\n",
    "FB = Fourier_Basis(order, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(used_alpha, used_epsilon, env, order, discretizer, steps_per_dim, target_update_delay, test_delay, memory_len=10000, num_runs=1, num_episode=1000, save_file=\"\"):\n",
    "\n",
    "\n",
    "    # Initialize storage for reward data\n",
    "    rewards_data = {epsilon: {alpha: [] for alpha in used_alpha} for epsilon in used_epsilon}\n",
    "\n",
    "    for epsilon in used_epsilon:\n",
    "        for alpha in used_alpha:\n",
    "            print(f\"Training with epsilon: {epsilon}, alpha: {alpha}\")\n",
    "            run_rewards = []  # Collect rewards for each run for the current config\n",
    "\n",
    "            for run in range(num_runs):\n",
    "                \n",
    "                # avoid decay\n",
    "                current_epsilon = epsilon\n",
    "\n",
    "                print(f\"Run {run + 1}/{num_runs}\")\n",
    "                \n",
    "                # renew the wrapper class\n",
    "                ddqn = DDQN(\n",
    "                    env=env, \n",
    "                    order = order,\n",
    "                    discretizer=discretizer, \n",
    "                    steps_per_dim= steps_per_dim,\n",
    "                    use_cuda=True, \n",
    "                    learning_rate=alpha, \n",
    "                    gamma=0.7,\n",
    "                    memory_len=memory_len, \n",
    "                    start_epsilon=current_epsilon\n",
    "                )\n",
    "\n",
    "                best_test_reward = 0\n",
    "\n",
    "                episode_rewards = []  # Collect rewards for each episode in the current run\n",
    "                for i in range(num_episode):\n",
    "                    score, reward = ddqn.train_one_episode()\n",
    "\n",
    "                    print(f'Episode {i + 1}: score: {score} - reward: {reward}')\n",
    "\n",
    "                    if i % target_update_delay == 0:\n",
    "                        ddqn.target_net.load_state_dict(ddqn.policy_net.state_dict())\n",
    "                        ddqn.target_net.eval()\n",
    "\n",
    "                    if (i + 1) % test_delay == 0:\n",
    "                        test_score, test_reward = ddqn.test()\n",
    "                        print(f'Test Episode {i + 1}: test score: {test_score} - test reward: {test_reward}')\n",
    "                        if test_reward > best_test_reward:\n",
    "                            print('New best test reward. Saving model')\n",
    "                            best_test_reward = test_reward\n",
    "                            # torch.save(ddqn.policy_net.state_dict(), f'policy_net_{run}.pth')\n",
    "\n",
    "                    # Add the rewards\n",
    "                    episode_rewards.append(score)\n",
    "\n",
    "                if num_episode % test_delay != 0:\n",
    "                    test_score, test_reward = ddqn.test()\n",
    "                    print(f'Test Episode {num_episode}: test score: {test_score} - test reward: {test_reward}')\n",
    "                    if test_reward > best_test_reward:\n",
    "                        print('New best test reward. Saving model')\n",
    "                        best_test_reward = test_reward\n",
    "                        # torch.save(ddqn.policy_net.state_dict(), f'policy_net_{run}.pth')\n",
    "\n",
    "                # save the run rewards\n",
    "                run_rewards.append(episode_rewards)\n",
    "\n",
    "                print(f'best test reward: {best_test_reward}')\n",
    "\n",
    "            # Store the rewards for the current epsilon and alpha configuration\n",
    "            rewards_data[epsilon][alpha] = run_rewards\n",
    "\n",
    "    if not(save_file == \"\"):\n",
    "        save_rewards_data(rewards_data, save_file)\n",
    "\n",
    "    return rewards_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_data = main(\n",
    "    used_alpha=[1e-9], \n",
    "    used_epsilon=[0.9],  # start epsilon\n",
    "    env=env, \n",
    "    order = order,\n",
    "    discretizer=FB,\n",
    "    steps_per_dim= 7,\n",
    "    target_update_delay=2, \n",
    "    test_delay=10,\n",
    "    memory_len=10000, \n",
    "    num_runs=1, \n",
    "    num_episode=500, \n",
    "    save_file=\"DDQN_Fourier_SM.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards_data, algorithm_name=\"Q learning\"):\n",
    "    \"\"\"Plot the average performance and IQR of the policy.\"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    for epsilon, alphas in rewards_data.items():\n",
    "        for alpha, data in alphas.items():\n",
    "            # Calculate average and IQR across runs for each episode\n",
    "            data = np.array(data)  # Convert to numpy array for easier slicing\n",
    "            avg_rewards = data[0] #np.mean(data, axis=0)\n",
    "            #lower_quartile = np.percentile(data, 25, axis=0)\n",
    "            #upper_quartile = np.percentile(data, 75, axis=0)\n",
    "\n",
    "            episodes = np.arange(len(avg_rewards)) + 1\n",
    "            plt.plot(episodes, avg_rewards, label=f'ε={epsilon}, α={alpha}')\n",
    "            #plt.fill_between(episodes, lower_quartile, upper_quartile, alpha=0.25)\n",
    "\n",
    "    plt.title(f'Average Performance and IQR by Epsilon and Alpha ({algorithm_name})')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(rewards_data, algorithm_name=\"DDQN (Fourier Swimmer)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
