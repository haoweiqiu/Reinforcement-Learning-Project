{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from numpy.random import choice\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils.convert_parameters import parameters_to_vector\n",
    "from torch.nn.utils.convert_parameters import vector_to_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TRPOAgent:\n",
    "    \"\"\"Continuous TRPO agent.\"\"\"\n",
    "\n",
    "    def __init__(self, policy, discount=0.98, kl_delta=0.01, cg_iteration=10,\n",
    "                 cg_dampening=0.001, cg_tolerance=1e-10, cg_state_percent=0.1):\n",
    "        self.policy = policy\n",
    "        self.discount = discount\n",
    "        self.kl_delta = kl_delta\n",
    "        self.cg_iteration = cg_iteration\n",
    "        self.cg_dampening = cg_dampening\n",
    "        self.cg_tolerance = cg_tolerance\n",
    "        self.cg_state_percent = cg_state_percent\n",
    "        self.distribution = torch.distributions.normal.Normal\n",
    "\n",
    "        # Cuda check\n",
    "        self.device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "                       else torch.device('cpu'))\n",
    "        policy.to(self.device)\n",
    "\n",
    "        # Infer action dimensions from policy\n",
    "        policy_modules = [module for module in policy.modules() if not\n",
    "                          isinstance(module, torch.nn.Sequential)]\n",
    "        action_dims = policy_modules[-1].out_features\n",
    "\n",
    "        # Set logstd\n",
    "        self.logstd = torch.ones(action_dims, requires_grad=True,\n",
    "                                 device=self.device)\n",
    "        with torch.no_grad():\n",
    "            self.logstd /= self.logstd.exp()\n",
    "\n",
    "        self.buffers = {'log_probs': [], 'actions': [],\n",
    "                        'completed_rewards': [], 'states': []}\n",
    "\n",
    "    def __call__(self, state):\n",
    "        \"\"\"\n",
    "        Peforms forward pass on the NN and parameterized distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : torch.Tensor\n",
    "            Tensor passed into NN and distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Action choice for each action dimension.\n",
    "        \"\"\"\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Parameterize distribution with policy, sample action\n",
    "        normal_dist = self.distribution(self.policy(state), self.logstd.exp())\n",
    "        action = normal_dist.sample()\n",
    "        # Save information\n",
    "        self.buffers['actions'].append(action)\n",
    "        self.buffers['log_probs'].append(normal_dist.log_prob(action))\n",
    "        self.buffers['states'].append(state)\n",
    "        return action.cpu().numpy()\n",
    "\n",
    "    def kl(self, new_policy, new_std, states, grad_new=True):\n",
    "        \"\"\"Compute KL divergence between current policy and new one.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_policy : TRPOAgent\n",
    "        new_std : torch.Tensor\n",
    "        states : torch.Tensor\n",
    "            States to compute KL divergence over.\n",
    "        grad_new : bool, optional\n",
    "            Enable gradient of new policy.\n",
    "        \"\"\"\n",
    "        mu1 = self.policy(states)\n",
    "        log_sigma1 = self.logstd\n",
    "        mu2 = new_policy(states)\n",
    "        log_sigma2 = new_std\n",
    "\n",
    "        # Detach other as gradient should only be w.r.t. to one\n",
    "        if grad_new:\n",
    "            mu1, log_sigma1 = mu1.detach(), log_sigma1.detach()\n",
    "        else:\n",
    "            mu2, log_sigma2 = mu2.detach(), log_sigma2.detach()\n",
    "\n",
    "        # Compute KL over all states\n",
    "        kl_matrix = ((log_sigma2 - log_sigma1) + 0.5 * (log_sigma1.exp().pow(2)\n",
    "                     + (mu1 - mu2).pow(2)) / log_sigma2.exp().pow(2) - 0.5)\n",
    "\n",
    "        # Sum over action dim, average over all states\n",
    "        return kl_matrix.sum(1).mean()\n",
    "\n",
    "    def surrogate_objective(self, new_policy, new_std, states, actions,\n",
    "                            log_probs, advantages):\n",
    "        new_dist = self.distribution(new_policy(states), new_std.exp())\n",
    "        new_prob = new_dist.log_prob(actions)\n",
    "\n",
    "        entropy_bonus = new_dist.entropy().mean()\n",
    "\n",
    "        # Detach old log_probs, only compute grad w.r.t. new policy\n",
    "        ratio = new_prob.exp() / log_probs.detach().exp()\n",
    "\n",
    "        return (ratio * advantages.view(-1, 1)).mean() + 0.01 * entropy_bonus\n",
    "\n",
    "    def line_search(self, gradients, states, actions, log_probs, rewards):\n",
    "        step_size = (2 * self.kl_delta / gradients.dot(\n",
    "            self.fisher_vector_direct(gradients, states))).sqrt()\n",
    "        step_size_decay = 1.5\n",
    "        line_search_attempts = 10\n",
    "\n",
    "        # New policy\n",
    "        current_parameters = parameters_to_vector(self.policy.parameters())\n",
    "        new_policy = deepcopy(self.policy)\n",
    "        vector_to_parameters(current_parameters + step_size * gradients,\n",
    "                             new_policy.parameters())\n",
    "        new_std = self.logstd.detach() + step_size * self.logstd.grad\n",
    "\n",
    "        #  Shrink gradient until KL constraint met and improvement\n",
    "        for attempt in range(line_search_attempts):\n",
    "            # Obtain kl divergence and objective\n",
    "            with torch.no_grad():\n",
    "                kl_value = self.kl(new_policy, new_std, states)\n",
    "                objective = self.surrogate_objective(new_policy, new_std,\n",
    "                                                     states, actions, log_probs,\n",
    "                                                     rewards)\n",
    "\n",
    "            # Shrink gradient if KL constraint not met or reward lower\n",
    "            if kl_value > self.kl_delta or objective < 0:\n",
    "                step_size /= step_size_decay\n",
    "                vector_to_parameters(current_parameters + step_size *\n",
    "                                     gradients, new_policy.parameters())\n",
    "                new_std = self.logstd.detach() + step_size * self.logstd.grad\n",
    "            #  Return new policy and std if KL and reward met\n",
    "            else:\n",
    "                return new_policy, new_std.requires_grad_()\n",
    "\n",
    "        # Return old policy and std if constraints never met\n",
    "        return self.policy, self.logstd\n",
    "\n",
    "    def fisher_vector_direct(self, vector, states):\n",
    "        \"\"\"Computes the fisher vector product through direct method.\n",
    "\n",
    "        The FVP can be determined by first taking the gradient of KL\n",
    "        divergence w.r.t. the parameters and the dot product of this\n",
    "        with the input vector, then a gradient over this again w.r.t.\n",
    "        the parameters.\n",
    "        \"\"\"\n",
    "        vector = vector.clone().requires_grad_()\n",
    "        # Gradient of KL w.r.t. network param\n",
    "        self.policy.zero_grad()\n",
    "        kl_divergence = self.kl(self.policy, self.logstd, states)\n",
    "        grad_kl = torch.autograd.grad(kl_divergence, self.policy.parameters(),\n",
    "                                      create_graph=True)\n",
    "        grad_kl = torch.cat([grad.view(-1) for grad in grad_kl])\n",
    "\n",
    "        # Gradient of the gradient vector dot product w.r.t. param\n",
    "        grad_vector_dot = grad_kl.dot(vector)\n",
    "        fisher_vector_product = torch.autograd.grad(grad_vector_dot,\n",
    "                                                    self.policy.parameters())\n",
    "        fisher_vector_product = torch.cat([out.view(-1) for out in\n",
    "                                           fisher_vector_product]).detach()\n",
    "\n",
    "        # Apply CG dampening and return fisher vector product\n",
    "        return fisher_vector_product + self.cg_dampening * vector.detach()\n",
    "\n",
    "    def conjugate_gradient(self, b, states):\n",
    "        \"\"\"\n",
    "        Solve Ax = b for A as FIM and b as initial gradient.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/ikostrikov/pytorch-trpo/blob/master/trpo.py\n",
    "\n",
    "        Slight modifications to original, all credit to original.\n",
    "        \"\"\"\n",
    "        p = b.clone()\n",
    "        r = b.clone().double()\n",
    "        x = torch.zeros(*p.shape, device=self.device).double()\n",
    "        rdotr = r.dot(r)\n",
    "        for _ in range(self.cg_iteration):\n",
    "            fvp = self.fisher_vector_direct(p, states).double()\n",
    "            v = rdotr / p.double().dot(fvp)\n",
    "            x += v * p.double()\n",
    "            r -= v * fvp\n",
    "            new_rdotr = r.dot(r)\n",
    "            mu = new_rdotr / rdotr\n",
    "            p = (r + mu * p.double()).float()\n",
    "            rdotr = new_rdotr\n",
    "            if rdotr < self.cg_tolerance:\n",
    "                break\n",
    "        return x.float()\n",
    "\n",
    "    def optimize(self):\n",
    "        # Return if no completed episodes\n",
    "        if len(self.buffers['completed_rewards']) == 0:\n",
    "            return\n",
    " \n",
    "        # Convert all buffers to tensors\n",
    "        num_batch_steps = len(self.buffers['completed_rewards'])\n",
    "        rewards = torch.tensor(self.buffers['completed_rewards'])\n",
    "        actions = torch.stack(self.buffers['actions'][:num_batch_steps])\n",
    "        states = torch.stack(self.buffers['states'][:num_batch_steps])\n",
    "        log_probs = torch.stack(self.buffers['log_probs'][:num_batch_steps])\n",
    "        rewards, actions, states, log_probs = (rewards.to(self.device),\n",
    "                                               actions.to(self.device), \n",
    "                                               states.to(self.device),\n",
    "                                               log_probs.to(self.device))\n",
    "\n",
    "        # Normalize rewards over episodes\n",
    "        rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "\n",
    "        # Compute regular gradient\n",
    "        self.surrogate_objective(self.policy, self.logstd, states, actions,\n",
    "                                 log_probs, rewards).backward()\n",
    "        gradients = parameters_to_vector(\n",
    "            [param.grad for param in self.policy.parameters()])\n",
    "\n",
    "        # Choose states for conjugate gradient with np.random.choice\n",
    "        number_of_states = int(self.cg_state_percent * num_batch_steps)\n",
    "        cg_states = states[choice(len(states), number_of_states, replace=False)]\n",
    "\n",
    "        # Compute search direction as A^(-1)g, A is FIM\n",
    "        gradients = self.conjugate_gradient(gradients, cg_states)\n",
    "        # Find new policy and std with line search\n",
    "        self.policy, self.logstd = self.line_search(gradients, states, actions,\n",
    "                                                    log_probs, rewards)\n",
    "\n",
    "        # Update buffers removing processed steps\n",
    "        for key, storage in self.buffers.items():\n",
    "            del storage[:num_batch_steps]\n",
    "\n",
    "    def train(self, env_name, seed=None, batch_size=12000, iterations=100,\n",
    "              max_episode_length=None, verbose=False):\n",
    "\n",
    "        # Initialize env\n",
    "        env = gym.make(env_name)\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            env.seed(seed)\n",
    "        if max_episode_length is None:\n",
    "            max_episode_length = 999\n",
    "        # Recording\n",
    "        recording = {'episode_reward': [[]],\n",
    "                     'episode_length': [0],\n",
    "                     'num_episodes_in_iteration': []}\n",
    "\n",
    "        # Begin training\n",
    "        observation = env.reset()\n",
    "        if type(observation) == tuple:\n",
    "            observation = observation[0]\n",
    "        for iteration in range(iterations):\n",
    "            # Set initial value to 0\n",
    "            recording['num_episodes_in_iteration'].append(0)\n",
    "\n",
    "            for step in range(batch_size):\n",
    "                # Take step with agent\n",
    "                #print()\n",
    "                observation, reward, done, info,  _ = env.step(self(observation))\n",
    "\n",
    "                # Recording, increment episode values\n",
    "                recording['episode_length'][-1] += 1\n",
    "                recording['episode_reward'][-1].append(reward)\n",
    "\n",
    "                # End of episode\n",
    "                if (done or\n",
    "                        recording['episode_length'][-1] >= max_episode_length):\n",
    "                    # Calculate discounted reward\n",
    "                    discounted_reward = recording['episode_reward'][-1].copy()\n",
    "                    for index in range(len(discounted_reward) - 2, -1, -1):\n",
    "                        discounted_reward[index] += self.discount * \\\n",
    "                            discounted_reward[index + 1]\n",
    "                    self.buffers['completed_rewards'].extend(discounted_reward)\n",
    "\n",
    "                    # Set final recording of episode reward to total\n",
    "                    recording['episode_reward'][-1] = \\\n",
    "                        sum(recording['episode_reward'][-1])\n",
    "                    # Recording\n",
    "                    recording['episode_length'].append(0)\n",
    "                    recording['episode_reward'].append([])\n",
    "                    recording['num_episodes_in_iteration'][-1] += 1\n",
    "\n",
    "                    # Reset environment\n",
    "                    observation = env.reset()\n",
    "                    if type(observation) == tuple:\n",
    "                        observation = observation[0]\n",
    "\n",
    "            # Print information if verbose\n",
    "            if verbose:\n",
    "                num_episode = recording['num_episodes_in_iteration'][-1]\n",
    "                avg = (round(sum(recording['episode_reward'][-num_episode:-1])\n",
    "                             / (num_episode - 1), 3))\n",
    "                print(f'Average Reward over Iteration {iteration}: {avg}')\n",
    "            # Optimize after batch\n",
    "            self.optimize()\n",
    "\n",
    "        # Return recording information\n",
    "        return recording\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy': self.policy.state_dict(),\n",
    "            'logstd': self.logstd\n",
    "        }, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy.load_state_dict(checkpoint['policy'])\n",
    "        self.logstd = checkpoint['logstd'].to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over Iteration 0: -3.753\n",
      "Average Reward over Iteration 1: 4.927\n",
      "Average Reward over Iteration 2: 13.541\n",
      "Average Reward over Iteration 3: 16.453\n",
      "Average Reward over Iteration 4: 18.682\n",
      "Average Reward over Iteration 5: 20.857\n",
      "Average Reward over Iteration 6: 20.421\n",
      "Average Reward over Iteration 7: 22.697\n",
      "Average Reward over Iteration 8: 22.53\n",
      "Average Reward over Iteration 9: 24.425\n",
      "Average Reward over Iteration 10: 24.402\n",
      "Average Reward over Iteration 11: 24.921\n",
      "Average Reward over Iteration 12: 22.718\n",
      "Average Reward over Iteration 13: 22.625\n",
      "Average Reward over Iteration 14: 24.08\n",
      "Average Reward over Iteration 15: 25.147\n",
      "Average Reward over Iteration 16: 25.045\n",
      "Average Reward over Iteration 17: 23.851\n",
      "Average Reward over Iteration 18: 21.489\n",
      "Average Reward over Iteration 19: 21.498\n",
      "Average Reward over Iteration 20: 22.388\n",
      "Average Reward over Iteration 21: 24.698\n",
      "Average Reward over Iteration 22: 25.712\n",
      "Average Reward over Iteration 23: 25.74\n",
      "Average Reward over Iteration 24: 25.034\n",
      "Average Reward over Iteration 25: 25.836\n",
      "Average Reward over Iteration 26: 26.15\n",
      "Average Reward over Iteration 27: 25.709\n",
      "Average Reward over Iteration 28: 25.885\n",
      "Average Reward over Iteration 29: 25.431\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    nn = torch.nn.Sequential(torch.nn.Linear(8, 32), torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(32, 2))\n",
    "    init_weights = (lambda param: torch.nn.init.xavier_normal_(param.weight) if\n",
    "                    isinstance(param, torch.nn.Linear) else None)\n",
    "    nn.apply(init_weights)\n",
    "\n",
    "    # Initialize the agent\n",
    "    agent = TRPOAgent(policy=nn)\n",
    "\n",
    "    policy_name = type(agent).__name__\n",
    "\n",
    "    # Train\n",
    "    records = agent.train('Swimmer-v4', batch_size=10000,\n",
    "                        iterations=30, max_episode_length=500, verbose=True)\n",
    "\n",
    "    rewards_per_iter = []\n",
    "    s = e = 0\n",
    "    for i in range(len(records['num_episodes_in_iteration'])):\n",
    "        num = records['num_episodes_in_iteration'][i]\n",
    "        e += records['num_episodes_in_iteration'][i]\n",
    "        avg = sum(records['episode_reward'][s:e]) / num\n",
    "        rewards_per_iter.append(avg)\n",
    "        s = e\n",
    "    reward_list.append(rewards_per_iter)\n",
    "    \n",
    "agent.save_model(f'{policy_name}.pth')\n",
    "pickle.dump(records, open(f'{policy_name}_records.pickle', 'wb'))\n",
    "#plot_info(records, policy_name)\n",
    "\n",
    "# Display\n",
    "'''\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode = \"human\")\n",
    "ob = env.reset()\n",
    "if type(ob) == tuple:\n",
    "    ob = ob[0]\n",
    "for i in range(2):\n",
    "    action = agent(ob)\n",
    "    ob, _, done, info, _ = env.step(action)\n",
    "    #env.render()\n",
    "    if done:\n",
    "        ob = env.reset()\n",
    "        if type(ob) == tuple:\n",
    "            ob = ob[0]\n",
    "'''\n",
    "\n",
    "def plot_info(records, policy_name):\n",
    "    rewards_per_iter = []\n",
    "    s = e = 0\n",
    "    for i in range(5, len(records['num_episodes_in_iteration'])):\n",
    "        num = records['num_episodes_in_iteration'][i]\n",
    "        e += records['num_episodes_in_iteration'][i]\n",
    "        avg = sum(records['episode_reward'][s:e]) / num\n",
    "        rewards_per_iter.append(avg)\n",
    "        s = e\n",
    "    print(max(rewards_per_iter))\n",
    "    max_iter = max(enumerate(rewards_per_iter), key=lambda t: t[1])[0]\n",
    "    plt.plot(list(range(5, len(rewards_per_iter) + 5)), rewards_per_iter)\n",
    "    plt.text(min(max_iter, len(rewards_per_iter) - 10),\n",
    "             rewards_per_iter[max_iter],\n",
    "             \"Max Reward: \" + str(round(rewards_per_iter[max_iter], 2)))\n",
    "\n",
    "    plt.title(f\"{policy_name} reward per iter on MountainCarContinuous-v0\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
