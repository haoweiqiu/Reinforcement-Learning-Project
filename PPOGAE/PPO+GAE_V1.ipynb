{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "def layer_init(layer, scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "    \"\"\"Fully Connected Model.\"\"\"\n",
    "    def __init__(self, state_size, seed, hidden_layers, use_reset, act_fnc=F.relu):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            seed (int): Random seed\n",
    "            hidden_layers (list): Size of hidden_layers\n",
    "            act_fnc : Activation function\n",
    "            use_reset (bool): Weights initialization\n",
    "        \"\"\"\n",
    "        super(FCNet, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        dims = [state_size, ] + hidden_layers\n",
    "        if use_reset:\n",
    "            self.layers = nn.ModuleList([layer_init(nn.Linear(in_put, out_put)) for in_put, out_put in zip(dims[:-1], dims[1:])])\n",
    "        else:\n",
    "            self.layers = nn.ModuleList([nn.Linear(in_put, out_put) for in_put, out_put in zip(dims[:-1], dims[1:])])\n",
    "        self.act_fuc = act_fnc\n",
    "        self.feature_dim = dims[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.act_fuc(layer(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Convolution Model for raw pixels.\"\"\"\n",
    "    def __init__(self, state_size, feature_dim, seed, use_reset=True, input_channel=4):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            feature_dim (int): Feature dimension\n",
    "            seed (int): Random seed\n",
    "            use_reset (bool): Weights initialization\n",
    "            input_channel (int): Channel number of input\n",
    "        \"\"\"\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.size1 = ((state_size-6)//2+1)  # 80x80x4 to 38x38x4\n",
    "        self.size2 = ((self.size1-6)//4+1)  # 38x38x4 to 9x9x16\n",
    "        self.size = self.size2**2*16\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        if use_reset:\n",
    "            self.conv1 = layer_init(nn.Conv2d(input_channel, 4, kernel_size=6, stride=2))\n",
    "            self.conv2 = layer_init(nn.Conv2d(4, 16, kernel_size=6, stride=4))\n",
    "            self.fc3 = layer_init(nn.Linear(self.size, feature_dim))\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(input_channel, 4, kernel_size=6, stride=2)\n",
    "            self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "            self.fc3 = nn.Linear(self.size, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"Actor Critic Model (shared weights)\"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, main_net):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            main_net (model): Common net for actor and critic\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.main_net = main_net\n",
    "\n",
    "        self.fc_actor = layer_init(nn.Linear(main_net.feature_dim, action_size), 1e-3)\n",
    "        self.fc_critic = layer_init(nn.Linear(main_net.feature_dim, 1), 1e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.main_net(state)\n",
    "        pi_a = self.fc_actor(x)\n",
    "        prob = F.softmax(pi_a, dim=1)\n",
    "        v = self.fc_critic(x)\n",
    "        return prob, v\n",
    "\n",
    "    def act(self, state, action=None):\n",
    "        prob, v = self.forward(state)\n",
    "        dist = Categorical(prob)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return {'a': action,\n",
    "                'log_pi_a': log_prob,\n",
    "                'ent': entropy,\n",
    "                'v': v.squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 seed,\n",
    "                 hidden_layers,\n",
    "                 lr_policy,\n",
    "                 use_reset,\n",
    "                 device\n",
    "                ):\n",
    "\n",
    "        #self.main_net = ConvNet(state_size, feature_dim, seed, use_reset, input_channel).to(device)\n",
    "        self.main_net = FCNet(state_size, seed, hidden_layers=[64,64], use_reset=True, act_fnc=F.relu).to(device)\n",
    "        self.policy = Policy(state_size, action_size, seed, self.main_net).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        self.device = device\n",
    "\n",
    "    def update(self, log_probs_old, states, actions, returns, advantages, cliprange=0.1, beta=0.01):\n",
    "\n",
    "        traj_info = self.policy.act(states, actions)\n",
    "\n",
    "        ratio = torch.exp(traj_info['log_pi_a'] - log_probs_old)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        value_loss = 0.5*(returns - traj_info['v']).pow(2).mean()\n",
    "        entropy = traj_info['ent'].mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        (policy_loss + value_loss - beta*entropy).backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), 5)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return policy_loss.data.cpu().numpy(), value_loss.data.cpu().numpy(), entropy.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collect_trajectories(envs, policy, rollout_length=200):\n",
    "    \"\"\"collect trajectories for a parallelized parallelEnv object\n",
    "\n",
    "    Returns : Shape\n",
    "    ======\n",
    "    log_probs_old (tensor)   :  (rollout_length*n,)\n",
    "    states (tensor)          :  (rollout_length*n, envs.observation_space.shape[0])\n",
    "    actions (tensor)         :  (rollout_length*n,)\n",
    "    rewards (list,np.array)  :  (rollout_length, n)  --> for advs\n",
    "    values (list,np.array)   :  (rollout_length, n)  --> for advs\n",
    "    dones (list,np.array)    :  (rollout_length, n)  --> for advs\n",
    "    vals_last (list,np.array):  (n,)                 --> for advs\n",
    "    \"\"\"\n",
    "    n=len(envs.ps)         # number of parallel instances\n",
    "\n",
    "    log_probs_old, states, actions, rewards, values, dones = [],[],[],[],[],[]\n",
    "\n",
    "    obs = envs.reset()\n",
    "\n",
    "    for t in range(rollout_length):\n",
    "\n",
    "        batch_input = torch.from_numpy(obs).float().to(device)\n",
    "        traj_info = policy.act(batch_input)\n",
    "\n",
    "        log_prob_old = traj_info['log_pi_a'].detach()\n",
    "        action = traj_info['a'].cpu().numpy()\n",
    "        value = traj_info['v'].cpu().detach().numpy()\n",
    "\n",
    "        obs, reward, is_done, _ = envs.step(action)\n",
    "\n",
    "        if is_done.any():\n",
    "            if t < 199:\n",
    "                idx = np.where(is_done==True)\n",
    "                reward[idx] = 0\n",
    "\n",
    "        log_probs_old.append(log_prob_old) # shape (rollout_length, n)\n",
    "        states.append(batch_input)         # shape (rollout_length, n, envs.observation_space.shape[0])\n",
    "        actions.append(action)             # shape (rollout_length, n)\n",
    "        rewards.append(reward)             # shape (rollout_length, n)\n",
    "        values.append(value)               # shape (rollout_length, n)\n",
    "        dones.append(is_done)              # shape (rollout_length, n)\n",
    "\n",
    "    log_probs_old = torch.stack(log_probs_old).view(-1,)\n",
    "    states = torch.stack(states)\n",
    "    states = states.view(-1,envs.observation_space.shape[0])\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device).view(-1,)\n",
    "\n",
    "    obs = torch.from_numpy(obs).float().to(device)\n",
    "    traj_info_last = policy.act(obs)\n",
    "    vals_last = traj_info_last['v'].cpu().detach().numpy()\n",
    "\n",
    "    return log_probs_old, states, actions, rewards, values, dones, vals_last\n",
    "\n",
    "def random_sample(inds, minibatch_size):\n",
    "    inds = np.random.permutation(inds)\n",
    "    batches = inds[:len(inds) // minibatch_size * minibatch_size].reshape(-1, minibatch_size)\n",
    "    for batch in batches:\n",
    "        yield torch.from_numpy(batch).long()\n",
    "    r = len(inds) % minibatch_size\n",
    "    if r:\n",
    "        yield torch.from_numpy(inds[-r:]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from multiprocessing import Process, Pipe\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "class VecEnv(ABC):\n",
    "    \"\"\"\n",
    "    An abstract asynchronous, vectorized environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_envs, observation_space, action_space):\n",
    "        self.num_envs = num_envs\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all the environments and return an array of\n",
    "        observations, or a dict of observation arrays.\n",
    "        If step_async is still doing work, that work will\n",
    "        be cancelled and step_wait() should not be called\n",
    "        until step_async() is invoked again.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_async(self, actions):\n",
    "        \"\"\"\n",
    "        Tell all the environments to start taking a step\n",
    "        with the given actions.\n",
    "        Call step_wait() to get the results of the step.\n",
    "        You should not call this if a step_async run is\n",
    "        already pending.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_wait(self):\n",
    "        \"\"\"\n",
    "        Wait for the step taken with step_async().\n",
    "        Returns (obs, rews, dones, infos):\n",
    "         - obs: an array of observations, or a dict of\n",
    "                arrays of observations.\n",
    "         - rews: an array of rewards\n",
    "         - dones: an array of \"episode done\" booleans\n",
    "         - infos: a sequence of info objects\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up the environments' resources.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Step the environments synchronously.\n",
    "        This is available for backwards compatibility.\n",
    "        \"\"\"\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        #logger.warn('Render not defined for %s' % self)\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        if isinstance(self, VecEnvWrapper):\n",
    "            return self.venv.unwrapped\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class parallelEnv(VecEnv):\n",
    "    def __init__(self, env_name='PongDeterministic-v4',\n",
    "                 n=4, seed=None,\n",
    "                 spaces=None):\n",
    "\n",
    "        env_fns = [ gym.make(env_name) for _ in range(n) ]\n",
    "\n",
    "        if seed is not None:\n",
    "            for i,e in enumerate(env_fns):\n",
    "                e.seed(i+seed)\n",
    "\n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        adopted from openai baseline\n",
    "        \"\"\"\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        observation_space, action_space = self.remotes[0].recv()\n",
    "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for remote in self.remotes:\n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "        self.closed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def train(episode, env_name, lr_policy, gamma, gae_lambda):\n",
    "    use_gae = True\n",
    "    beta = .01\n",
    "    cliprange = 0.1\n",
    "    best_score = -np.inf\n",
    "    goal_score = 20.0\n",
    "\n",
    "    nenvs = 1\n",
    "    rollout_length = 200\n",
    "    minibatches = 10*8\n",
    "    # Calculate the batch_size\n",
    "    nbatch = nenvs * rollout_length\n",
    "    optimization_epochs = 4\n",
    "\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    envs = parallelEnv(env_name, nenvs, seed=1234)\n",
    "    agent = PPOAgent(state_size=envs.observation_space.shape[0],\n",
    "                     action_size=envs.action_space.n,\n",
    "                     seed=0,\n",
    "                     hidden_layers=[64,64],\n",
    "                     lr_policy=lr_policy,\n",
    "                     use_reset=True,\n",
    "                     device=device)\n",
    "    print(\"------------------\")\n",
    "    print(agent.policy)\n",
    "    print(\"------------------\")\n",
    "\n",
    "    # keep track of progress\n",
    "    mean_rewards = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    loss_storage = []\n",
    "\n",
    "    for i_episode in range(episode+1):\n",
    "        log_probs_old, states, actions, rewards, values, dones, vals_last = collect_trajectories(envs, agent.policy, rollout_length)\n",
    "\n",
    "        returns = np.zeros_like(rewards)\n",
    "        advantages = np.zeros_like(rewards)\n",
    "\n",
    "        if not use_gae:\n",
    "            for t in reversed(range(rollout_length)):\n",
    "                if t == rollout_length - 1:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * vals_last\n",
    "                else:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * returns[t+1]\n",
    "                advantages[t] = returns[t] - values[t]\n",
    "        else:\n",
    "            for t in reversed(range(rollout_length)):\n",
    "                if t == rollout_length - 1:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * vals_last\n",
    "                    td_error = returns[t] - values[t]\n",
    "                else:\n",
    "                    returns[t] = rewards[t] + gamma * (1-dones[t]) * returns[t+1]\n",
    "                    td_error = rewards[t] + gamma * (1-dones[t]) * values[t+1] - values[t]\n",
    "                advantages[t] = advantages[t] * gae_lambda * gamma * (1-dones[t]) + td_error\n",
    "\n",
    "        # convert to pytorch tensors and move to gpu if available\n",
    "        returns = torch.from_numpy(returns).float().to(device).view(-1,)\n",
    "        advantages = torch.from_numpy(advantages).float().to(device).view(-1,)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(optimization_epochs):\n",
    "            sampler = random_sample(nbatch, minibatches)\n",
    "            for inds in sampler:\n",
    "                mb_log_probs_old = log_probs_old[inds]\n",
    "                mb_states = states[inds]\n",
    "                mb_actions = actions[inds]\n",
    "                mb_returns = returns[inds]\n",
    "                mb_advantages = advantages[inds]\n",
    "                loss_p, loss_v, loss_ent = agent.update(mb_log_probs_old, mb_states, mb_actions, mb_returns, mb_advantages, cliprange=cliprange, beta=beta)\n",
    "                loss_storage.append([loss_p, loss_v, loss_ent])\n",
    "\n",
    "        total_rewards = np.sum(rewards, axis=0)\n",
    "        scores_window.append(np.mean(total_rewards)) # last 100 scores\n",
    "        mean_rewards.append(np.mean(total_rewards))  # get the average reward of the parallel environments\n",
    "        cliprange*=.999                              # the clipping parameter reduces as time goes on\n",
    "        beta*=.999                                   # the regulation term reduces\n",
    "\n",
    "        if i_episode % 2 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            print(total_rewards)\n",
    "        if np.mean(scores_window)>=goal_score and np.mean(scores_window)>=best_score:\n",
    "            torch.save(agent.policy.state_dict(), \"policy_mountaincar.pth\")\n",
    "            best_score = np.mean(scores_window)\n",
    "\n",
    "    # Save mean_rewards to a .pkl file\n",
    "    with open(f'mean_rewards_{lr_policy}_{gamma}_{gae_lambda}.pkl', 'wb') as f:\n",
    "        pickle.dump(mean_rewards, f)\n",
    "\n",
    "    # Save loss_storage to a .pkl file\n",
    "    with open(f'loss_storage_{lr_policy}_{gamma}_{gae_lambda}.pkl', 'wb') as f:\n",
    "        pickle.dump(loss_storage, f)\n",
    "\n",
    "    return mean_rewards, loss_storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr_policy': 0.001, 'gamma': 0.1, 'gae_lambda': 0.1}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "configs = [\n",
    "    {'lr_policy': 1e-3, 'gamma': 0.1, 'gae_lambda': 0.1},\n",
    "    {'lr_policy': 1e-3, 'gamma': 0.1, 'gae_lambda': 0.9},\n",
    "    # add more configurations if needed\n",
    "]\n",
    "\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    mean_rewards, loss = train(500, 'CartPole-v1', **config)\n",
    "    plt.plot(mean_rewards, label=f'ppo learning rate={config[\"lr_policy\"]}, gamma={config[\"gamma\"]}, gae_lambda={config[\"gae_lambda\"]}')\n",
    "\n",
    "plt.ylabel('Average score')\n",
    "plt.xlabel('Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
