{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sJfJfEDM9wa"
      },
      "source": [
        "## Origin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cJ-eknINM1Tg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "gamma = 0.99\n",
        "goal_score = 200\n",
        "log_interval = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "max_kl = 0.01\n",
        "\n",
        "def flat_grad(grads):\n",
        "    grad_flatten = []\n",
        "    for grad in grads:\n",
        "        grad_flatten.append(grad.view(-1))\n",
        "    grad_flatten = torch.cat(grad_flatten)\n",
        "    return grad_flatten\n",
        "\n",
        "def flat_hessian(hessians):\n",
        "    hessians_flatten = []\n",
        "    for hessian in hessians:\n",
        "        hessians_flatten.append(hessian.contiguous().view(-1))\n",
        "    hessians_flatten = torch.cat(hessians_flatten).data\n",
        "    return hessians_flatten\n",
        "\n",
        "def flat_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.data.view(-1))\n",
        "    params_flatten = torch.cat(params)\n",
        "    return params_flatten\n",
        "\n",
        "def update_model(model, new_params):\n",
        "    index = 0\n",
        "    for params in model.parameters():\n",
        "        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
        "        params_length = len(params.view(-1))\n",
        "        new_param = new_params[index: index + params_length]\n",
        "        new_param = new_param.view(params.size())\n",
        "        params.data.copy_(new_param)\n",
        "        index += params_length\n",
        "\n",
        "def kl_divergence(policy, old_policy):\n",
        "    kl = old_policy * torch.log(old_policy / policy)\n",
        "\n",
        "    kl = kl.sum(1, keepdim=True)\n",
        "    return kl\n",
        "\n",
        "def fisher_vector_product(net, states, p, cg_damp=0.1):\n",
        "    policy = net(states)\n",
        "    old_policy = net(states).detach()\n",
        "    kl = kl_divergence(policy, old_policy)\n",
        "    kl = kl.mean()\n",
        "    kl_grad = torch.autograd.grad(kl, net.parameters(), create_graph=True) # create_graph is True if we need higher order derivative products\n",
        "    kl_grad = flat_grad(kl_grad)\n",
        "\n",
        "    kl_grad_p = (kl_grad * p.detach()).sum()\n",
        "    kl_hessian_p = torch.autograd.grad(kl_grad_p, net.parameters())\n",
        "    kl_hessian_p = flat_hessian(kl_hessian_p)\n",
        "\n",
        "    return kl_hessian_p + cg_damp * p.detach()\n",
        "\n",
        "\n",
        "def conjugate_gradient(net, states, loss_grad, n_step=10, residual_tol=1e-10):\n",
        "    x = torch.zeros(loss_grad.size())\n",
        "    r = loss_grad.clone()\n",
        "    p = loss_grad.clone()\n",
        "    r_dot_r = torch.dot(r, r)\n",
        "\n",
        "    for i in range(n_step):\n",
        "        A_dot_p = fisher_vector_product(net, states, p)\n",
        "        alpha = r_dot_r / torch.dot(p, A_dot_p)\n",
        "        x += alpha * p\n",
        "        r -= alpha * A_dot_p\n",
        "        new_r_dot_r = torch.dot(r,r)\n",
        "        betta = new_r_dot_r / r_dot_r\n",
        "        p = r + betta * p\n",
        "        r_dot_r = new_r_dot_r\n",
        "        if r_dot_r < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "class TRPO(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(TRPO, self).__init__()\n",
        "        self.t = 0\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        self.fc_1 = nn.Linear(num_inputs, 128)\n",
        "        self.fc_2 = nn.Linear(128, num_outputs)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform(m.weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = torch.relu(self.fc_1(input))\n",
        "        policy = F.softmax(self.fc_2(x))\n",
        "\n",
        "        return policy\n",
        "\n",
        "    @classmethod\n",
        "    def train_model(cls, net, transitions):\n",
        "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
        "\n",
        "        states = torch.stack(states)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        masks = torch.Tensor(masks)\n",
        "\n",
        "        returns = torch.zeros_like(rewards)\n",
        "\n",
        "        running_return = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
        "            returns[t] = running_return\n",
        "\n",
        "        policy = net(states)\n",
        "        policy = policy.view(-1, net.num_outputs)\n",
        "        policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        old_policy = net(states).detach()\n",
        "        old_policy = old_policy.view(-1, net.num_outputs)\n",
        "        old_policy_action = (old_policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
        "\n",
        "        surrogate_loss_grad = torch.autograd.grad(surrogate_loss, net.parameters())\n",
        "        surrogate_loss_grad = flat_grad(surrogate_loss_grad)\n",
        "\n",
        "        step_dir = conjugate_gradient(net, states, surrogate_loss_grad.data)\n",
        "\n",
        "        params = flat_params(net)\n",
        "        shs = (step_dir * fisher_vector_product(net, states, step_dir)).sum(0, keepdim=True)\n",
        "        step_size = torch.sqrt((2 * max_kl) / shs)[0]\n",
        "        full_step = step_size * step_dir\n",
        "\n",
        "        fraction = 1.0\n",
        "        for _ in range(10):\n",
        "            new_params = params + fraction * full_step\n",
        "            update_model(net, new_params)\n",
        "            policy = net(states)\n",
        "            policy = policy.view(-1, net.num_outputs)\n",
        "            policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "            surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
        "\n",
        "            kl = kl_divergence(policy, old_policy)\n",
        "            kl = kl.mean()\n",
        "\n",
        "            if kl < max_kl:\n",
        "                break\n",
        "            fraction = fraction * 0.5\n",
        "\n",
        "        return -surrogate_loss\n",
        "\n",
        "    def get_action(self, input):\n",
        "        policy = self.forward(input)\n",
        "        policy = policy[0].data.numpy()\n",
        "\n",
        "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nvHbCT_TOmWJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque()\n",
        "\n",
        "    def push(self, state, next_state, action, reward, mask):\n",
        "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
        "\n",
        "    def sample(self):\n",
        "        memory = self.memory\n",
        "        return Transition(*zip(*memory))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6b2PXAfOstG",
        "outputId": "1dd31717-c26b-4b38-8649-996c427146dc"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorboardX'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboardX\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     14\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_name)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = gym.make(env_name)\n",
        "    env.seed(500)\n",
        "    torch.manual_seed(500)\n",
        "\n",
        "    num_inputs = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n\n",
        "    print('state size:', num_inputs)\n",
        "    print('action size:', num_actions)\n",
        "\n",
        "    net = TRPO(num_inputs, num_actions)\n",
        "    writer = SummaryWriter('logs')\n",
        "\n",
        "    net.to(device)\n",
        "    net.train()\n",
        "    running_score = 0\n",
        "    steps = 0\n",
        "    loss = 0\n",
        "    for e in range(100):\n",
        "        done = False\n",
        "        memory = Memory()\n",
        "\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "        state = torch.Tensor(state).to(device)\n",
        "        state = state.unsqueeze(0)\n",
        "\n",
        "        while not done:\n",
        "            steps += 1\n",
        "\n",
        "            action = net.get_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_state = torch.Tensor(next_state)\n",
        "            next_state = next_state.unsqueeze(0)\n",
        "\n",
        "            mask = 0 if done else 1\n",
        "            reward = reward if not done or score == 499 else -1\n",
        "\n",
        "            action_one_hot = torch.zeros(2)\n",
        "            action_one_hot[action] = 1\n",
        "            memory.push(state, next_state, action_one_hot, reward, mask)\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "        loss = TRPO.train_model(net, memory.sample())\n",
        "\n",
        "        score = score if score == 500.0 else score + 1\n",
        "        running_score = 0.99 * running_score + 0.01 * score\n",
        "        if e % log_interval == 0:\n",
        "            print('{} episode | score: {:.2f}'.format(\n",
        "                e, running_score))\n",
        "            writer.add_scalar('log/score', float(running_score), e)\n",
        "            writer.add_scalar('log/loss', float(loss), e)\n",
        "\n",
        "        if running_score > goal_score:\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx1ItnEFQPk5"
      },
      "source": [
        "## CP - Try 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98KvMD8lQO9W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "gamma = 0.99\n",
        "goal_score = 200\n",
        "log_interval = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "max_kl = 0.01\n",
        "\n",
        "def flat_grad(grads):\n",
        "    grad_flatten = []\n",
        "    for grad in grads:\n",
        "        grad_flatten.append(grad.view(-1))\n",
        "    grad_flatten = torch.cat(grad_flatten)\n",
        "    return grad_flatten\n",
        "\n",
        "def flat_hessian(hessians):\n",
        "    hessians_flatten = []\n",
        "    for hessian in hessians:\n",
        "        hessians_flatten.append(hessian.contiguous().view(-1))\n",
        "    hessians_flatten = torch.cat(hessians_flatten).data\n",
        "    return hessians_flatten\n",
        "\n",
        "def flat_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.data.view(-1))\n",
        "    params_flatten = torch.cat(params)\n",
        "    return params_flatten\n",
        "\n",
        "def update_model(model, new_params):\n",
        "    index = 0\n",
        "    for params in model.parameters():\n",
        "        params_length = len(params.view(-1))\n",
        "        new_param = new_params[index: index + params_length]\n",
        "        new_param = new_param.view(params.size())\n",
        "        params.data.copy_(new_param)\n",
        "        index += params_length\n",
        "\n",
        "def kl_divergence(policy, old_policy):\n",
        "    kl = old_policy * torch.log(old_policy / policy)\n",
        "\n",
        "    kl = kl.sum(1, keepdim=True)\n",
        "    return kl\n",
        "\n",
        "def fisher_vector_product(net, states, p, cg_damp=0.1):\n",
        "    policy = net(states)\n",
        "    old_policy = net(states).detach()\n",
        "    kl = kl_divergence(policy, old_policy)\n",
        "    kl = kl.mean()\n",
        "    kl_grad = torch.autograd.grad(kl, net.parameters(), create_graph=True) # create_graph is True if we need higher order derivative products\n",
        "    kl_grad = flat_grad(kl_grad)\n",
        "\n",
        "    kl_grad_p = (kl_grad * p.detach()).sum()\n",
        "    kl_hessian_p = torch.autograd.grad(kl_grad_p, net.parameters())\n",
        "    kl_hessian_p = flat_hessian(kl_hessian_p)\n",
        "\n",
        "    return kl_hessian_p + cg_damp * p.detach()\n",
        "\n",
        "\n",
        "def conjugate_gradient(net, states, loss_grad, n_step=10, residual_tol=1e-10):\n",
        "    x = torch.zeros(loss_grad.size())\n",
        "    r = loss_grad.clone()\n",
        "    p = loss_grad.clone()\n",
        "    r_dot_r = torch.dot(r, r)\n",
        "\n",
        "    for i in range(n_step):\n",
        "        A_dot_p = fisher_vector_product(net, states, p)\n",
        "        alpha = r_dot_r / torch.dot(p, A_dot_p)\n",
        "        x += alpha * p\n",
        "        r -= alpha * A_dot_p\n",
        "        new_r_dot_r = torch.dot(r,r)\n",
        "        betta = new_r_dot_r / r_dot_r\n",
        "        p = r + betta * p\n",
        "        r_dot_r = new_r_dot_r\n",
        "        if r_dot_r < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "class TRPO(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(TRPO, self).__init__()\n",
        "        self.t = 0\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        self.fc_1 = nn.Linear(num_inputs, 128)\n",
        "        self.fc_2 = nn.Linear(128, num_outputs)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform(m.weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = torch.relu(self.fc_1(input))\n",
        "        policy = F.softmax(self.fc_2(x))\n",
        "\n",
        "        return policy\n",
        "\n",
        "    @classmethod\n",
        "    def train_model(cls, net, transitions):\n",
        "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
        "\n",
        "        states = torch.stack(states)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        masks = torch.Tensor(masks)\n",
        "\n",
        "        returns = torch.zeros_like(rewards)\n",
        "\n",
        "        running_return = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
        "            returns[t] = running_return\n",
        "\n",
        "        policy = net(states)\n",
        "        policy = policy.view(-1, net.num_outputs)\n",
        "        policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        old_policy = net(states).detach()\n",
        "        old_policy = old_policy.view(-1, net.num_outputs)\n",
        "        old_policy_action = (old_policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
        "\n",
        "        surrogate_loss_grad = torch.autograd.grad(surrogate_loss, net.parameters())\n",
        "        surrogate_loss_grad = flat_grad(surrogate_loss_grad)\n",
        "\n",
        "        step_dir = conjugate_gradient(net, states, surrogate_loss_grad.data)\n",
        "\n",
        "        params = flat_params(net)\n",
        "        shs = (step_dir * fisher_vector_product(net, states, step_dir)).sum(0, keepdim=True)\n",
        "        step_size = torch.sqrt((2 * max_kl) / shs)[0]\n",
        "        full_step = step_size * step_dir\n",
        "\n",
        "        fraction = 1.0\n",
        "        for _ in range(10):\n",
        "            new_params = params + fraction * full_step\n",
        "            update_model(net, new_params)\n",
        "            policy = net(states)\n",
        "            policy = policy.view(-1, net.num_outputs)\n",
        "            policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "            surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
        "\n",
        "            kl = kl_divergence(policy, old_policy)\n",
        "            kl = kl.mean()\n",
        "\n",
        "            if kl < max_kl:\n",
        "                break\n",
        "            fraction = fraction * 0.5\n",
        "\n",
        "        return -surrogate_loss\n",
        "\n",
        "    def get_action(self, input):\n",
        "        policy = self.forward(input)\n",
        "        policy = policy[0].data.numpy()\n",
        "\n",
        "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
        "        return action\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque()\n",
        "\n",
        "    def push(self, state, next_state, action, reward, mask):\n",
        "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
        "\n",
        "    def sample(self):\n",
        "        memory = self.memory\n",
        "        return Transition(*zip(*memory))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1qiFl8glQUTo",
        "outputId": "8160c889-d053-46c3-a1db-03e4cb7f294a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "<ipython-input-1-eae9cacfc7be>:95: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(m.weight)\n",
            "<ipython-input-1-eae9cacfc7be>:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  policy = F.softmax(self.fc_2(x))\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state size: 4\n",
            "action size: 2\n",
            "0 episode | score: 0.22\n",
            "10 episode | score: 3.97\n",
            "20 episode | score: 7.04\n",
            "30 episode | score: 10.85\n",
            "40 episode | score: 13.13\n",
            "50 episode | score: 15.92\n",
            "60 episode | score: 17.50\n",
            "70 episode | score: 20.36\n",
            "80 episode | score: 22.72\n",
            "90 episode | score: 23.54\n",
            "100 episode | score: 25.06\n",
            "110 episode | score: 26.65\n",
            "120 episode | score: 28.46\n",
            "130 episode | score: 31.42\n",
            "140 episode | score: 34.41\n",
            "150 episode | score: 38.92\n",
            "160 episode | score: 47.43\n",
            "170 episode | score: 51.84\n",
            "180 episode | score: 55.16\n",
            "190 episode | score: 58.82\n",
            "200 episode | score: 63.40\n",
            "210 episode | score: 67.51\n",
            "220 episode | score: 81.78\n",
            "230 episode | score: 86.18\n",
            "240 episode | score: 88.18\n",
            "250 episode | score: 90.64\n",
            "260 episode | score: 104.27\n",
            "270 episode | score: 113.02\n",
            "280 episode | score: 123.19\n",
            "290 episode | score: 130.04\n",
            "300 episode | score: 141.44\n",
            "310 episode | score: 147.50\n",
            "320 episode | score: 155.92\n",
            "330 episode | score: 164.92\n",
            "340 episode | score: 169.83\n",
            "350 episode | score: 180.20\n",
            "360 episode | score: 186.51\n",
            "370 episode | score: 192.59\n",
            "380 episode | score: 198.34\n",
            "390 episode | score: 192.81\n",
            "400 episode | score: 186.62\n",
            "410 episode | score: 192.85\n",
            "420 episode | score: 193.86\n",
            "430 episode | score: 193.82\n",
            "440 episode | score: 191.36\n",
            "450 episode | score: 183.96\n",
            "460 episode | score: 178.31\n",
            "470 episode | score: 179.28\n",
            "480 episode | score: 173.51\n",
            "490 episode | score: 168.28\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACvlklEQVR4nO2deZgU1bn/v73PPsM6A7KIguygosK4LygiMRrJjfESJcYsGvS6XRPNNa65wZ9JNNEQc5MYjVGDmkQTdxEVFxYVBREURZEBYdhnZaZ7urt+f3RX9Tmnzqmq7umuXub9PA8PPVXVVadqaup86109mqZpIAiCIAiCKFG8+R4AQRAEQRBELiGxQxAEQRBESUNihyAIgiCIkobEDkEQBEEQJQ2JHYIgCIIgShoSOwRBEARBlDQkdgiCIAiCKGn8+R5AIRCPx7F9+3ZUV1fD4/HkezgEQRAEQThA0zS0t7dj6NCh8HrV9hsSOwC2b9+O4cOH53sYBEEQBEFkwNatWzFs2DDlehI7AKqrqwEkLlZNTU2eR0MQBEEQhBPa2towfPhwYx5XQWIHMFxXNTU1JHYIgiAIosiwC0GhAGWCIAiCIEoaEjsEQRAEQZQ0JHYIgiAIgihpSOwQBEEQBFHSkNghCIIgCKKkIbFDEARBEERJQ2KHIAiCIIiShsQOQRAEQRAlDYkdgiAIgiBKGhI7BEEQBEGUNHkVO7fccgs8Hg/3b9y4ccb67u5uLFiwAAMGDEBVVRXmzp2LnTt3cvtoamrCnDlzUFFRgcGDB+O6665DNBp1+1QIgiAIgihQ8t4ba+LEiXj55ZeNn/3+1JCuvvpqPPvss3jiiSdQW1uLyy+/HOeddx7eeustAEAsFsOcOXPQ0NCA5cuXY8eOHbjooosQCATw85//3PVzIQiCIAii8Mi72PH7/WhoaDAtb21txf33349HH30Up556KgDggQcewPjx47Fy5UrMmDEDL730EjZs2ICXX34Z9fX1OPzww3H77bfjxz/+MW655RYEg0G3T4cgCBticQ3ReBwhvy/fQyFcpisSQ3mwOH7vkWgcXg/g91G0RymQ99/ip59+iqFDh+KQQw7BvHnz0NTUBABYvXo1enp6MHPmTGPbcePGYcSIEVixYgUAYMWKFZg8eTLq6+uNbWbNmoW2tjasX79eecxwOIy2tjbuH0EQ7vCVe9/E5FtewoEIuZv7Ess/24PJt7yI+9/cnO+h2BKLazjz16/jnEVvQdO0fA+HyAJ5FTvTp0/Hgw8+iBdeeAH33XcfNm/ejBNOOAHt7e1obm5GMBhEXV0d9536+no0NzcDAJqbmzmho6/X16lYuHAhamtrjX/Dhw/P7okRBKHkox1tiETjeG9LS76HQrjI+i/bEI1r+GBbS76HYktbVw8+39OJ9dsTYyaKn7y6sWbPnm18njJlCqZPn46RI0fi8ccfR3l5ec6Oe8MNN+Caa64xfm5rayPBQxAEkUPiSQtJMRhK4swgi2G8hD15d2Ox1NXV4bDDDsOmTZvQ0NCASCSClpYWbpudO3caMT4NDQ2m7Cz9Z1kckE4oFEJNTQ33jyAId9FAs0hfQjeQxItAPcSYMRbDeAl7CkrsdHR04LPPPsOQIUMwbdo0BAIBLF261Fi/ceNGNDU1obGxEQDQ2NiIdevWYdeuXcY2S5YsQU1NDSZMmOD6+AmCIAg5xWTZKYYxEumRVzfWf//3f+Pss8/GyJEjsX37dtx8883w+Xy44IILUFtbi0suuQTXXHMN+vfvj5qaGlxxxRVobGzEjBkzAABnnHEGJkyYgAsvvBB33nknmpubceONN2LBggUIhUL5PDWCIAiCQQ/0LQZLSZwsOyVHXsXOtm3bcMEFF2Dv3r0YNGgQjj/+eKxcuRKDBg0CANx9993wer2YO3cuwuEwZs2ahd/97nfG930+H5555hlcdtllaGxsRGVlJebPn4/bbrstX6dEEARBSNDdWMWgHWJxVuzkcSBE1sir2Fm8eLHl+rKyMixatAiLFi1SbjNy5Eg899xz2R4aQRA5phgmPSJ7xIvIssMOkVLPS4OCitkhCKLvQFNI3yIe18VOngfiAN6NlceBEFmDxA5BEASRc1JurMJXD6wbi1R5aUBihyCIvFAMkx6RPYxsrDyPwwms1ikGtxthD4kdgiAIIucUU50djbKxSg4SOwRB5AWaQvoWqdTzPA/EAWxRwSIYLuEAEjsEQeQHmkX6FKmigoX/i4/Hmc9FMF7CHhI7BEEQRM4ppjo7nMApgvES9pDYIQiCIHJOMdXZodTz0oPEDkEQrqFxsRA0i/QltCIKUKZsrNKDxA5BEK5B80bfpZgagcYpQLnkILFDEEReKIZJj8geeqG+Yvi9x9neWOTHKglI7BAE4RrstFEMkx6RPYqpzg7pm9KDxA5BEK5RDGnHRG7QijZAufDHS9hDYocgCNegaaPvUlTtIuKUjVVqkNghCMI1qHxJ3yXlxsrvOJzA9QEly05JQGKHIAjXoHTzvksxVVCOUZ2dkoPEDkEQeaEYJj0iexRXnR0ulD5v4yCyB4kdgiBcowjmOSJHFFOdHY0sOyUHiR2CIPICzSF9i2KK2YkxjUCLQZwR9pDYIQjCNWji6LvE48UTs0Op56UHiR2CIFyDApT7LsXUCFQjsVNykNghCMI1uNRzmkP6FMUUs0NurNKDxA5BEHmCZpG+RHG1i2AagRb+cAkHkNghCMI1aN7ou2hFZNnhu54XwYAJW0jsEAThGhq9MfdZitWyUwzZY4Q9JHYIgnANmjf6LsXVG4v5XATijLCHxA5BEK5B80bfpVgtO0UwXMIBJHYIgnAPagTaZ9FdmKzVpFDhxQ7dqaUAiR2CIPICzSF9i2JqBBonUV5ykNghCMI1KLOl7xKLF1HMDhugTBHKJQGJHYIgXIMrKlgU0x6RLYoqZidO2VilBokdgiBcg+aNvosRs9OLm2Db/gNo7+7J0ojUxEmUlxz+fA+AIIi+QzHEaxC5QRcQmd4CX7Z04fj/9yr8Xg82/fys7A1MAmVjlR5k2SEIwjXYeYMmkb5FbwOU39m8DwAQdcGvFIuT2Ck1SOwQBJEXaA7pW/Q2ZsfjyeJgbGCHWAwxRoQ9JHYIgnANvus5TSJ9iWzE7LgF3y6iCAZM2EJihyAI12CDPWkO6VsUU52dGNcIlCgFSOwQBOEelOXSZ9ErJxeB1iELZAlCYocgCNdgp41iaBtAZI+44cYqfPEQpwDlkoPEDkEQrsEXFST6EvFexux4XIxQjnExO64dlsghJHYIgsgL5B7oWxh1dopA5rICpxgsUYQ9JHYIgnANClDuu/TWsuMmGhUVLDlI7BAE4RrUG6vvov/ui8GixxcVLPzxEvaQ2CEIwjWognLfpdcxO1kcix1xii0rOUjsEAThGhoFfvZZeltnx90KylRUsNQgsUMQRF4gN1bfQi81UAwil3pjlR4kdgiCcA2+WFv+xkG4Dx/027tffq7jaCgbq/QgsUMQRF6gwM++BS8gerevXN86ccrGKjlI7BAE4RpUVLDvEuulZcfDhCjn2trCiR26U0sCEjsEQbgG1dnpu2QzOD3XcT9c13Nqa1ISkNghCMI1qMFi36W3cTBsNlbuLTupz3SXlgYkdgiCyAvFkJVDZI9iioNhG4FSgHJpQGKHIAjX0BSfidKH6yTey9++qzE7JHZKAhI7BEG4RjbTj4niQuPcWOl/n60pmPuYndRnuk1LAxI7BEG4BrWL6LtwQb+9/OXn3LLDubFyeijCJUjsEAThGtQItO/SW2sJG6Cs5ThDilLPSw8SOwRBuEjxBKkS2SWbcTC5FiDZLIBIFAYkdgiCyAs0ifQt4kVUZ6e3BRCJwqNgxM4dd9wBj8eDq666yljW3d2NBQsWYMCAAaiqqsLcuXOxc+dO7ntNTU2YM2cOKioqMHjwYFx33XWIRqMuj54gCCeQG6vvks1+U7mO2dGKKE2ecEZBiJ133nkH//d//4cpU6Zwy6+++mo8/fTTeOKJJ7Bs2TJs374d5513nrE+Fothzpw5iEQiWL58Of7yl7/gwQcfxE033eT2KRAE4QAKUO679LbOjpvNOdmqyVRnpzTIu9jp6OjAvHnz8Mc//hH9+vUzlre2tuL+++/HXXfdhVNPPRXTpk3DAw88gOXLl2PlypUAgJdeegkbNmzAww8/jMMPPxyzZ8/G7bffjkWLFiESieTrlAiCUEAVlPsmmqb1+nfPfz8Lg7IgRpadkiPvYmfBggWYM2cOZs6cyS1fvXo1enp6uOXjxo3DiBEjsGLFCgDAihUrMHnyZNTX1xvbzJo1C21tbVi/fr3ymOFwGG1tbdw/giByD/XGSpEPsZcQHfk4Lv9zJjE32Uxdt0Nz8ViEO+RV7CxevBjvvfceFi5caFrX3NyMYDCIuro6bnl9fT2am5uNbViho6/X16lYuHAhamtrjX/Dhw/v5ZkQBOEE6nqe4Md//wAn/uJVdIbdiy+MxzWc+7vl+Nb9q1wXPKJgyERAuOkCpaKCpUfexM7WrVtx5ZVX4pFHHkFZWZmrx77hhhvQ2tpq/Nu6daurxycIom+/MT/27lZs3deFZz/Y4doxt+w7gLVbW/DWpr0IR91t5S1acjL5zbtpbYllsbUFURjkTeysXr0au3btwpFHHgm/3w+/349ly5bhnnvugd/vR319PSKRCFpaWrjv7dy5Ew0NDQCAhoYGU3aW/rO+jYxQKISamhruH0EQucfNuAuicBDFSVckho3N7WlZmNy8d7KZJk8UBnkTO6eddhrWrVuHNWvWGP+OOuoozJs3z/gcCASwdOlS4zsbN25EU1MTGhsbAQCNjY1Yt24ddu3aZWyzZMkS1NTUYMKECa6fE0EQ1nAxO3kcR8Hgsd8kF4dy26omHu4/fr8cs379Ol5crw43MO0D7ll2iqlDO+EMf74OXF1djUmTJnHLKisrMWDAAGP5JZdcgmuuuQb9+/dHTU0NrrjiCjQ2NmLGjBkAgDPOOAMTJkzAhRdeiDvvvBPNzc248cYbsWDBAoRCIdfPiSAIaygbK3+w7RZiLpsrYsLvev+BHgDA4+9uw5mThjjaB58OnrWhOTgW3aelQN7EjhPuvvtueL1ezJ07F+FwGLNmzcLvfvc7Y73P58MzzzyDyy67DI2NjaisrMT8+fNx22235XHUBEE4geYQVw078DBHi7sbsqMUDGm5sTL8XiZks7UFURgUlNh57bXXuJ/LysqwaNEiLFq0SPmdkSNH4rnnnsvxyAiCyAZUQdma95r2484XPsaNcyZg0kG1Wd03a9lx3Y2VBXGluRhHQ26s0iPvdXYIguib0CQCeDy8bee83y3Hys/3Yd6fVuX0uKJbKdfYiavXNu7C+037Lbdx0wVKjUBLj4Ky7BAEUdrwQaZ5HEiB09rVk/V9svog7vLFV7qxAGzbfwDffuAdAMAXd8xR7sPNe4ez7JAFsiQgyw5BEK5BbixeaLgZs8NO4O5bduTLNQ34cn9X2vvIfW8sEuWlBokdgiBcQ1P+0HeI5mn25MSOy2NQuZ3SGYW7dXbYY/XRG7XEILFDEIRrUM8h/rw9Lpp2OMuI69lY6nVO7wI3e2NRgHLpQWKHIIi80FcnEday46bYyafQdHo8KyuKm72xYnES5aUGiR2CIFxDU3zuS7jtQtJhD1so2ViJLuzszxY7cbXrOfM5p0ci3ILEDkEQrsFlBPXRN+YYF6DsnmmHcwO5nY1l4TZz2gbC1QBlcreWHCR2CIJwEYqFyFdH7fxmY6ksO/zPVuNiXVy5Hn2MTDslB4kdgiBco68KHBZW7MRcDBRmr73brjTnMTtW+2C3c8+NRZad0oDEDkEQrsFOG311EonlyZ3Eu7FcO2zy2PLlGjTuprC6J/h7JzvjUsG7sXJ7LMIdSOwQBJEX+qjWQSyWH3eSmzEvIk4tMVYWJ81Fkci5GvvofVpqULsIgiBcgyooA1HGrOKmOykfMTvRpJ/OqoKyU4sN71rq/disIDdW6UGWHYIgXMPNztWFSr4yfdy0jAAJIXf8/3sVp/zqNU7g8WNy3uCTFce5FsokcEoPsuwQBOEabhaGK1SiXIByftxYbhx3Z1s3mtu6AQCd4Zij71iNiw9Q7tXQ0hoHCZ/SgCw7BEG4Bj9v9M1JJJYvscMeN8sTeE8sjuWb9qArIhc1qvPUoDkOBnbTtZTP+CYiN5DYIQjCNTSqs5M3q0EuLSO/eukT/OefVuGKv71vLGNbYVidJyu8rNxYbmZIcTV9+uh9WmqQ2CEIIi/01Tfm/NXZyZ1F6c9vbQYAvPzRTmMZWx1aadnRMrM45freiQnCav32VhyIRHN6TCK3kNghCMI9XIy7KFQKwbKTbTeWXcCz6ngaxOuh3ofm0AKUDdjzeeXjnZhzz5u46P63c3pMIreQ2CEIwjU0xee+RN5idnKYjWUn2pTH05yPy80AZXb/O9vCAIB3t+zP7UGJnEJihyAI16D6Jfm07OROZMl2x8bsZCPLys06OypLVNRNvyORVUjsEAThGlx9lL6pdbjUczfbRbgtNNl+7lZiJ+YwZsfN+kSq/X/Z0pXT4xK5g8QOQRBZR9M0PPn+Nny2u0NYznx2eUyFAjuhu9suwt2CjuwhoqoAZVPquVVRQeazi6nnLJ/v6czpcYncQWKHIIis8+y6Hbj6sbU47VfLlNv0WTcW2xvLRa+I20UFnXRZ1zSxD5WF2HFRrKksbl+Q2ClaSOwQBJF13m9qkS7n385dGUrBEXPRHcPidpsK1mWpLiroPBWfbyvR29FZo7o+JHaKFxI7BEG4Bpc+nMdx5JN8ZWPlss6O/Hipz9YByk7dWG7G7MiXt3b15PS4RO4gsUMQhGuwc0ifdWP1kd5Y7O+3R9EINLGd/DuZbtdbrILGe2J9854tBUjsEAThHlRopyBSz11xYzmK2dH461EAbiyraxOOUup5sUJihyCIrKOaL7jeWH1U7Tix7LA1arIFbxnJ/v6tiCosIhrScGO5JNasrk0P1dkpWkjsEASRF6ze4kuZfFl28hmzozpPMRvLeep5b0eXEC6/eflTrBYqI1uNgcRO8UJihyCIrKOy2vB1dvqmZSeat5gdd0UWewxVnR0gDbGT5fH/dcUW3P3yJ5h733JuOYmd0oTEDkEQruFm+nChwhUVVMydOfBicZY0Vyw7zGfn2Vjq/WW7N9anuzqky63GEKEA5aKFxA5BEFlHHbMj/9yXiDEKx003Vsx1NxZj2bGM2Un9bJUJ5Va7C6tr00MBykULiR2CIFyDq7PTh0w77d09WPx2E/Z1RhBzMGl7chChnI4b6OUNO/Fe037LbWyPx3xWtsUQsrGc98bq1dD0gyuGRG6sUsSf7wEQBNF3yHaQabHw4398gOfWNWPxO1tx1uQGY3m+6uxYHfaLPZ347kPvJj7fMSfj4/EB0XKRoIG35ji9J3IZ70XZWKUJWXYIgsgLfUjr4Ll1zQCANVtbuDgdpWUnB2OIO3RjZauzN3tqKjcW4Lx9hluNTK2uTYTcWEULiR2CIFzDrbiLQoa1cuTNsmNx3N4ILZ839W32CMqu5xo/FqvLwQe35+66We2bApSLFxI7BEG4SPoui1IjFpd/zjWcWylHF9/HxBo5743FfLYKUGZ7Y+VQJMp2HfAlzovcWMULiR2CILKO6u2Yr7PTN2EtO+oA5ewfl7OgOBQL6VpQvMyM4sRtpkFLw40l/5xtZEKwuiwAgMROMUNihyAI1+ADlPum3MlfUUH5ZyvS/RWpLDtZd2OlNyzb/bHIhGBlyAeAxE4xQ2KHIIis42Qy6qNax5Elw5ODEOV4Bm6sdN1dXi5mxz4bK7HOmfhzq2yBbNeVwUTick9M67MivdghsUMQhGtQuwggFsuPZYcLDrc6LqOznIyPnfy5AGWHlp2YQxHjWlFBqRsrVaWlh4KUixISOwRBuAYfZJrHgeQRR5WMc9L1PH2R5URUsEJG5cayEleO3Vhwtl1vkZ1zVSgldiLkyipKSOwQBOEaZNnJX9dzViBYuadYF5pKFHVFYugIRwHwcSwqN5bSspPGuOJZtuyoizrLYnYYyw7V2ilKSOwQBOEafIBy3oaRV5zGqGQbriifU8uOYl6f9rMlmHTzi+iKxDi3DqN1HKWea5qWkRsrl/eOzHBTEfQZ50ZBysUJiR2CILKOk7dmEjuAKvwjFxWU+d5Y6u3YtHeZpUXTNByIxAAAm/d0IspM/pwbi/mOVZxL3KGly60AZdkYgn4vAr7EdElurOKExA5BEHmB3Fjupt87dRfZWWTYRT2xOCdkNG67TLKxlJsJ+1Zv11ukYsfnQzApdlTCLRKN49kPdmBvRzh3gyMyhsQOQRBZx4mQIcuO82yn257egL+u3NKr47KT+KOrmvDsBztst5NN/OyyhNiRF0nkRJPFaWbWGyuHlh2J4Ar6vQj4dbEjV2S/feVTLHj0Pcy9b3nOxkZkDokdgiBcg3pjpV9U8L2m/fjzW5vx06c+7NVxxUMtePQ96XZ2Yoz9vUViceF82C3tLTtiUUHnqefKzXqN7L4M+b1GywhVM9Bn1yXE4xd7D+RucETGkNghCCLrKGN22N5YLo3FKY+s2oLfL/ss58dxaskAEpN/e3c042NlEudil6LO7qYnpnGWDlVckKrruQaNd685dGNlw/2nsj7KXHxOYnY8uejxQWQNEjsEQeQUVVByIRl24nENt/x7Pe54/mO0HIjk9FhOigp6PMDPntmAk3/5WsZi54UPd+Do/12KFZ/tBeDckqZyRcnW90R5N1ZM5cZyWFTQqRsrl/eOTEiF/N5UzA6lnhclJHYIgsgpcYXAKSCtgwgTaNvdk/3JTJXhZOWO+dObm7Fl7wE8/u5WY1k6Fo1LH34PezrCuOCPK22PxcJ6nGRWDqsAZZVLyrJ+jtOAbZdcoLLrFPJ7EfRbBygThQ2JHYIgcopqYiqkHkNh5m09muPSzk4ClFW9sXoTq+JUINhVeDbF7HBurNR27DetYpP4IovqccUdisTeIhsr68aiOjvFCYkdgiCyjioFmY+7cG04trBBp7ko9MdKl3SzsdhYkN5MtE6vt2bjVtKYISRiduQWHPa76pgd520seBdoLi07KrGTDFBWxezkbERENiCxQxBETlFNUoVUZ4edwNJxU2za1YGv/e4tvPrxLsffcdIugnV7qYSSHTVM80rAedVkdi4Xj/fXFV/gG/+3wvjZKvWc/fWqrGWaxgcoF0LquWzXIb/P1rJD8cmFDYkdgiCyjirFnLP4FJA3IFPLzg8fWY33m1pw8YPvWG7HWmfSteywLRhUFhIZdRVB7menp2Xlxvrpv9Zj48524+dINM4JGfZ3yn7TqjcWX2RRPa5sWwVV+5C6sXxszE4B3biEY0jsEASRU7hJpUADlMPRmPE5nZidpn3p11RJtxGolxFK6YytriLA/ezUGmLnxmKJRIUAZUXGlGXXc4eBzOyqP725GS9v2Gk5tkyxbRehyMZSxVkRhQGJHYIgcoBiAmTr7BRQ0A47gaVjPckkc4sVLKrWEey0ycW+pGF1qi1PiZ3unpjj652O5SkcjSndWE7H7VT8ieP/7kPvWo4tU+RuLDZmp3DuW8I5eRU79913H6ZMmYKamhrU1NSgsbERzz//vLG+u7sbCxYswIABA1BVVYW5c+di505ezTc1NWHOnDmoqKjA4MGDcd111yEazbwIF0EQ2UU1zxWQ1uHFTo47kbNzpZNMJHb7dMZWGUzF7OzrjDhPPVe4IFnrl84vX/oEj6xs4r6rixJH2VhaZm6sXGKbjUV1doqSvIqdYcOG4Y477sDq1avx7rvv4tRTT8U555yD9evXAwCuvvpqPP3003jiiSewbNkybN++Heedd57x/Vgshjlz5iASiWD58uX4y1/+ggcffBA33XRTvk6JIAgBZVHBAnJk5Tobi4WdLFXCgo3xYV1AsTSsCuz+EmLH2Xe54zHzemtXj3T7FZ/v5X7WD6M5sOykk43lVnsRlRvLLmaHApQLG7/9Jrnj7LPP5n7+3//9X9x3331YuXIlhg0bhvvvvx+PPvooTj31VADAAw88gPHjx2PlypWYMWMGXnrpJWzYsAEvv/wy6uvrcfjhh+P222/Hj3/8Y9xyyy0IBoOywyIcDiMcTnWmbWtry91JEkQfhysqyCwvJMsOV2cnBwGo7DzYGUlZnp1kGHEVi9OI2WG/tzcty45cfLQpxI5ITNPghUcIRleInTSysdy6X/Tx+Lwe4/xDfrbrOVl2ipGCidmJxWJYvHgxOjs70djYiNWrV6OnpwczZ840thk3bhxGjBiBFSsSqY8rVqzA5MmTUV9fb2wza9YstLW1GdYhGQsXLkRtba3xb/jw4bk7MYLogyizsRxObG4T7qUbqyrk/L2xM5wSOyoXTm/q8si23dcZdh6zo+inpbLsiOi/VyeWHXGcVqeX7btFtT99/D7GVBPiemMVzn1LOCfvYmfdunWoqqpCKBTCpZdeiieffBITJkxAc3MzgsEg6urquO3r6+vR3NwMAGhubuaEjr5eX6fihhtuQGtrq/Fv69atym0JgkgfVdBpoTYCZevsZOLGqgz5LNezLo6OsL1lR2UBS8eqwM7JHd3RzNxYmYid5BAd9caCcA0siwq668byMTn/TrKxiMImr24sABg7dizWrFmD1tZW/P3vf8f8+fOxbNmynB4zFAohFArl9BgE0ZdR9sPiZnHXhmMLO4E5FRRb9nYan9Ox7LAZXCqrhqqvVDpCjBUO3T3xjAKU2eOlb9lJLdMz0Lwes/XGeTaWo8P3Gn04rEAN+lLZWLFCKhBFOCbvlp1gMIjRo0dj2rRpWLhwIaZOnYrf/OY3aGhoQCQSQUtLC7f9zp070dDQAABoaGgwZWfpP+vbEAThPqqgZJbCcmOlMo2cCIpNuzpw0i9eM35OR+ywsNlLqnozXMxOGi6UGCd2Ys57YynER1uXsyxXQ+xI9slaS4Bk1/MMemPlEl0ksuMKBbzG2FUuOU+BRSg/uqoJyz/bk+9hFAx5Fzsi8Xgc4XAY06ZNQyAQwNKlS411GzduRFNTExobGwEAjY2NWLduHXbtSpVqX7JkCWpqajBhwgTXx04QRAJH7hkXx2NHuqnnr23k20NUZih2gNQEz/acinOfM4zZ4dLG444tI3yAcmp5um4sWZ0dk9iB5rgNhHuWHbP4DPq88Ht1y45C7OR+aI5ZvWUffvLkOvznH1fleygFQ17dWDfccANmz56NESNGoL29HY8++ihee+01vPjii6itrcUll1yCa665Bv3790dNTQ2uuOIKNDY2YsaMGQCAM844AxMmTMCFF16IO++8E83NzbjxxhuxYMECclMRRB5hDRBxhR+rgAw7gtixd1OwBfsA8yQukqiumzphNtMnFtfg83qUlh32czqZYvEMLTuqbKzeuLH0z36vF0Bc2F4+Zqtx5RJ9CKzo9fu88Hm9puWFytZ9XfkeQsGRV7Gza9cuXHTRRdixYwdqa2sxZcoUvPjiizj99NMBAHfffTe8Xi/mzp2LcDiMWbNm4Xe/+53xfZ/Ph2eeeQaXXXYZGhsbUVlZifnz5+O2227L1ykRBAF1BhZv2SmcSSOdCso3/+tDLH6HT2pIdx6uKfNj/4GEeJBZEvgMrNT30plo2f11R2O9LiroVOykxJn5gKIm1DRe4LiajaVyr8bNYg0A/HrMjuL+KDAvFiGQV7Fz//33W64vKyvDokWLsGjRIuU2I0eOxHPPPZftoREE0QtU/ZX4fklujsiasMOigvs6I/jLii2m5ekKt5rygCF29OOpK01n6sZKfQ73xDOK2cmWZUfH7zNHTqjik3a1dWPF53tx1uQhiUwol91YIrr1Lp1aR/mChJeZvGdjEQRRenAxJ4oso0KCTT3PxE1hO/8Jk09NWcoNFjPEgfy4rOBIJ/Wcc2NF446vPW9p4eN+nKB/RXYZ7QOUU5+/+YeV+HxPJ7bt78KCU0a77sY6ddxgtHX1YMqwOgCwjdkhChsSOwRBZB3V2zpLIQmfiMMKyqqJLt2JuLos9eiN21h2ogpLix3stuGemOO3fVXquVU8jey4MmtXQBLbpBJXn+9JpPY/+f6XWHDKaPd6YyXHEPR58ffLjjWW+22zsXI/NiJzCi4biyCI4odPPZdnYxXSC7LTCsqiqKkIJooJph+zw1h2DLEj3wlXAyjjmB3ndXZUdX2cCi0rN5bXZNnRBGFs/o7epsIty45+v3qF2dGXdMGl058s3eP+8sWN+MfqbTnZf1+HxA5BEFmHD3JNfeaDlQtH7TitsyMKoZPHDgKQ/kRcxVh29MletY8errpzGhWUhWwsmSXNbpkqQ8wKo4KyZJ3flHpun43V3h1NjsvR4TOCO+fkGLyCqUYfe0tXBGu2tpiunaeXyefvfLEfv311E659Ym2v9kPIIbFDEETWcZSNVThax3GdHXEyrqtINBu2EzviNFgZ9KEskHj8hpMVlVW7YMVOWkUFhXgb2WnJjqkq8ufUjSXrjaUji9lRubF0unoSQjTr2ViQ36P6JRbFjj72lZ/vw7mL3sLzH6pbEmXCzrburO6P4CGxQxBE1lH2xrJxWeQLp6nnotWnLllvJ91zqQz5URVKfFfvlaW27GQWs8OKiLCizo58Weoz58Zyms1llY0l+IbiDtxYOrmM8eJFuG7Z4bcRrVL/XrM9q2Ogbuq5pVdiJxKJYOPGjYhGnZURJwiib8ClmCsnqcJROxGHriJxwteLC9pNxGLwamXIbwQpp8SO/djSq7OT+pwoKmjeRrY3VUVj0bJz6KBKU3FFIHUtZG5KMWYnrjkLgo7HtZxaAmXnLI5VljbP0tsAZWowmlsyEjsHDhzAJZdcgoqKCkycOBFNTU0AgCuuuAJ33HFHVgdIEETxoXJjsRSSGyvc4ywI2OzGSoqdNI9XFfIb/bQ6uq0tO2x2WDoVlLlsLEXqueyYqqBkUeh5PR6TtSOxz8T/csuOOUBZdGM1t3YjHtc48bC3M5LTAGXOjZW8xKqYHRW9TcZiLTu72ropxT3LZCR2brjhBqxduxavvfYaysrKjOUzZ87EY489lrXBEQRRnCh7Yzmy+LgPb9mxcGOZLDvOYnZEygM+Q+y0h/UAXIVVQ+FWsoPLxmLcWD//2mRjueyQTio5AwkxIGuTYVUkUdw+rmncdi+sb8aMhUtx7RNrubHtbOvOvjhWBMsblh3h1OxagvQWNiPwmJ8vxfceejfjfRVaU9JCICOx89RTT+G3v/0tjj/+eO6iTpw4EZ999lnWBkcQRHGiLCrIBoW6OB47nMbsiOvKk6nn6RbV9Xo9RkZWRxrZRnr8zorP9uJbf1qFzclaNDL4bKy4MUbWQiGN2VHEW4lWLY9Hbu2wClCWZWPFNH6cQKK2DsvujrBrlh193KK4sbPs9JaIoCZf+XiXYksiEzISO7t378bgwYNNyzs7O0lREgThyLJTQIYdx41A2XOpqwjAl3zepTsR+7xAte7GCut1ZOy/p8cTXfDHlXhz0x788JH3HI01HI0ZosLHiR3Z99jjMZ8lbiyfT1YkMPG/7HRMlp245ijLi3Uz5gKZG0ucy3Jt2emJ5uYPopCKd+aTjMTOUUcdhWeffdb4Wb8p/vSnP6GxsTE7IyMIomhRZWBpim3yDVtnxyoIWLeW+L0evPGjUwxXh92piDVYvB6zZcfJ9RBTz2Xpynrsh5hCrgs6v8/asqOqfi2KEp/XY8qu4r4ji9nxiTE7zrK8IrG46RpnU3vI3Fg+U8yOzXTZyxf9SCxmv1EGUOhPgozaRfz85z/H7NmzsWHDBkSjUfzmN7/Bhg0bsHz5cixbtizbYyQIoshQFRLkKICH8P7OCNZsazHcJ4B1hVx9IhxaV47qsoDxopeucPN6PKaYHSf7sIvZ2brvAGb/5g2cf/Rw07bdyXo1rIVCkxhMVI1HzZYdubXDqkiiTxAMMc1ZllUkam5kapcdlQ6y/m2m1HOJFYul9wHKubTskMclo7vl+OOPx9q1axGNRjF58mS89NJLGDx4MFasWIFp06Zle4wEQRQZqno6qgKDbtO09wB+/PcPcMTtS3DxA++gmbGQWHW1jglxL/rLvG1RQUmwayYxO3ap5797bRM6wlHc/+Zm0xv9gUhS7HhsLDuqbCxTzI48GyuVem5G3N5pKn0kGjftT9ZnK1M4gZf8LLqxch6zk8XUc3akZNlJkLZlp6enBz/4wQ/w05/+FH/84x9zMSaCIIoclStE5i7IB9996B18srNDus4yG0tvJZCc+PT05HRPxevxMDE7zi07maaeA4zYsQtQ1iBdL7qxvB65tcMq9VxM53aaXRaJmosi9tayoyk+60MSrVa5jtkRA5SzRSG5i/NJ2ndLIBDAP/7xj1yMhSCIEoHPxiq8AGWV0AGcxezo1hEjZsfmeOI0yVl2bIoKOh2biDjJ6W4sPmZH8j1FNpY0QFkSx2LV9Vy0jjgWO7G46SJnU3xokvvVXEGZP1fRWtfb3JyeHBUVJK2TICNpfO655+Kpp57K8lAIgigVHBUVLISgHQlW1hN9wvcabixnMTviWp8XRruI9jQClMVMMTHTxqqO0YFI1BhzKrDaOkCZy8aS1NmxSj2X1tmxiXtRIXNjZTPLiLM4KhqBFqtlp1D/ztwmowDlMWPG4LbbbsNbb72FadOmobKyklv/X//1X1kZHEEQxYlq0tUciKB8Y2Vt0CdC3YOiz3/pugo8Hg+qQokaPR02RQWdjk21bXnAhy6mXYTX44HX40Fck0+DSjeWMD6Px5yxBDBWPQd1dpwiC1DOZiwKf78m/je3izBnkrFks4Jyb2F/LRSzkyAjsXP//fejrq4Oq1evxurVq7l1Ho+HxA5B9HGcNHcsVLGTnhsradmxmadMbiyPJ9Ubq9u5G0vM2LGqa6bvryLoMzqHJ8asj1uzLSpoFaDs9XikrhujqKBkTDJx5IREuwv5cbKBtDeWTSPQbBPJUZ0ditlJkJHY2bx5c7bHQRBECeGoqGCBmtctu54LhflSAcrpnYvPm0o9N2J2HKidTPolVYR82MsUWmZFiryooFzgmAKUvWZXD/t92SXJ1BUUllh2ejuHK+s/KdxYtnV2ekk23VhsXSdZeYG+SK9/e5qmUYVGgiA4uEeCJv1YwJYdiwrKcV7sWIkGFtEC4/V4UMmIHU3oEaUem01skGR1RYB/p/V4WIuUJGaHWda07wC+9adVeG3jrjQagWrc/yx2tWpUyIoK9tZioboXWXcfi128UW+7B2QzQLlQsh4LiYzFzkMPPYTJkyejvLwc5eXlmDJlCv76179mc2wEQRQphdYI9J0v9mHV53sdbWtlPYkKb/1O6+yI+LweBP2px29iMncQoGzz9i8TQ3r/Lh0vF6Bs3ge7i1c+3oU3N+3Btx94R1pnh83G0gWg0S4ii5adiKRre2/vH1khQXa/6XY97y25C1AmgAzFzl133YXLLrsMZ511Fh5//HE8/vjjOPPMM3HppZfi7rvvzvYYCYIoMsRWBTr5aAQajsbwH79fgfP/sBId4aiRgq1Cj4vZsrcTl/51NdZubTHWGa0ERDeWzRjMqedAiBU70XhWLDvdUfO5VYZkYicxog07WvGDv76LT3a2G+tVIkJc7hUagQaSlg/dAiTbS7quoIqkUJNlY/U28NaujYld13PRAlhIAcosZNlJkFHMzr333ov77rsPF110kbHsq1/9KiZOnIhbbrkFV199ddYGSBBE8aHOxpJvk0vYyrT7OyMIBawnXF2o/eCvq/FxczteWN+ML+6Yw63rbcyOx+NBkCmK1xOTBwuL2Fl2whIhVxHkH/NeD4yZ+dKHE41EP93VgVeuPRmA2rIlC1Bm3VJBnxfdPSkLjOyapGvZ6VcRxIFIV9Kyk1g2ZVgtPtjW2uvwCdW9qGsYUzaW8HO22ztks4KyKqOuL5ORZWfHjh049thjTcuPPfZY7Nixo9eDIgiiuFGlm4u4Ee/HPvjD0RjauqKW2+vWk892mwsPmsWO+RhO8Hk88HpTMS+y1Gqrsanokood3rLjYSw7Ol/u7zI+qy07/M+iZUd3y8Us3FjpuoKG9y8HkHDx6OO6auYY5f7TQRXXonRjCRWbrWK7MiGczZidIijx4DYZiZ3Ro0fj8ccfNy1/7LHHMGbMmF4PiiCI4oafPFLLxeeuGw9i1iLR3RNHW3ePzfaJSccqU8lnxOxk1ghUF0u6QIhIUqtlWGWKAeAamurILDui5uhfGTQ+Oz0XMWYnkBQDqdRz835Ea4kdw/tVAODdWPoxex2zw1wqdk9G4Uib1HNzGYBeDUdqlcuUfMXGFTIZubFuvfVWnH/++Xj99ddx3HHHAQDeeustLF26VCqCCILoW7DelrjKXwB34nZ4sRMzUr1V6IJCNkkYAcqiZSdN045uNUgIhBgiMXPvJxl2qeddEfOEWR5Qx+zocGLHoYFBZdmxSj1P37LDiB1BaPY6ZoeNH5NYQux6Y1nF2GialnZ2VneOsrFI6yTIyLIzd+5crFq1CgMHDsRTTz2Fp556CgMHDsTbb7+Nr33ta9keI0EQRYay67mwnRtvnaxAOBCJoa3bmRtLmqmkKCpoexqKYNeUZcdp6rn1hCgGX/u8HpQJMUqJOjtqsSOmmKvwejxcOnZQsOxI20Vk6MYKM6nnbIxzb9ygfDYWuzzxg3iNxIKIopWNrW2TiRDLpmVH1ZtOR9M0fOfBd3DJg+/0mdIxGVl2AGDatGl4+OGHszkWgiBKBKctIlxxY2mC2OmSu7H0tgpOup6b6+yk68ZK/K8LBDYmxQrbbCxR7Hg8CPnFmB2zi6ZfRUrsOJ38xDo7hmVHj9lx0AjUjgGVIQB8TBMrOuIakGHpHmWAstHZXtiv6IKzs+ykm5+V1Zgdbizm9e3hKF75eBcAYNv+LsOCVspkZNl57rnn8OKLL5qWv/jii3j++ed7PSiCIIob1VuzOAG6UUU5zll2osqsFz1F22oS01/mU24sZy4VcdrTrQYhLmbHSTaWcP2E74gByh4PpJYd0Y3FpsE7rdLs8fDnpcfsGKnnkt2kG7OTsnzFmJgdVuxkfv/wqef2biwRK+GZrmUnGoun1dHeDqueZgAQY+6jXe3dWTtuIZOR2Ln++usRi5lNbpqm4frrr+/1oAiCKG7sHrY6blh2ooIbSzWZ64G8ThqB+gXLTrqizcfF7KRTZ8fOjcWv93k9nJAB9DYP/Pe4HlgOT0UUTLowsRJt6Vp2DLHDuLGyJnaYz+y116+FXcyNqQwA13wzvXFl06oDgDs52X3F/k3saCWxo+TTTz/FhAkTTMvHjRuHTZs29XpQBEEUN6w1RWXlkf2cC2KCZUf1Bq2naFs2AhXSkp1adkTEmJ0eh26sDiHeyOPxIBqLozPZckK07Pg8HpRJApTFiZx19Tl1Y4mWj1TMTuJnWdB2ukUFg5wY5F2IibGmtTsOlatVVVRQxJSNxXxOd1x2WXbpYlf6gRXNbNmBUiYjsVNbW4vPP//ctHzTpk2orKzs9aAIgihuVEXNxMeu02DY3sCKnc5wzEgtF9HbKlgV7kvF7CR+zrSooFcQO2GHlp3trd3Y3R7mln3l3jcx8eYX0dxmfkP3emVihw/yBXiBl44biyVVZyfpxpJ8J53eWD848RDOzafvMHtuLOYzs1w/fbsO7ZY91NIcV7Zr9vB/f5LjMeJqewuJHSXnnHMOrrrqKnz22WfGsk2bNuHaa6/FV7/61awNjiCI4sRpgLLb2VhdPTGl5aYy6caytOxksaggwAco2wmm2vIAAGAN074CAD5uTrR6eHnDTvNxJG4sj8fDZQ4BakucFaKFSD+XVAVl+Xec8PAl0/HjM8dxdYhklp3ehLnYW3bs3FiCZYfZPN1hZdLN3gpVWr0OG5f2JYkdNXfeeScqKysxbtw4jBo1CqNGjcK4ceMwYMAA/PKXv8z2GAmCKDI0xZulGNuSbn2aTGAntc5w1CJmJ2EBcZKNlWoE6qyooLnreeL/QHIyf3nDTttJ5+iD+wEA3m/aL13fKamx41W4sUQXDSvwnP5OTL2j9N5YhmVH5saSCwhRkI0eXAUv0yw1EksVFfRzbqzs3D/y1HPr71gFsqcr4rNt4bSz7LD3+LY+4sbKKPW8trYWy5cvx5IlS7B27VqUl5dj6tSpOOGEE7I9PoIgipCY5E0ZgOmVN9tvtDLEAOWacvkxy5mYHdUkqp+LX7DsaJp1ITlxf4YbK2kN+ffa7fj3WuvzOHJkP7z80S6s+7JVuv6AROz4vDD1AktUULay7DgVO/I6NEbMjmQ3qgyn2vIAdjHuOb2pqH59emIavB6zxSVrlh1uufVYdax6Y2kSHWR1f6hidjIpTpj8YuqjRHSyY6cAZQkrVqzAM888AyDxpnLGGWdg8ODB+OUvf4m5c+fi+9//PsLhsM1eCIIodVTtIszb5X4sYoCySmAFmN5Hds0wvV7esgNYB6WKq3RhIFo0rBhYlag5o+ra3hUxF0uU1dlJuJ/47biYnTTaRbAiLuXSSy6T7EcVs6O76HR0i1eQ7QyftKT4smTZUbVUiMfNokqGueu5OpaoMxzFib94Fdc9IVe0qvst09PjLDsS4cWOXXU/lRppiZ3bbrsN69evN35et24dvve97+H000/H9ddfj6effhoLFy7M+iAJgigeNE3jH9IWAcr5qKCsv0V/+9iD8cC3jzbWse4RVdyOLgRSFZRT66zORDxNMUDZCbqVQzUxbtplblzq8ZgrKHtklh3JZG+Hqeie4NKT7UUlIMTroJ8ru9yooJwLy05Gbiy1BVBc+vyHzdi6rwtPrN4m3V4lMDN1b8likFhYy0426/sUMmmJnTVr1uC0004zfl68eDGOOeYY/PGPf8Q111yDe+65h3pjEUQfxxyEzK7jV7rhxuKzsaJGNlZNmR8njx1krGOtDkqxE+ODZD0e9ds8i3je+vcDaWQnhfxmscPu972mFgBAVSgVneCTZmOZiwqybpR0ApS5n728G0t2OVSp516Ph7sWfsHNx5KTbCyJ2HHS2kJ1/6Yds5Ol/aS+l/os2wVXV8nCbVtKpCV29u/fj/r6euPnZcuWYfbs2cbPRx99NLZu3Zq90REEUXSIb6OqN2jAHbHDHv9AJJWN5fN6ObHCTsQxRQyFUWfHa7bssMd55oPtePL9bcw6fj9GNlY6lh1d7ChchK3JNhgzDhmQOo4kG6si6DNZLcTJzwlmyw7/fdlErRIQXi/vbmPFpCh4WDdctsQOb9lKHccOVhSzsTHiuOz2pBQ7GWak8zFI5n2L5RWs4o9KhbTETn19PTZv3gwAiEQieO+99zBjxgxjfXt7OwKBgOrrBEH0kmgsjh2tucueaG7txtn3vokn3s38pUV8uFo1As1nBWUxfoSd3FR1T8QKyl5JzE44GsPlj76Pqx9bi/2dkcQ64cz1rwV9vNXFCj2miLPCSCbJYw9NiR2vB5xlpyLoQ215wDSRZ1JU0NQo08vXHJLtRZWN5RPaWnAp7abU+TQasFqgqnrttKggwGdkWVlT7GopZtuyw7fCMNMjHM8qs6xUSEvsnHXWWbj++uvxxhtv4IYbbkBFRQWXgfXBBx/g0EMPzfogCYJIMP+Bt9G48BUs37QnJ/tf+PxHWPdlK677+wcZ78NcJVn9wHajqGBcEDspyw4/m3k8qWV2MTtiBWUgNTGxb8l6RWOTZSfNmB2vJyUU2AlQHGfA58HYhmruOKzYGVQdgsfjsS4qmGE2li5Q9HlTthufwm3n85oDqXVMYgeSYOgMsOt67siyw1pEFJYiJ/uyu9/SRdauZdv+A3jgrc2JKuKCuMl2BedCJK3U89tvvx3nnXceTjrpJFRVVeEvf/kLgsFUt9w///nPOOOMM7I+SIIgEry1aS8A4C8rvsCxowdmff+dYXNGT7qYLTv5dWPxlp3Ug160MniQmHRjcU35pqsvFrueA6nJkz0nVYXlVFFBZzE7Pq/HcJ1ZuZxCfh8nbrwe3o01oDLxvM5OUUGAzeNJpZ7rlh3zjlSWHXGcLKIby2MUM9R6FaCsCuI1mr06EDs9cdayo77PWWJxzSS0ldlYmbqxJHFy5/z2LeztjODz3Z04fgz/7OjJcgXnQiQtsTNw4EC8/vrraG1tRVVVFXyCCfaJJ55AVVVVVgdIEERxIT64rYoKuhEYyY6nh+kuLU44GhKTcQTqdNy48F2ZZYc9ntEo1OTWSNey4zGEArt/0d0W9HtRHuBjX9iU+gHJ9PWsFBUU1I5obZFadqzETkBu2RHdjQk3VnpjlaFyr+r3pJM2XqxFxCpOhr1PemJx+Lz8uSpLHWQhQFn/vDfpUl32yW4urgsgy46S2tpa6fL+/fv3ajAEQeSbDAqYCYjPbUvLjhtuLI0VO5rxYJdZGfRlYoE+vbibLgpSFZSZbZK6Ixozv+2r3C1OxY6ftexY1DAK+PhUc1FcDKxKWnYsigo6r7PD/yxanmRCVtVvysdUSxYRf0+ybLJMkAkcID03FmsBtGqLwseDSQKGVTFimcbssMHSwvFicc10PIrZIQiiT5GFOcQ0yXWGo3jhw2Z0SSr8uu3GAhIBxEAiGwsADqorBwDMntQAf9IK0hnmx6rH4aTSkhPLuQDl5ATTI3EJqc5Sllotw+tlLDsWb+EBn5dzY4miZkClyrLDCDTH2VhCgLKHF2PS1HOF284ryRoz9itxN4o1fTJBU4jGWBrZWD2KlH1TNhazK1mjWZUXKVPLFefGEtYl3LR9L0A5I8sOQRClSRa0jsnasOjVRMPguUcOQ1WIN9+7ESogThh60LAuHl68+kRsb+nCYfXVxsTa1cPHLkXjcQThZRqBJiZmPvU8uW3MLBxUc3LAoWXH501ZM6wsLyY3lvALHT+kJjlusaig/LMVXg8wckAlN0aAOWfJd3wK35BPyBpjEWvz8KnnzsYqg7+MrFVG7uaUwYpE3o2l/k5EIizUlh3bIUgRLVXszzFNM2pNpY7v/EDxuGZY8YoJsuwQRBGSK+9PNiw7qrftf7xnrh7rdgVlAOjq4dsOVIX8OKw+kcEUULix9MkgVUE5sVxWVJB9a7Yr2ObUsuP3egyriJU1LChYdvRz/MOF03D5KaNx1uQGAGaxE1UE2lrh9XjwrRkj8YMTD8Ej353OuLHU+1GmnltYdkzWIE92LDu9raAMiNlYZsGkw8VZSSxzuYzZWfX5Pky99SXuWJladt78dA+m3voS/rXmy4zGlU/IskMQRFaxMr2bTOp5EDvdSSEjc6noqdEmsaO7sSTBzV5PYnKRBSjHNOuMoXQClA03kcUOAz4vJxp0UXDGxAacMbHBWG4qKigINCd4PIk4mxvOGg8AWPFZIlPQKkBZ5RryepzH7Hg8bOB3b9xYqc+8GyuzmB2ZdeyFD3fg4+Z2jBxQYaxLR+xkw4218vO9aOtOWSqjsXjGRQW/df8qAMCVi9fgnMMPymhs+YIsOwRBGIgpyZlg9Xw2tZJwo12EcFDdjSVzU+guEzG+SJ8cxEaggLnAXY/gxrKakJ02AlWlnosE/V4u/sUq+4klprByWGGqoOxgfMqYHY8Hp09IVOdn210A5nNgA5Sz1xvLfP6yazf5oFruZ1UFZX1/lz78Hn798qd49ePdxjqZGyv77SJS3wtH48I6s9tKFkdkRTYswG5Dlh2CIAxy6cYCzKnn3T1x3PXSRrR09WD+sQfj0EHZL10hPtjFmB0WVTaWHnRsxOx4RLGjGectFuizmpADTgOUFann5v0ltikL+BCOxi3aM/A/i9Yop2NiMQcom/ejGo/P68FXpw5FVciPSYKgEGN2slVUkIWPt9EtO+btHr5kOlZu3ouFz32EL/YeQE+UzcZi98F/r2nfAeOzLD5HFTOTccwO8zkSFeNz4qbjpdsuYmT/CvuNCgyy7BBEEZIre0jOxY6w6tWNu3DPK5vw0Iot+P1rn/X+4LLxKNxYsmBZnyF2hABl3bIjC14VgmXZt+RYXLO8Hk6vt9/nLEBZF096kLKV24glk95YonDRdZt+vWW7CagagXoThQJPG1+P+poybp2srYceK9WbAHdZlWEgdX3FTDYAqK0IYNbEBlQEE3aCiNKNJYr6lHiWubFU90im2YoaZ9nhhXssrpkrKDu4kOx+RjCB6cUCWXYIgsgqVs9N8dG9pyNsfBatKdnCHKBsYdlRxeyIlh0hZgdITTB8gT7rsTl9o/Z5nAUo6+4rvdaO06QZfczsJBnye00ukIDPY4zZts6ORJKrCvVZFZKWtfXIhmWH+yorVByknutZdOzvz6rODns/SbOxFPdBpjFJVm6sdAOUn/lgO2rLAxhSmxKheiXuYoLEDkEQBtmJ2XH+gO5gAydzlIcuigN9YpfFj+guE2WAssSyI8bscI06NWvLjtPKtV6v8wBlIJXG7TRmJy6IOQCoKQ9gd3uY287v9aInFpPuw1GdHQvLjgqZKM1GI1AuZodZbjSKtRhTyKeLHZUbix8Yez+5kY3Ffk0es+MsQPnLli5c/uj7ABIZfal95D7WLtuQG4sgipCcPWtcdmN1ML24clXXTDVhyAOUk3V2BDdWjxigbIrZYVLPmYkkpmmWvyuHITvwez2pOjYW+wv6ebGjdmPxP0clrqeaMvO7MCsQxX04qbOjtuyobzzR3cgGKKs6lztBFqcDpISAKpgaAAL+xDpO7HAByrxVhndjSQKUFTdJpvqf/T2KMTuALEBZfvx9HRHj8x7mswt5BVmHLDsEQRjkoqigsW8PIE6BvNhxx7KjI7My+BQByoYYEBqBAjAVuOMsOzYxO6eOq8fUYbVYu63V8hy8Ho+jIncpy0562VhxTcPj727Fix82G8tqywOm77F1gcR9pGKKEj+LLhiPRy1qLF1Gkt5Y2S4qyH7uMdqJqJWofp3DygBljbvv2BgweVFBVYByhpYd5u9MjNkBzOJGZVVlfy2sYHMjizLbkGWHIAgDWVBmuqge0F6Px2TlaGfcWLnqRagSOzIhoE9iXT3y1HN9UrCy7LCiLRa3tj0E/V489oNGmzPgU8+tEN1Yqu+Iv+doXMOP/v4Bln68K/l9L4ZLMm5Ya4e4D5NlRzhxNrBYxOrcrFPPexOzI3dj6b9rUWSxBCRuLHYsezrCeI4RjuwtKHVjKcyaGYsdCzdWYgz8Mpn1B+CvfTcjmsiNlSYLFy7E0UcfjerqagwePBjnnnsuNm7cyG3T3d2NBQsWYMCAAaiqqsLcuXOxc+dObpumpibMmTMHFRUVGDx4MK677jpEo7wZmiAId1A9CD2wdmPl6m1RbdmRFBU03FiqCsrm74oBylwFZU0zGoSqcKIvfUxvLCv0AGU9G0ttSeF/Fn8v9TVluH72OIwcUMHVvWGtHXZ1dsxiRx0wbeXOMxUVhPmaZ4Iqe0ovM+C3GJTuLuRSz5n1l/zlXfzX396XfldmRVEJ/WxkY8kuUY/oxlIchxX13RESOxmzbNkyLFiwACtXrsSSJUvQ09ODM844A52dncY2V199NZ5++mk88cQTWLZsGbZv347zzjvPWB+LxTBnzhxEIhEsX74cf/nLX/Dggw/ipptuyscpEUSvaG7txpqtLXk7fjbcWKoHtMdjjrFgxY5bAco6ljE7JsuO7sZSBygbbiyuqaZ9XIlVvAo7VicVfdk6O+I4Wez2VV9dhiG15Vh23Sm48rQxxvKgpDqzMUYxQFk4b4+FZSedmJ2EG8s+fskObnyc5SVp2bEQl0GfORvLafhQRBqgnN3eWHbfE5vJqooKspegO8paLDMbVz7Ja8zOCy+8wP384IMPYvDgwVi9ejVOPPFEtLa24v7778ejjz6KU089FQDwwAMPYPz48Vi5ciVmzJiBl156CRs2bMDLL7+M+vp6HH744bj99tvx4x//GLfccguCweJLkSP6LjMWLgUAvHjViRjbUO368bNSZ0fxIJRNdKz5PFdNQVXBn9JsLJ9c7OhBx7IKyqn4Ebllx27icRKL4/M4s+yIMTvKAGWb19zBNSHjcwXTvFVm0TLGaFNnx8qyY+XGYt1JQZ83KZr4Y2UCe7/pwicRY5VYZmXZ0ccUUbixrJAJi6zH7Nh8rcdhNhYLa+3sjUUtXxRUzE5rayJIr3///gCA1atXo6enBzNnzjS2GTduHEaMGIEVK1YAAFasWIHJkyejvr7e2GbWrFloa2vD+vXrpccJh8Noa2vj/hFEIbF2W4vNFrl52GTFsqOM2bF+CLtt2ZFXUE4GnvYIRdfE1HMPK3b4NGj2eHYByvr37USmz+ssQNmcjaU+phVsYb+KICN2GAEg7kMsephWzI6lZSe1ThcZ2WgXwaLfeqwIsMzGSl6HiMKNZYWspo1KtGUq5uzuOTFuSFVnh92KfQFwo6ddtikYsROPx3HVVVfhuOOOw6RJkwAAzc3NCAaDqKur47atr69Hc3OzsQ0rdPT1+joZCxcuRG1trfFv+PDhWT4bgkgf9sFWznSuLjaUbix4LCcE9wOU1dlY6gBl3bKTWicWuIsKb/tO5gU7V5YvWWHYTu9kWmdHpJ617ARTDoAAl3ouD1BWFRX0WYzfSsixolQv5pedmB1zgDIrAlTVngEmZicDy47MiqKy7ORKVIgvFsrjM8u5bKzi0zqFI3YWLFiADz/8EIsXL875sW644Qa0trYa/7Zu3ZrzYxKEHR1MempZnsRObrOxrC07hRCgbLixhADlz/ck4gj1MfKBurxlh3Nj2TQCNfZho2L09XbWHT2WZFBVQqzUVcjd+Ha/ZdayU8mJHXWAcqoOUPJ8hdNOVD7OxLKTOqZ+fH37f6/djll3v45Pd7Yrv6+C/bVs2dsJTdM4sWNl2QnaFBW0IhqLQ9M045iA+h7NVOvYCS/9/tZ/hyrLDrsfVuyQGytDLr/8cjzzzDN49dVXMWzYMGN5Q0MDIpEIWlpauO137tyJhoYGYxsxO0v/Wd9GJBQKoaamhvtHEPmmravH+Gz3Bp+rZ01uA5Q9lsG6qrfLXI3HKkC5W6hN8osXN+LF9c1Mb6zUOjENmgtQ1pyVvbOz7Pidip2kxeH8o4fjl/8xFT848RDpdnb3F5uBVR5Uxewo3FhGzA5/5tbp5eqxiDE7QEqUL35nKzbubMcdz3+s3oECdnw/e/Yj3Pr0Bt6NZRlHJGsX4ey4PTENdy/5BCf94jXctyzRD05ZQTlHbqzOpNjRLcjqdhWpz909bIAyiZ200DQNl19+OZ588km88sorGDVqFLd+2rRpCAQCWLp0qbFs48aNaGpqQmNjojZFY2Mj1q1bh127dhnbLFmyBDU1NZgwYYI7J0IQWaCtK2XZsepVk1OyEqCscmPBMrAhZ5YdVYCyNPU88UiUfeX+NzdLKyibigpyTTWduTfsRIx+PDtRpE/ClSE/vj5tGPopehjZubHYGjuVTIBygIvZ4b+TqrOT+Fk8a7Y+joh1NpYsZoffZunHu3DRn9/mrA92iON7cPkXxqTv96rjixLjkBUVdOjGisdxzyubAAB3vpAotaISD7kKUO5MZkHqQlb1vGHH1dVDqecZs2DBAjz88MN49NFHUV1djebmZjQ3N6OrqwsAUFtbi0suuQTXXHMNXn31VaxevRoXX3wxGhsbMWPGDADAGWecgQkTJuDCCy/E2rVr8eKLL+LGG2/EggULEAqFrA5PEAVFW3fKsiNLT3WDbPTGUokLOw9ZzgKUFddSXlRQPcj6mjKb1HM9ZofPxnIyL9hZWnSNYefuYlPDrbCayH927iQcVp/KBGTdWH6LmB37AOXMsrG4mB3BjcXy+ie78cTqbcr9iMjEiT7pW7mwAHnMjtO/2HR6Y2UqKuzeG/QK4bq7XJWNpXJj5SpzMpfkVezcd999aG1txcknn4whQ4YY/x577DFjm7vvvhtf+cpXMHfuXJx44oloaGjAP//5T2O9z+fDM888A5/Ph8bGRnzrW9/CRRddhNtuuy0fp0QQGcO6sXoUFU2LAWs3lppcWcbVlh11gLKMhpqQsS+ZZcfoes4GrTrIxrI7LrveLv3cSqyxWO3m69OGcT8r3VjC5TMHKPN4LCw76cfsyLc9EI7i+n98gP9LuoeskP1adKucVXByYhyJAWQSs9MTi3NtN9jjimQuKmzcWLplR3djKev8qAKUi8+yk9c6O07MfmVlZVi0aBEWLVqk3GbkyJF47rnnsjk0gnCdtu78u7HYOUfTtIwClpUVlD3Wf/O5igNQucd80q7n6vOtqwimOmJLLBxGgDLnxnKYjWUrdqx7XemIk6gKK3EhXgPWssNaAMzZWIn/9d+/KWbHo7buWWZjMddaz8ZS3ZfvbtmPJRsSMZs/OOlQ5T5l4wNSQjUTy0462Vg15QHs6Uh1lM9213M7kaSLHXvLTuozG7NTjGKnIAKUCYIQLDsSseNGBgT7iM/0cCqd5rWx7ORK7OhvzaKLxypmR0Y8nmru6JNYdvThs+fh3I1lI3Y8+viy48ayMlyIx9ALFALWIlwMUBZ/2ZnW2WF/T0FFzI5OZ9h5myDZr8VoAmojGntTZycai6O2nLczqERNpn/zdmHxYoCyMhtLkXqeJy97r6Cu5wRRINjF7LBaIFfPGnbOiWkavBnE8Kjr7FgLqFyJHX0iKfN7uckp3ZidKCN2vJKYnWc+2I5wNMZPgFp23FhG6rnDAGU7VKJDr1Cs2lbVMBLg3Vg727rxftN+br1+nbwes8vSatg+ScyOavzpaAOZxU9359i5C3ULWoTLxnJq2YknSwJ0Gt9TxZVlauB1+qekuyhV7SJYEdZV5KnnJHYIokCwy8Zyw3TMBijH4hoyKfdj5cayImep58mJpCzg41yFMuFgJTqi8bgxibDf1b/y0IoteGjFFhw6qDJ17LgzYWpn2TFSz23cK07Fjuo07dw3VpYd/ZrENQ3Tf77UtF4/Ra/HI3FxpRugLN/WyqLx+ie78eamPbhu1lgEfF7pllGJm1JGQNYI1GnMTlxDbXnA+LmtK5r1ooJOv2ZYdhzU+Sn2bCwSOwRRILTaBCi7XdtC9jzbsrcT5UEfBleXmVcmyTxAOceWHUa5eT3yDCCrN3rWsiPLxtLZ3tLNHTubAcp2lh3HbizFfuwsGlZZgmLXcxH9HGSHto7ZMQcoO4klE2POLvrz2wASafUXzhgpD1COOQtQDsoClG1HpB8jzp3v3s6w8h7J2I3l8Hu6i1Jp2WF+l+wui7ERKMXsEESBwLqx5DE77OfcCAPRjcWyrzOCk37xGo75X/NbO4vSsgP5uPXnfq4DlNnYE1VMhrj8tnMm4pCkpSYa04zGj6yoECfecJRN0c1OgLLhAspSgDK7F9Z1Z2cZikTVdWxSFZTl6/VzkAkVxzE7fuuYnTg3IcsHsm3fAQDye9FpgHJAUkE5nQBldmz7OiOO2jWkg9OxpOrs2KeesxSjG4vEDkEUCGyAsjxmxwU3FvOMF4/3+e4OR/tQlr6H/O1Xt7jkOkCZ7TemsmCIy2vKAjhr0hAACTO+PkY2HVvclTjhOmoXYWOo0Mdlm3rudxZjxQoOViDZu7EsLDses7WDP6a+neS7ltWVZW4spR/LQCUg9KWy1bo7R1aWgEUXu+FM3FixOHev7+2MIKZM/Xa2TxGnXysP+I0xSfej2FExVlAmNxZBFAjtNqnn7nQaTk0iYgAnO0FapaVblr6XrAr5vTgQieXOspO8biFG7KgsKeLygM9rLGPFaAWzLyuPiuN2EQ4DlJ3G9qRD0O81snPsJnnLmJ3ksTsUGVE+j/ocrNxzfonlye46APZWCdlvRrfs2NUrYi07T76/DYOqyhxbO6ISy46yqGDGlh1n25UHdTdWepYlitkhCCJjOiPWYkdz2U9uzphJTQDRuKacEFQPwlhck04wCctOT+4sO7E0LDuCG8fv8xjb6mI06PNy21lNvDGHRQXtU889xniscSZ22ImZdcnZxfxYZmN5zKJQZ8zgKnz7uIOVI7TynvklRQXVhp3UedkFvEstO2mmnn+2uxNXP7YWAFAdcjadipad7h610M+8XYTDmB2/86KC/P4zGlZeIbFDEAVCR0FYdpjjCQ9gVh9EY+pMLdXLP7s/n9dj/Gy4sXIeoJyawFT1dEQRFPB5jAyo9mRMVUWIP3GrYFmndXZsA5R9zgKUndaAZH+1rMCxswzVlAewtzMiXadfUnHevnPuFHzj6OGp7XoTs+NTW4cAIYjWrrmlLEA5zdRzFscxO3HeshONaalaUD6vERcGuJCNlXTHqgLPVXrR7WdRNqCYHYIoEFjzfyRqHbOTq0cN+0ZolR7cY1GiVfUgjMbjxkOYnbRDycnWaXxLusQFUQWoJzOZG0vfVk9brxBUntW8qGnOJkGnlh07UVRTFrBcb4yLuYP4mB35lPDgxUdj0kE1WPSfR6b2YWH54xAW66fKnrLVecljduTbsveeslBf8n95BWWNO44KmQWs22GLl2gszllSIoylR7SWZh6zY/9Fjwc4qK7cGJMM1b1bjG4sEjsEUQD0xOJcsKO0zo7QhiAXsM8wK7Gj8vEDkDbLTCxP7Z9dx8bS5OK0ZAHKqslVnGz8Xq9hBdLdM2xwMmDvxnJUQZkZjz4BXZx0+7DjVY37kIGVuPv8qRhU7az5MTumoJ/tai7f/8ljB+OZK07AhKE1yn2qrE7iUv1cgw5dgbJ2Eartw0xLA2WgfHKxbK3TRqCy6+T0b7InFucqELMxPKLYzDhmx4HuGjWwEjXJej+qv2el2CnC1HNyYxFEASCWuZcXFUx9ztWLFftws3p4q94E2e/VCS6PaDxuvHGylpUQ85Yci2u21ot0iRtuLMayo5jMRPdWQBKzUxHkH5tWw3XcG4vZx+/mHYmygA8+rwcPvPVF8hjWYmfOlCH42hHDpOtkZOrGskKVUWWqyMwcVxf41l3PndfZ6WZS49XZWInlUsuOw2wsp8UbZSRSz/kXm6jSspOhG8uBZWfM4CppCj1LKQUok2WHIAoAMYPFroJy7jKXUp/F5xl7fFXFVXa7uoqAsDy1f3ZyY0VILs5Lf2sNcTE7zlLPAz6vIYz0CrKiZccqZieeQVHBUMCLsQ3VnIjy21h20heIqTEFmO/aBebye+DPS2XZEYemCzfWsmOZjSWJ2VFtzlp2VFYRw7Iji9lxmI3ltHijjNauHs6S0hOPG2MVRVTmdXbstxlUHTKsnWx1ZBbVrUtihyCIjBDFjixgUGwwmQs0C0HFiS0LN5Zh2akIKtexE1gZa9nJRcxOcp8VgZRFxmmdHTYbS6dSFDsWx05koNnDumVkKdp2qefpWmRYN4SPExOZTwkqwSUOWReHrGBItzeW6nTZZpVW2Viq2DCn2Vi9uU5N+w5w4+yJaso2FZn+OTj53qShtYZwPxCRix11ZmVm48on5MYiiALA5MaSBDu6UUGZEzSaKHZSn50EKPerMAfL6g91L2fJyK1lR9/nmPoqYxmb+cYiTjaJOjv8xGZ2Y1lnY6Vr2fFJhI2+TCVq7Cori7BjYs/ZPrVdjdKNJchBfTNW7DiO2bGps8N15ra4R1W/Ej1wOGBzPQO9sOzE4hq+2HuAO2ZMZdnJQer5ov88Eu98sQ9fnzYMu9rDAIADEfnfgzruqfgsOyR2CKIAaO+2d2PFXHBjsXsVH2hiuqzIaxt3YWNzu2GSry2XWXYS58W6LcqEmJ1so+9zYFUqeHd7a7d0W3PMjtckMEwByhbznqxdhF1vKFkPKTs3VtqWHWZMrHCwi1WxQhmgrMjGchygzMbs2AQod3FiR74/q070KctO+gHKmcLW3REtRpm6i6y+N2fKEMyZkqgKXsG0i+iJxU1iq5TcWCR2CKIA6AzzZmTbmJ2cBSinPotDYMWPbHzffuAdAMDRB/cDwMfIpL5nztQK5FrsaPoxgeoyv0lYsohv9H6vxyQwKtLKxjKLRrs6M4ZlR9JsVB2zk55IYZ1rfFdx55O4ON+phiDGNHmlbixnqed2MTvs7aMqlKdBXbohmmM31riGanzc3M4t6+GyscQMxkwDlJ3BCvcDkRhqy52JrWJsF0ExOwRRAHSEE2nN+mQji9lhJ0033Fjig46z7Fg87LYkTfSyN33ZQz3gTcXF5DJA2ef14qTDBllua1VnRyfdAGXxjGRbSy07kvVKsZN5fDK3z3QClM1jULmxeGRixzIbK8N2EXzHbvazehJ36sbyeDwZWXfGNVSblvUwdXdEy0qmfw5Ovxdk7u8uSdyOslZR8WkdsuwQRCHQkbTs1FUEsbs9LHdjxdnP+QhQTn12knoum5j174lBp16vB4hrOQ1Q9nk8uP2cSWjt6sHXp8nTtM0xOx6TAKgMOn9sxjXN0du51LIjWaay4PjSFCnsZM/9LnqReu40QFnHeTaWOfXcyTDZ+1f8FahuM6cByvpYemLqLvAyZHWQojHNGF+2srGcvgx5PB6UB31o745y7Wp0SqmCMokdgigA9ADl/hZix5XUc+awlqnnFn40feyyt2/DjeXhrQl+rwcRZG62t4IVX/0qg/jrJdOV24oxK36JZUd0Y1mNOdEbi18mm9f5NHPzhG6IHcUkn27MDjskWbZTJjgOUE4egrfsqPfrl4wvXcsO69LSoK59pItxJ9czmGxga7sd0wKiTNJjJcJZdsRsrEzFjvNtK5JiR2bZUd3bxRizQ24sgigA9NRzvTaNLBuLN8vnZhys08WcjWWePFZv2YdvP/A2PtvdwaxLxciIyKw+AZ/HED92zRsBubndCitLk4jMsiN+T3RjqWJDAN2NxZ+TOPkDgvjw6HEpHtMylaCw65llHlfqMyvw0snGGtavwtEYVHV22GKSVuPnYnb8yc8OhsneS6KIVxUEUKWAyyhXNYcTYEWdTOxEY3GjlEPWsrHSaCijZxfKhJu6XURGw8orZNkhiAJAFzv9krVp5DE7qc+5MiPzAcpWbqzED3PvWwEA2LrvXdM62cSsCwMuTsTrNba1s1g98NZm3Pr0Btx7wRE4e+pQu9NJ7FNTiy+R2nI+XV4WsyNadqxaZ8gqKIuNREX0pp/sYfXr47Snlx2sxaCmPDUNOLHsPPrd6fhsdweOGdXf0RhEHSOL2bH6rctidpzM5Vx7FYsyCiypRqD218Gp2GGtNTKx0xNLuW+zFrOTRh0c/Txk6eeq4+fCAptryLJDEAWAXvelX2VC7NilnufqYWMVBM0eUxzftv1dqXWS9HKdqMTKwhbusxM7tz69AQBw5eL3LbdjiTEBynb0r+TT5WXZWOUB/h3RyhoVi/Mi9eABFXjg20ebtmO3kRYVTH7MVgVldsT9mOKPTtw3x44eiAsbDzYtV7uWPNKf2Indyi0ii9lxYgFkt+GsolC7h1KNQO2vg0y4yAgxvcfKJBmKlo1AM87Gcv69yqT4LnU3Fll2CKIA0N+qDDeWTcxOrh421pYdZ9lYbLPPZNxx6nuS1POgz7llxziGo60S6P2XQg4KwYlv6x6Px+TSqBQsM1ZjZttFjB9Sg+evPMF2DLI6O55six3md8lWuu5VNpbjCsqJ/9kAZStLhJ+zAjq/T2IqsaOpLRbpBCiL7kwVAX9q/DJrUJTpjSUK8szr7DjftjzpxupM042laZplJmKhQZYdgigAdH+57kaRiR0uU8qFdhHiAzNmYdmR4fV4MG1kP25Zyk0gt+xkW8TF4pplcKiI7OEtTkDihGU18bLtIqz1iDk7ysNZdvQAZflOetMuoo5x3fWmWJ5qCKLFR+bGsrqffcyYPEZsl/39pxY7mlIty+5PFc7dWNYxO1ydHeG4mbZlSCewuULvj5WGGytxjLSHlVdI7BBEAaBXfk2JHU1SwTj1OR2ffDqwDzdReMgsNFb4vB7ce8GR+ObRw03f8wrZWN40ApTt6InF8V7TfkRjcYSZLthOLDuA2dUgTkDihOXUsmP1Esxeav1w7GH1SV7liutNu4h+lazYyXxK8Hg8UsEjLpIVSLRy17DXX//oxLLDBSgLmYzKOjuKQGEZTt1YQU7syAptptxYojB0Iv7D0Rjea9qfcQJDhUV/LKvjF5sri8QOQRQAXYJlBzCnd7vjxlIfQ5aNpSMbjc/rQUNtGe6YOwVDa8uS3zO7sQJMXIxjN5bFZrc+vR7n/W45Fj7/MbqZLthOJ6fqMj5IWXRjifuxy8bSL46TdGkgJWzY7fVPqjm4N6nnvBurd24JmStL5cZiF1v93tmYHf2a9MaNZdWcVbdYOsrGcujGYn+PcstOHJGkq1Xsph7XNKzb1opfvrhRmYX4o79/gPN+txy/X/YZ9z2nWDUDFUUoe58VW60dEjsEUQDoD5qaMlbs8JNoXPHwziZcxpdFzI4oxGRmc64ono+fpMSqvXaxGFv2dnJNHq14eGUTAOD+Nzcblh1ZCrmK6hAfyigKCdFCZOfGMiw7FseU7YETO7q1J2sxO6nPbIByoBe9sQC5oFN1PWfddFYTJ2/ZcW4BVPWSi1pZdvRAYQfXocyhpZD93cjEzoFIzDiuqYaTpuHs376J3766Cb999VPp/v+1ZjuAxP2uk87ToTKkp56b3Vji74XLoCsurUMBygRRCIhuLEAidixcTNnC2rKT+ixWUJZN+KwVQo816ZEUbQv4PJYByu817cd5v1uOw4fXOTsJBt2yU+Z39hYOJPpnsYiuI3HCsiqwGIunrptVMKdMLLKb65N8LlLP68rN3ekzRW7ZEWN2Up/HD6nBRzvaMOOQAcp9sgJP77fmzLITZz4Llh1lgHL2LTuc2JHch21dPal9mlykqc8f7eB7aonUMPdtWgHKASs3Fv8zW0ix2PpjkdghiAJAN1FXlfnh8STemiKioLAIHs4W7CQgemfiwtsxt04yHlm7g5Rlh08ntgpQ/sfqbQCANVtb7E9AQLcGhRy6sIDE74DFHLPj3LIT11KxV5YxO5JlUjeWMkA53UagKWoYsaPXe8omqpgdjwd45orjEYnGbYXDFaeOxu72MMYMrgLgMGaHEaExwQVrl3ruKBvL4T3F3j6ymB09Cyro85pEVjr98NjfYzpmF92aJHOTicdkrZrFFrNDYocg8oymaYZlpyLoQ8DnRSQat4zZyfStqiMcxVPvf4lZExukfXr4zurO3VgyxMKBgLy6st/rsXRP9CZoVhc7sklGRVVItOyIbiwxZsfOjZX47DRmR4dPPbcOUE43i0rVG6uzl2JHdi1Ey47hkvMkXItOLCTXnjGW+9nJ/c9ZKVmhHlPH7DhtBApkFgNm9R3ZdUjn75y1Cqu+JhNoVgHK4vGDnNhxPLSCgGJ2CCLPsEXFygI+40Eruoo0xcM7HW7993rc+NSHuPD+VdL1rL4xZ4OxE4Z9Ohg7iYqxJmI/Jv1n2XmJQZvpYLix0rDsmAKUhWwgUVjE7AKUoWfaqI8pe0mWFxWUfz/d+jiql3JZM8h0kN0X4mkbMTu9OU6a2VjiZ7uYHSduQTuRdts5E3H0wf3wk7PGG8usMgLFeB3AXlCw2YZsvJ8o574yZQimDqvF374/w7QPvc7OAUlMnMmNxdVGKi61Q5Ydgsgz3ZHUBFER9CHg9wKRmMl6wqWeZ2hCfnF9MwDg42a5/5+3HvHr2EP2OHjQcenlJrHD92PSJxfZJJZuPApLd3IycJp2DgDfPHo4/r56GyYfVJscH+9yEy0VTi07sp5YVqSTep6uZUdl6eqtZ8LOnZn4OfF/b+rR9TobS/F1fQLPRm+sixoPxkXJStN3nz8V1aGApSiViSf271E25H2dEen3Rf193OiBuOCYEdLjViWLZHZ095jWiS88QcaqSW4sgiDS4kBP4m3a7/Uk41cSD0RzgLLaxeSUsoAPbd3qt3erY/DZLelZdkTBws4lfq/XMvXcaRaWjLDhxnJu2Tnq4P549b9PxpBkurxdV3CrN9xEzE7ic7oxOx5ZzI5irkzX1XfjnAn4bHcnvnPcKADArV+diL+u3IL/Om1MWvtxgqo3Vm+q76YrdsSyCaqJWr/HnYzNacwOAHztiGEArJvYVgbN07Fd8c497Smxw1rVxLOzuj9qyxPZeK1dZrEjXueygNeIKSwyww6JHYLIN/oDUH8z09/SxcJ97KSaaVFBu0mfPaKpNxY7YTiJ2ZEEKKd+Zq0lqa7nskmow0Kc2aG3ikgnZgcARg2sND6L1Z5FrCw78XjqnKyLClpfT/1yqSw76dbZGd6/Ai9fc5Lx8/xjD8b8Yw9Oax9OMbmxFMvTId0KymKfLNXl1vWCky7yZQ6zsVisLHAyy45+/wJyq9uezrDxmbUEi/eT1XH1FjWi2Nm67wDe3bKfW3bK2MH4YFsrYhrvCtywvQ0Bnwdj6quVx8k3FLNDEHlGDwzU3xT1t7CeePZTz+0mfeveWKnPTgKUvRaWHb8iZkcmHHqTIWQEKKeRei7CB1qbJw3LOjsa2y4i8+ldd4GpJq3eBHHnnBxYdtKN2REzCVV/Prp1xFHMThqWHR2r/cpidlixI2NPe0rssNmb4vMhaHF/6GKn5UAPJ5JOuPNVUwbkvOkjTC8m7d09OOueN3D63a8XdDp6Af+FEETfoJvJxAJS1oOeaG7cWFZY9cbiJwwnbqzUZ1PMjo+3llgFKPdO7KQfoCzCN6I0PzLtYnacpJ7boX9XJWoKWeyIIs+ooNyL6+EkONa6grJ1gLKTsaVrLUzsN02xw7hwZSPey8TssC4v8fFg7cZKiJ1oXJNmZOks+s8jMaAqZFwb/Zru70xZhJz0zMsXhfsXQhB9BMOyk/TZB4Q0bR0uWFFLr9mfDmvhkH1fla4rrnOWeu5lPosxO84tO+02bixN0/B/yz7Dkg07TetSdXay08073XYKbG+s3lh29O+qJq3etnnIJeZsLPnydHBUQTkufzmIxjRlvIk+WTtxY2Vi2bGiQhKzw7uxrF28rGtZtOwELAL0ywM+w/LTIonbAYArTxuDOVOGAEjdi7JYNBI7BEEo0WvslCcn5IA/admxCFBO/Jz+sdhJn+0bZezTIuMr7dRzy5gd3jXUG8vOe00tWPj8x/jeQ++a1unnKNbGSQf2bTzd2JgPtrXizhc2mvYjYqdbU5YdhRurl20ecom5gnKqqGCmpNsI1JyNJf9+TxpurN5YC3XYw2TixmJd3RGrAGWL8/F4PKg1XFkR6TayZAOZK91JLF++KNy/EILoI+gByvqbXSobS516nvg5/QcLaxlok6WasvsXHmbsj07erC3dWELMjtHgMYMAZTb9lsXjSaWeZ+JykJFJGvyO1u7EeHpxXH0iVMVeFLRlRxWz04sr4qiooELsJLKx5N/RJ2snXeSt6uzccvYE2+8DfFkDeYCydSZiT5S1tlq4sWxKL+gtQ2QZWYBYBiHxv35NuZIUBWzZoWwsgsgzXUJ6dCoby86yk77YiTBvim1dPaivKePWWxUujGnyB6sKviiehWXH5zHEUCYByqy1g71mHgDhLMTssKTbloHFsqigIobkullj8fnuThw5oh+AYo3Z4X9OVVDOfJ/pWnbM1cetLTtOXI4qN9b6W2cZzTXtCCarpQNARcBvug8idpYd5n7nxY6YjWV9f+hxO60HeqTflyUb6JeXtS45qb+VL0jsEESeORDhA5RT2Vjq1HOg92JH9hbHNwJVr3OUem4R6+JVxOyYzjGu2YodtsIyG9/j9XhSlp1euLFYZBaUgM/jKIYpk+yjBaeM5o+leENPt6igu4gByr2PUK6rCHDBuTL4RqCp5dG4VcxOYoWTmB1VZW+nQgfg76fKkM8Un2brxmLFTlT9t2t3fxgZWclngvjSIXtx0Z8HrPAUkyoKicJ9HSCIPkJ3D596rpu2zdlY/PcycWOFmYejzI1llXrOu7EcWHYYsWOV+hrwqgOUnbQvYB/E+5iYA48ns95YVsgq4D72g0YcPrwOV5w6WvINdpzqdU51q2rS6k0ad64xu7GSy3uxz9/Nm4YjRtThoe8co9yGFThiB3S7rudODHiyIoDpErBxY3Uz2VGyMUdUlh3BQmT19weYCwuKf/vsvat/1sUO++Lj5LmQL8iyQxB55kByQjeKChoTv40bK4PnCu/GMgsJPvVcHaDcE9Ns03/Zt2MxQJh9GFulnjtJO2cftmyApQeeHLixzFP0kSP64akFx+G1jbtsvp15gLKO3aRViIhnnY0A5bEN1Xjyh8dZbmNt2bFOPXfkxgr68PtvHYlYHFjw6HsORm2GDRyWBSjbiX3WosjV2RGeDXZuLLbWTmK//A6klp3kJuxzyomFM18U318OQZQYXcneWOWiG8ui6zmQWa2dCBPwKA1QZnZpFSMUjcdtg5RZN5aY+s3u2u/zKAOUnVRPZh+2+5iaH9F4PJV63otmoixWAcp2wcu9iVHRKeTYHBXK3li9su3Yw7qAYkLMjt2fjtNA9DMnDTFSsjOBdUuWB8y2B/ZPTBbXFVVYdqyOIyMVoBxJ7te5GyvKvQQVrmWn+P5yCKLE6AgnJuiqpK/fKCooBihnI2aH2acejKjap/jciguWHTuTtZez7PCPGnZfAa/XsJiI5vNOiyJnOuyDeT8TxxHXUm/GubTs6NjFeVitdppNVYxix3zeiQXZEH9WLH5nK371UiLtnyuIGVP3xtLpTU2kdGDvp8pQ+vcoH6BsUWfH5v6qLks8e/S+eeaYHfNnacwOWXYIglCxPyk6dFOyPqGJb1fic8RJFVkR1o0lC360aknBrovG0rTsCG4s9qter8eI7xHFTtimCejf3m7CXUs+MX7eL9QJ0V11uYzZ0bFqj5EYg0Wq8lcnor4mZJuyHPQXbmyOCtGCk42u5yw/PnOcct29r2wCYK65c8/STy33mWshljoOK3b8ihyxBPKYHXNw8COrthjlDnTs3J/6vam7fcUXGfZ+Fv9W2ecUWXYIglCiWyP6VySCBPW3MLE3lpgOmpkbK7VPmVixqqDMVaGNa7YZWezzVRQbokleZdmJWDw8NU3Dzf9ejw072oxl+wSx0560moWyZNmxKs7GTgiyyUV/e5Zx6KAqrLzhNHw72YVcefwSsOxkozcWy2UnH4ql155kuQ17L3dGYnhJUm2bJZN6Spnw6a4O4/OEITWW28r+Xtkkhkgsjmgsjv958kPTdnb3je5m1uv6iH/bHqkbSx8X80whyw5BECr0CbpfZULspLKx1AHCQGYVlFmxE5O4ofiYHX6d2C4iPTcWLzaqhfRcn9Kyoz5GOBo31SFp6eRdc3rfnmylnltNgnw6vXm76rKA5b6dTP4lIXZycAp2la1VLwZHjewnXe6WG0u/n04ZO8hk+RNdv7Kq5WKdHVHs69iKneTfh27ttUo918esFVnMDmVjEUSeMSw7SbGjzsbiv5eRG0vh49dhrUdWFZQ/2tGG4//fq5bHsgpQPmJkP1x83ME4ZGAlACgDlPXxVof8aBcys2RNC8WHvZ7NdeigSsuxOsVq0mDnW1kNlhobsdPb4xcqohtL/zmbesJOnMjKNNSWB1BfWybZ2lkF5Wxw//yj8MKHzfjJnPGmdUG/l3M1Sy07XOFEYFdb2LQNYB+zowurcDSOR1c14SdPruPWs7edWEE5ViRuLBI7BJFHYnHNqG3Rz3BjOczGSlPsxOMat0/Z97nsD5vj2VV3tQpQ9gC4+eyJxs96gK7qGFVlZrHTKUlL3y8pNHdQXTkG18gntXRxmo2VrhvLKcWSeu71pO4lUYekGoFmT1DYBXjL7vWAz6sqpOyoqGA2OHnsYJw8drB0XcjvQztS97js5USsxdUsxOoACaFjZzU0LDs9MZPQAdJwYxVwBeXi+MshiBKlravHeGjoAcqGGyvL2Vhi/ItdzI7ZbZbe8awClMWHr+6GEAWUHkMgq0ors+yIAcoAcMSIOmcDdsD0Q/or17HiTnalsiF2AkUSoMy6ZFQxO9k0nvD3mnlak4mdoM+jvKcLoa9qum4sANjRJhM79iejW15VLzBcU1+j63lxubEK4FdK5JrWrh48tOIL7OmQmziJ/KG7XarL/MZDSd0bC8LPvRM7spgdroKyqYhh5mJHDFAW57maZJ0PsfaPYdmRiJ12SZ2g/ZJ0+iNGyOMy0uGVa0/CnXOn4JtHj1Buw56v7Hejn2NvKHQ31n9OT1yf62enMqSUdXayKXaYncnuFdnvI+D3qsVOmoPLhSFIdP1K3Vgx0bLTZdrGkdhh3FgyvDI3FqWeE4XGdU+sxU3/Wo/vPPhOvodCCIjxOoC6N5YoPtjn3NZ9BzDvTyvx6sfqKr7iW5vc5Jxa9uanezDvTyvxxZ5OAOkHRFsFKIuTQ53QiFAnbCF2ZL29ZJad0YOrnA3YgkMGVeEbRw937MaafFAtZk9q4NbXZMGyIwvEdStzyAn/e+4kvPM/MzF7UqrQnjg63UqXTg8pO9gGrbK2C7J7PeDzKu/pdN1YuXB7iX8zMquJKC52tGRo2TEClOWlHmQBykYj0CKJ2SGx0wfQ0yw/2Naa55EQIvuSYkeP1wGYooKCOBFjaNi30l+9tBFvbdqLiy0ErUnsSN7C2If/+u1teGvTXly5+H0A6ae6W7kWxHgNo+tyl1zsyAqu2VWA1qmvCTkbcC9hJwS/14v7vjUN35qRsgTZZWM5QRZ74VZ8iRM8Hg8GVYc4USYKiktPOhTXzx6Hs6cOzdpxWcuDrBu5KmZH/JtK7S+9a5rO9la/Ll0gH1RXLnFjmccqWmvF+jpAwl1nh2HZUWQ/yttF6JYdNvWcxA5BEAI9sbjRi4az7CSf3E+s3oa/rtxiLLcKUGbjV1QPHFHsyAOUzcs27mwHYBZbdnC9sWyK+tUKXZd1Um4ss1CQ9faSUV+dneBkO1hxp39mJ95sZGPJKIT4EhEfM8GK99nw/hW49KRDs3o9WMuOrMeUzAWbiNmR7y9da5ld6juL1Zbjh9TgjR+dgqXXnmTK6JOVehD/1ptlMTsOWqWk6uw4ETuJ/2XtIiLkxiIIgqVp7wEccdsS/OgfHwDgLTtsmuhPn0oVCDO1b2DEx4Cq1Pd1cSJiDlCWxOxInv7dybe9dBuPspOwvRsrMf4WwY2lj7lKZtmRuLFk6IHfuYad7/TJkrXEZCNAWYa/ANWO3yZ+Kduwl6BC6EauaZrUKmlp2UnTWJaOdc0uHmh4/wqUBXwmy440G0tYtr0l05idxN+Xqogn1y5CcGNxXc+F7+/pCGPZJ7szKpORbQrvr4Qg+gCLXt3EdfTuX5makFUtCcxurNRn1v1z9r1vYockUNHKsvPnNzfjryu+sCxXr08YF84Y6ejNN50AZV2QtHX1cOepj7k8aBYKMjeWjGxV6rWDd2PpQeapc5FZHLJBIcXs6NgFa2cbK8tONK4p3VgqQ0S6AcrpuLGcbuskZkcUJzLLjJP7zq5RLjtmoyZW3GzZEWOjzvz1G5j/57fx5Ptf2o4h1+RV7Lz++us4++yzMXToUHg8Hjz11FPcek3TcNNNN2HIkCEoLy/HzJkz8emnfE+Tffv2Yd68eaipqUFdXR0uueQSdHR0gCAKGTEQcDDjahELgOkTvvjAZn9mLSJxDXji3W2SY8oDlLftP4DbntmAn/5rPdotuozrk9awfuVYc9Ppyu10fBYByqLa0WN2IrE4Dr9tCT5NWqf06yR7GMsClPOJzI3FWs9yJboKUeyw4iPdelCZILOq6fTE4tIx+H0epWUnl24sp5uasrEEZaZpmiGAVL3fZk9qwDWnH2Z/LDuxI3FjaUY2FtOyImq27ADAEpv2HG6QV7HT2dmJqVOnYtGiRdL1d955J+655x78/ve/x6pVq1BZWYlZs2ahuzvll5w3bx7Wr1+PJUuW4JlnnsHrr7+O73//+26dAkFkhPgGNKg6FUQrmp314nlWqediyrXMnK0KUH6/qcV2vN09McMU7fN6TK4CGezboF2AckXQZ4i81q4eXPP4Wm7MsorETmN23EImdtxIxXWrtUE6sBO6G5YdVkiKx+uJyt1YQZ9XGtAO5Nayc//8o1EZ9OFX/zHVcjuTGytutszq41f9Pf70KxOURQtZ/D6vTSsU9nPSsiOJ2VG1kCmEWzSvFZRnz56N2bNnS9dpmoZf//rXuPHGG3HOOecAAB566CHU19fjqaeewje/+U189NFHeOGFF/DOO+/gqKOOAgDce++9OOuss/DLX/4SQ4dmL9qfILJJl1AQjxU7ohurMxJFv8qg+SHOmLBbkinX86aPwCOrmqRZGaqYHSdip7m1m6mI63Hmxkoj9dzj8aAq5DdE27ovWxGPa4bYkb15OnFjyURSrpCl57qRnZKOVcEtePHh7rFlzWRlMSMBnxddPapU6/SOmU7MznGjB+KDW2bZ/g2JfzOalji3lNWQd5Hu6zTvIx3RFvJ7pYU6AbkbSxazQ3V2MmDz5s1obm7GzJkzjWW1tbWYPn06VqxYAQBYsWIF6urqDKEDADNnzoTX68WqVauU+w6Hw2hra+P+EYSbiFkTnGVHeAh2hhMPIFHsdDAuJ92NNT7ZOVlWNl607KzZ2oJFr27Cu1v22Y53R2u38SbnIJM1sR2zoV02FgCTC+3D7a2G601q2XEgdiYOte4knU18kgnh0pMPhccDXHDMcFeOW4i43eJCvFd6YnF5nR1FUUGvJ32XY7q/AyfbywQ++4LDvryo4nLSOQ0rV1ZvG4H2ecuOFc3NzQCA+vp6bnl9fb2xrrm5GYMH8yY6v9+P/v37G9vIWLhwIW699dYsj5goVTRNw+WPvo+6igD+92uTs7JPUYwMqrJwY0V0Nxb/YG5LioPunpjxhqqLHScByt09cfzixY2Oxvte037j4ebUZM9bdqwDlAGza29vZ8TasmPhxvrBSYdgY3M7bj9nkqOxZgP2suinfuigKmy49UxlTEU2KFSxc92ssdi2/wCmDKt15Xg/OnMsvtjTiTlThuLF9akYkR6lZUeeep6JWzAXvwPZCwInLJi/Z1kAP5Cu2PEBkL9AsH/LpkagjOtKKXay2ActUwrWspNLbrjhBrS2thr/tm7dmu8hEQXMZ7s78ey6HXhkVVNWUii7e2LYKzSsrC1ns7FEy05iUhefI3q7BN2q4/N6MKY+US24rTtqapQZiclN1E7464othvBw+tbLTgAej4d743ayj3BP3Hh7Tdeyc/r4ejx48TEY3r/C0VizASsC2bMrD/pymhFWqGJnwSmjsfC8Ka5lw/3w5NG48+tTceKYgfj1+Ycby3ticYuYHYllJ4PrmQtXYrWkwjTrFtVdRn6vh3sZYEscpCMyrKyvspgdeeq54vlYALdowYqdhoZEJcmdO/ko7p07dxrrGhoasGsXXx4/Go1i3759xjYyQqEQampquH8EoYJ9WxGDBDNhV5u5Rxn7gDUHKCdEivhg1t0+LV0J4VRXHkBNWcBorSC6ynSxImZ7WTFmcBX6VwbR3NaN1Vv2A3AenyC+IYc4sWPefsIQ/u8wEosbFV2DPrOZ3iobK5utCJzCv/2693QvVLGTLzweD8494iAMTrqGI1HN9KIAqNtFZFKROhOBZIesl5qsNYPf5+Fc2mPrq1PjypIbyyNxY8UlqeeqmJ1CuEMLVuyMGjUKDQ0NWLp0qbGsra0Nq1atQmNjIwCgsbERLS0tWL16tbHNK6+8gng8junTp7s+ZqI06W2ju+bWbjz7wQ5DrOy2aciqysYSAy91y87+zsT/eq2ahtoy47gs+thl5fQHMkUJWSqCPhwysDJxnKQFyekDVJyE2YBL2S7+cNE0/PjMcZg2sh8AINwTQzimdmNZJfnIemnlmnyJjkJqF1FIGD3mYnFp01tVUcFMfo25+B3IilBGJS6jgM/LCf9TxqVCO9ILUFbX4+Hj0RL/x43UcycxO/m/R/Mqdjo6OrBmzRqsWbMGQCIoec2aNWhqaoLH48FVV12Fn/3sZ/j3v/+NdevW4aKLLsLQoUNx7rnnAgDGjx+PM888E9/73vfw9ttv46233sLll1+Ob37zm5SJxVAA91lRwz4PxX5VTrj+nx9gwaPv4d5XNiEe12wr/4purGufWItHVzUZpnj9IajHrLTqlp1kFeYhSbEjZmTpDyJZmurQunLpWHa1h00ViJ2+xZo3S13I+hpzC4dh/Spw2cmHYkCydUYkFrdMPbciH5YdrySuwQ3IsiNHv2cSYse8PuBXxOxkcD1z8TuolrRIkWU+BX1eXDdrLCYMqcGz/3U8F6ycvQBl9jPvxmIFjjL13PkwckZeA5TfffddnHLKKcbP11xzDQBg/vz5ePDBB/GjH/0InZ2d+P73v4+WlhYcf/zxeOGFF1BWlnpQPvLII7j88stx2mmnwev1Yu7cubjnnntcPxeidOHepjJwY722cTcA4K4ln2DJhp1Y92UrgEQw8WnjBuMrU4dw2wck5f9/8uQ643NteQDt3VG0hxOiSRc9eldtPdhZL+i1ZmsLygJe46Ek6wo9pLZM2ih2Z1s3jj10ILfMydvimRMbTG9zezpScUrD+snFFQCEkpancE/cKCqYvtjJTbViK/Jm2SGxI0V310ZicWnWVdDnzVoXefEFJRvILDuRmNyyc+4RB+HcIw4CkPh710nHomIVs8PuR6yzw1p2ItHUZzeKSaZDXsXOySefbNlc0OPx4LbbbsNtt92m3KZ///549NFHczG8ouDz3R2477XPcNnJh+KQQVXSbTyAZRsAwhq28nC6bqx2IYhWFzpAohv3f88aa/pOwG/9gKqrCGDb/i4jZkcP1NW7aut9svZ2hLG9pQvnLnoLQCI7BpC/wQ2p5cVHbXkArV09mDq8jgueBuzN/EeOqMOd/zFFub7CJmBXT1UOR+OW2VhWWJnkc4Xs7dcNSOzISbmxNHnquc+LhedNxkV/fpuzgmYUs5OD37csZuf3r32GgwdWYsEpow3hIz4vWDd4+tlYcjg3VnL3stTzqKKaciF4Fwo2ZodwxoX3v40nVm/Dd//ybr6HUrKwYifdInGbdqlbl6jiSuwaO+qtJXR3mJ6CXlOe2N/ApGVnb0cEb366x/hedzI9XWbZYev8AMAj352Ob80YgV+ff7jJjWU1uX7jqGH45w+Ps+xoPWmodSqy/ob5/174GNv2J1Lo3SwOmCmsgHPz2U5iR44hdqLqooJj6qux4obTcOJhg4zlmcSX5MSNJbHsPLF6G37x4kZs3XcAKz/fC8BsCWbrGqVbVFCF1I2lByhLrE0Ab4UqhDu08J8ghCVfJtsCfL5HUj4zSSEEhxUzkaj8j9kJn2YgduyypfQAXt2y026y7CSEy+6OMD7cnrIk6VldsgBl8UE3ckAFfnbuZIwcUGmy7FjdT1YP/StOHY3qkB93zLWuVSQrQpeuZacvQWJHTpALUNa4ZQD/d8b+yWVSAzE3AcrqF4Y/vfE57nwhUSNLTGjgLDtpHM9pUcGUGyvxsyobi31uFoJngZ4gRE6IxuL4aEebpZuyWGCbdrI+aSdkYtkRH14ix4zqDyBl0dFFj/4mmHJjRbB+e6o6+IFkcUJVtVW/1/xAA2C27CTXfX3aMNM+rN4krz1jLNbefIbS3aojix3Ih1uqN7gaoEwvM1J0906EqbNTwcRysX9nXF+zArHsWGUU/mXFFuOz2Y0l/zu2w+pvzLoRKFtnR27ZUdbfcRESO0RO+Om/1mP2b97A7177LN9D6TV6rRdAnW2gYmey1s2oZPo2iypjyO6x0JDMZNJjdXR3lu46GsRYdj7akRI7HckU9jKJZefcIw7igizZh5sqZuf2cybhd/OOxLGHDjDW2T30nWS6hCRirxjcWCxuWlP17DuCRxczn+3qMLpuVzKZiHxsS+9qJOVC7DjdJ/t8AhJtMHTSitmxKiroZT8nLTtJkdOj6I3FWnbEvnz5oLieIISSdArFucHf3m4CAPzm5U/zPJLeI8uAcIoeJyMTOzKfPABU2XQU1wMXI9FEtpLKsrO7Pcw19utUiJ13b5yJgVUhLlaIfUiaxE7yYVce9OGsyUMMtxmQnUDNkESMOemvdMrYRNxFpcJy5Sa5/Gs8lamjMnN8Pf5nzvgcHq140cXMPa9sMpZVcpYduTUnE+GSi6KCTtnewreGCWUcoKz+G/NJ3FhxLfF8a25LHZ+L2emF+z8XFGxvLCI9rCaDfMqgUognCDOdkdPNxupOvnUdPMC5Zae2IoCHvnMMHl3VhBfWJ3q8/ezcSbjxqQ/x/+ZO5szbbAq6btkZUBky7xRAZ0Qes6OLJFU2kV6/R7YOAMqYh2Q2xI40ZsdBb6nvnXgIZk8agkbG0lSK3H3+4Xh+3Q6cOanB9LshUsjuowlDavDJzoRrmbUWcm6sAmkX4ZROoVN5IMO/R1n9LR2ugrIhdjScfe+bXFwiiR0iJ7DxMAHLUt9ujEZOPh8C2SI7lh1zjyYrn/yJhw3CkSP7of2vPZg1sQHfmjESsyY2GFlTVSE/OsJRdHRHjTo7umgJ+r2oKfMbMT06esyOmI2lZ3P4FW+EdTap56wQyUaDa5mwcWLZKQv48I2jc9ddPB1y6caqLQ/gm8eMyNn+SwXR2v3Gj07Bm5v24Kk125Pr5fd7Jr86N0sN2JFpgLKqijogvAgldx+Pa6YEDDZYme3F15NmrGMuILFTxHQxFgcnk0E+8BWYey0TWJ84K3ZaDkTw6sZd+MqUocqg4u7k201DrbmInl1Lg6qQH498d4bxM5seHvJ70RFOpMXr2VhsXY6B1SGT2JFlY3k9KRO8KkBZrPchmuzLmMDGbJjzZfey38H9LSvGmC8KaO7rs7B/k+MaqjG8fwXXfT67AcoZDjIDygM+7tkvkmmA8sAquUUYMDf1BeQvfmyFeTaZg2J2+jiRDFoPsOjdrguZUrDsqIoKnn7367j6sbV46v0vld/tZlxH3z1+FLeuShGz4wT9QR2Jxk0xOwAwQtLtWw9QZi07XkWsAvtr83k9XAdmkxuLEU/ZyAwSg5EvOEZtrWEPl4sqtkTxwlq7jxhRB4AX5tmM2bGrjZUpujjT49EAYPbkVJPrw+qr8JtvHs59h3Vjp/PnOMBC7LB/84cmsymXMTW8dHricoFTCG4sEjt54uGVWzD+phew7JPdGe+Dbf7WFVErfU8eo3ZKIWZH/KMNR2O4a8kn2N2eaMew8vN9yu92J9PWywJe3PiVCfj1+Ycb6yptApGt0AVBa1ePYTpm63KIHcQB4IAeoKzw6cve3nTYlF1R0IQUsQ+ZwqbAHjd6ABael6jGfOVpY0zbssHThSSsC2ckfRfWQqhP0Kww54rvWdz7TnASU5YJz/3XCbji1NG4+/zDDeHz4zPH4bZzJmLx92fgpatPwjmHH8R9p6G2DHfOnYJ7LzgirXOxcmOxuzlzUkJsrWXaUujoz5gDkSieXrvdWN7bF/tsQG6sPHHjUx8CAK549D18cMusjPbBWnY6I1FommZ7czvZJpvk6o3HKdk4XzZAORrT8PfV23DP0lSWmapuDZCK2dEfsnqVY0CdjeUEXez89F+J+8jr4bOQJgw1ix0jQJkdL2sZsRALieDFsHEslrKA3FKUKaxlh3W5XXnaGMwcX4/P93TgysVrACTEjv534MTV5Rbkxso/rOVmeNLSyYoSPpA39b1MbqP/PmMs3vliHy6cMTL9L1twyKAqXHtGos3Lmz8+Fa1dPaivKcNFjQdbfi+T2DUryw7LQXXlmDq8Tip2OiMxdISj+PHfP8Cz63YYywvBskNip4jRu10DiTTAcDQuraHCEotrrpr782nZ6e6JYfZv3sCEoTVY9J9HZryfsFAvomnvAW79vs6I+BVmDInv6m9lbMZDbzpz62+lm5OVs+Ma/0Yqs+zolAkxOzpWvyvuO2LMTkBuKcoU1lIkHnfysFrDHQckgqf18mqFZNkh8g8bk6O7ddn7KZsxO0PryvHGj07NZJiOGVgVsoyr6S01Fi9fYm3YwwZXScUOAOxq6+aEDpB+FmsuKJxXoT5Kb6wOYsyO0pXFHELWEC+X5HMCWvn5Xmze04lnP9gh7Y3jFDGFUhc34xqqAQB7O8PK7+qWHd01wz5Ue9OZ267InizVXUdlibGywpVbCBrW7ZQN44rKsqPTvzJlbmeDp+0qT7tJPl3HRIJuJrHAsOww95YqkLevttexOm+xa3x/C5fXBqaQqQ4FKBO9Yt8B3qJwQBGhz97CMRfEDpsSn0/LDuuT339AbX2xg28Eqhn7Gj04EQcgWna27T+Ak37xKv70xufGd3WBwQYO96YFgih2fvufR3A/e70e3Dl3CuY3juQqHAPqh7zV74oPaubXsa6BbGRjsROSzEXYj2lfwTYcLaQAZTIy5R+9ejmQynxUxeyorDxEAnHeGFBpFjv6M+mp97eb1pEbi+iVb7+5tZv7uSsSVWyZwg3LDisO8urGYnpa7WwLO/ZJi7C9sVjLjkrs3Lt0E7bsPYCfPfuRsUx39dTXlOGJSxtt087tYAXBqIGV+MqUoaZtEn774bj4gbe55ar6IlZiodzCjcVZdrIcs1MmETtsIT32jbOQUs+p2F/+aW7rNi1TubG4AoN91LKjYlB1yLCM6fSXFC49qK4cm/d0YunHO03regogQLlwng5E2uwQxI5eR0WElTduWHbYFgX5fNtuZ+rM7Gw3P/hkvN+0H2f++nW88WkqS05MPdfdh7rY2X+gh3OTyd5i2Ifs0Qf3x3iLmBonsA9qu47gohVIVXTMKt6mPGiVep67bCyZG4s9H/ZaF0JNp1/9x1TMmTIE/zmdiv7lm+NGDwSQmIR1yhSZg5y1sw/PiofV8016f3DiIVh+/akmF7HMsqNfZ1nv50KI2SHLTp7pzeNZtOwcUMTssG6ldBtZZkKXkL2UL9hA1l2StzwZVz22Blv2HsCF97+NL+6YA0AUO3HDfXjIwMSDIRbX8PJHOxHXEmmZYm8nn9eT9XiSYBpiRzx2QJFyaxVfxcbsiG++rJDLRrxDyCZmh4X93RRCgPLcacMwV9INnnCfH558KIbVleNkpkYNe6+yVsGgL7sZhcXKI9+dgVc/3oUf/eMDAImYONmzq79E7AytUzekjcTirmcCi5DYKWJ0y47f60E0rqGrR+7GYq052RIfmqahtatHaq5n3Wn59NV2dLNiRx1EzCKzjrET6q72bsOyM7gmZLRl+P5fVwMAVv3kNFOZersJOxNY64Zd7I9o2eHqi7AByg7dWOLziquzk4VnmV3MDsuxhw7EG8niZoUUoEzkH1n7EFbssK7kgL93RQVLhUHVIXzj6OGIxOJ4cX0zvn3swdLt5GLHXCWeJRrX8tqwmsROntl/oAf/WvMlvjp1aFqqNxKNY09HYgI/ZFAlPtnZIbXsxOMaWM9VttxYP/3Xh3h4ZRNuPnsCDqorxxkTU1U9uyJ8qrZsTP9eux3HHjoAg2vUbwO9hbXsqNxYqz7fi1hcQ0tXDw4ZVImBVUHjuuqw2VgPr2wyPteVBzCgim/LsLs9zIksgHfzZAtO7NjsX7T8+LkAZWa5hf2+TFF1GRAqKGejXQSXMSMf0wtXnYCVn+3FhY0HY9TACoT8vj49SRHO8Hk9eODbR6O7J8bF8KleAPoq35oxEt+yqBk0QJKNNa7B2jXfE4vn9YWExE4eEAXHlYvXIOT3GZUpnaBnGgT9XgytK1eKnZjgQLULUA5HY46yhPRJ/9anNwAAnr78eEweVgsg1XASkDeAW/LRTlz12BoAwOaFZ+XMtMnF7EgsO5FoHOf/YSW37PjRAwG0AwDm3PMGhvUrR4skkyvo98Lv86K+JmTUugES5QBauviSAL3JulLBW3Yyd2Ox194qk6oikHpUiKKCPX52srHsXQrjGmqMh+uZk4b0+phE3+GUcYNNy4KKAoOEHFmH9OH9y3HCmISl9aiR/fDulv3c+p6oBuQxbp/svnlA1sRt9RZ1ywEZeqbBkNoyo+3AgbDZjSUKq1gyZuejHW1YLdyM/3xvGybe9CJe+JAvCOWEd5nxs+cns+ys29ZqfH6vab9pfbbgxY7ZsqM30GRhJ/L129vw4vqdpkBwIGXt+clZ47lg1G/dvwqvfLyL2zYnlh1fGm4sn9qNxT7XLWN2gurJIJe9sbIhngjCjiClnveaYXUV+P23puHa0w/DbedMMq3Pd60dEjt54IAkRTxd64Y+eddXlxktCFq77MVONK5B0zTM/s0bmHvfcuxi3DvXPL4W0biGSx9+L62xAOBcP2xxQ1nKIfsw+ed76iaavaUjnBIzrV1mYdPebb5em3Z1pHWMKcPq8POvTcZxowcot7Grap0JoTQsO6ZsLD/rxnJYZ8eygnJ2z4+rZktPKMIFAuTGypgfnHQI3vjRKaitCKAy5McVp42RtqvJd60depTkge6I+Zee7t/XgWQgbVWZ3wgW2yep5Cu6raIxjVPY6U7uKr7Yk2qhwFp2wrE4drV344NtLcYyVnjs7ci82J8dbMxOp8Tq1Sax7HzZ0uVo32dMqOd+tqqbkwuxk07MjlXqOR+z47BdhEUj0J4slzagiYdwA96NRfecE/72vRn43gmjcM3ph5nq8AAwFTMlsdMHOSDJmkq3vLxe6C7k92JAssDTXkmPJpllh7W8tB4wT/iZ8OmuduMzGzsUicZx9r1v4qu/fQu3P5OI72FjYNrD2Tm+DDZQWBc+Hze34ZZ/r8eejrDUssMyNRmDJDKuoRq/+I+p3LKqUEC6LZAbN1YgDTeWKWbHK4/ZsbLssD56q9TzbBcPGyApXkYQ2YYqKKdP46ED8D9zJiifP3+86Cg8+r3pRsNjEjt9EFkPK1X14+6eGD78spWrlZNYnmpDoEfGyxpSinV1YvE4Z3l5f2sL9ie/15u0wM17Oo2buVuISdKDg+9/czN2tXVzlh07waGirbsH5y5KCKgPv2yVXtN2xprT3RNHNBbH1xYtx4PLv8BP/rkObRLXls6TPzwWc6bIA19nTWxAbTkvbqw6mJflOUBZXM+5sZhVTmN2xBdfdnLIVh2nO78+BZccP8rSPUgQ2SLI/U2Q2MkGlSE/jj10oOECj0iSVdyExI6LhKMx3LP0U7y92RyMrJr0r1z8Pr5y75v4hxDbwlp2Um4ss9gR555oTOMsL394/XNc8MdERpIswt4pPTHNEE2qSs4A0NLFZyuJadpO+fObm7Fmawvuf3MzvnLvm7j2iTWmbcR9d0ZihtBbtXmfpdCqDPmlJdEBubCxcmPJAtJ7SzpFBa0rKLOWHYvUc4uYHZZsVUr9xlHD8dOvTOizTRkJd+GLCuZxICWI/rwhy04f4pGVTbhrySdY+PzHpnVtion3xfWJPiN/fP1zbjnbYFIXOzI3ltmyo5msIB83t6Np7wFUMrVUZDEuAPDOF/tw5q9fl67TLSlWrRnau6Oc60x13nas/Hwv9/Nz65pN23QI58CeUyQal8bs6JQHfNKS6IBc2FRZWHa27XcWB5QOfMxOetlYrAXHacxOucOMq0gB9MAhiHRhrdrUGyu76M8qysbqQ3y+Rx0MbDXxytBdRWzMzv7OiMndJY3ZkVga3ty0h/tZLKynM++Pq/Bxc7t0nW5J+VIyueuTZUc4yll2ZOnfdnSEo3j3C3PKOituYnHNVHeIEzuxuKXQqgj6pFVCAbmwsbLsbN1/QLkuUzKtsxP0efnaOmzMjoUbs4LrjaU+Vr7f3ggiE6jcQe4YUpsoHKtXOs8XJHZcpK5cXVEp3dgV3bIT8nvRrzIRPxKNa6ZOv2I2lsyyAwBvbdqDA4wI2t2eEjsHIlFceP8q/OmNzy3VuS42tksymvS+Ke3dPVzMTjgaV1oDtuztxDd+vwIvb+C76H65v0taHPHeVz7F6Xctwzf+bwUnuAZXh7jxAYnrYCW0KoJ+TuzU16RcWjJhI7q2bjl7Ao4Z1R8AcNq4etP2vSXT1HOxJYTHoWWHDbK2dmOR2CGKDy5AmSw7WWXe9EQl5kdWbjHFc7oJiR0XsQpilU28bCdt8e/PsOwEfAj5fahOTsCNC1/hGoTGJZYdWaXlj3a0GensAC92Xv5oF974dA9+9uxHyvEnziEKTdOk6dt635Tm1m6TtUl0N+nc+cJGvP3FPnz3oXe55bICgQDwf8s+x6e7OvD25n14YX2iMGLQ70W/ZP8uMZbISmCWBbxcSfR6pq2FXczOb755OL593Cj8/lvTcOOc8bhj7mTlcTIlraKCFu0XvAorjwjX7NPhsQiiWOCrgOdxICXIrIn1GFpbhr2dEVP4gZtQuwgXsXrrlWUGsRYQwTvFWXYAPvNo7bYWNNQ2oK27B98ThEI0FucaW+plvbfuP8AFl+5ixI7oGlPREY6itatHKqZ0K4kevxL0e+HzeNDVE0NHd1TqMmLdbWzHXJXYYdGPUx3yozKUmKh3d/DfU7nqgERKNuu6YbOvZGnmrNjRt+1fGcR3TzjEdqyZwKWe29XZ8anFjlPLDjsZyPqr3Xz2BDy1Zju+l6PzJYhcQkUFc4ff58X/+/oUDKktx+jBVXkbB2lYF7HKymnrjmLt1hZuGRtwrFt+PvyyFRu2txmWHVnBOt2a8/vXPsMXe/l4EdayM3N8PR7+7nQA5iwa9tiy6sM6180ai9nJnl4d3T2GyBhYxWcy6WJAX19XHjAsJKp4JdZ1xLZsYIWYCv04VWV+VCaPvb2FFztsTysV+hjYRqfSmB1mmawTfLZJJ2aH3TZo0ZHdb1GumBVzNeVmsXfxcaPwrwXHuXLuBJFt2L8RqrOTfU4YMyivQgcgseMqem0cFecsessQNfG4xqWS7+2MYHd7GF+5902cdc8bxjp9orv9nInGtp1JMSOzXMSYAOWKoA9lAR/6VZgnL7Yac4ui8OCg6hAWnDIaNWWJ73eEo0a8zkF1fDdzXQxs3ZcQX7WM2FG5k8LM9dqwvc34rFt2zj9qOA4eUCGtxbItGRRcFfIbE7XoXtuSFIK3nTMR7944UzqGF686EUuvPQkjmQqhMjdWNWPtqZOIgWzDi5003FjJz7/8j6kY3r+cK45oZdnx+7x4+yenYcUNp+akIjRB5BOy7JQ+JHZcRLTsDKo213Fpbu3Ghu1tmHrbS/h/L6RS1MPROJ58f5vx8zvJbCR94rmw8WCcNTlhfdALFMrqpiQqKCfWVyRTzRtqy03bsUJrv6Trd+LYif3rQqY9HGUalPL71GOKPklWWm6oLUM1I5JksBafDTvMYmfSQTV47bpT8Kv/ONz0XT1AuSrEWnbkKeAThtRgYFVIGm9SVxHEoYOqONdfpaQeURlTdM8qDT1bpNUuQhJ8+fVpw/DGj07FYfXVxrqTxg4ybc8yuKbM9HsliFKA/XuKO3TbE8UFxey4xKJXN2Hx203csleuPQkbtrfh/D+sNJb95Ml1hpARu5I/vJL/PsC7MMoDiV+nbtmRVURmKyjrQmlIbRk+YsQEwPesUll29MrAuqWjoztqfI8N7vV5PYZ1RX+ODOtXbriaVFlRbUxjU90iBKQqMg9OBg0PlohG/RpUl6UsOzta5LE+ulumIuhTZoaxWQQyM/fAyhCOHFEHr8ejrM+TTdIJUD54YKXx+XML193RB/fHkz88VtrnhiBKGfbvSZbpSRQ/ZNlxgQ+/bMUvXtwI9m/omSuOR3VZANMPGYDffPNwY/k7kvoxOk37zPVa2IJyeiDugbBu2TFPymzMTsqyU2bajrXstCgsO7pFQRcTHeGo8T12wi8P+FBVxrt2DqorT4kkB5adnUyczq6kZUcXOVap0FVMgLLMslNXEcCopBiosHDPjG2oVq7Tx/CPy47FE5c2ulL1N53U89ryAK6aOQYAcHLSeqPiiBH9TPFWBFHqsJadWJaqgBOFBVl2XEC0jNz59SmYdFCqyeQ5hx+EF9c3S6sA21HG/JHq2UO6mJHVi4jGNMNKoQenDmHSqkN+L8LRuODGklte/Ek3GWvZiSYfFGx2VVnAZ4pzGVpXjq37dMtOFG9v3od1X7bi28cebIg0NkNNFzg7WruMAGU2HVwFG6CsZ6zpGWgA0HjIAMNfP25IDba3yq0/h9VX42/fmyEVhjputjZgH85OGo1eedoYHDWyf96DBAmiEOH7u5HYKUXIspMHZAGemb5Ns5Yd3VKju3DCEpcMa9kpT24/rH8qDkN3Yew/EDGyulSWHd23radit4ej2JsMbO7PnE950GvE7OgcVFeOfklB9MWeTlzwx5W4/ZkN+OMbqbYYbIVjPU7n1n9vQDSu4YgRdUZlTgA48bCExWLi0BruONVlAVMRwHFDqg3L09wjhxnLF543GbMm1uPRZIaaSOOhAwwrUL5Jp+s5kBBix48ZaCnWCIJIuPqJ0oPEjguIgcnlErEzQNF0MrEuZSUJ+DxcuX72rV4XOweSAciyejexeNwkdo4fnXJt6PuOazDaOqgsO3q9lSrGsqN2Ywlip185ThwzEADw8kc7jX0temUTNE1DNBbn3Fv7D/SgIxzFKx/vAgDcfs4kzpJyzzcPx51fn4JfMtlFQNKNJQQU968I4olLG/HHi47CzAmp6sb1NWX4vwuPwrGjB0rPt5CwKhRIEETmkGWnNCE3lguITTVlbgc2oFfk8OF1WJqc5KtCfmhIucbYt3rRjdXVY46FicZTbixdHLFZYZ/s7EBNmR9t3VFs3tOJ2vKAsg6OIXaYmB1daPWvDBr7OWXsYM664vUkhEVDTRn6VwY5l1l7OIq59y3HlGF13PZxDVi+aQ8isTiqy/wmC05dRRDfOGq4aazVZX5T1tuQunIcMqgKhwwqXpcO66Kkiq8EkT0oG6s0ocekC4gBuDLLjpUb66B+KTdTZcjP1XFhg1ONAGULy05HdzRl2WHGccd5iZYG/3PWeCPgd+59y/HGp7uNDCoxJVl/A2KLA+pWoAGVQTxzxQm4+ewJuPr0wzjLztThdQj4vPD7vDj5MHPA7HtNLXhw+RcAEoJMP/9XN+4GkEgVV8XHVAX9nOWrKuQ3WlXolIIrh42B6keF/Agia0QpQLkkIcuOC4hiRx6zo56wKhmrSGXQLwSnmi07eg8omdhpbus2GoGWM+6dbx4zAqeMG4yBVSG88vEurEj2MPnuXxLtJqpDfgT8Xs4KExPEDhuI3a8yiIDPi4uPGwUgIcrOnNiAmKbhZ+dOMrYbXW9tXakpC6C+ugxb93XhtY0J69YEwarD4vV6UFseMERXVciPgwSxM6QExI7f58Xam88AQG4sgsgmsnYoRPFDYscFzG4ss9hRFaL79rEHcy6gypCPK+sf8ptjdnQxI+sw29zabcQQiRYmPbvpJ2eNxz/e24YHl39hWG8O6leOuMZXdY4mA/kGV5ehLOA1KkRXl/klPZg8+P2F00zjGTXAOuC3ptxvjEtvGTFhiFrsAImChobYKfOjPOhDRdBniL8hNaVRGK/WhUrNBNHXoJid0oReCV1AbIegBwazjBlcjWNG9eeWPX358fifOeMFsePnxJJM7HRauLG27D2APcnUbd3tJTJ5WC1u+epEHD68zlg2rF8F7j7/cAxnMrf0ehQ+rweHMvEv6RTVO9gmu6l/ZRBnTR7C/Xzy2MGW35k5PrVeb+PAppbWlJPGJwhCDsXslCYkdlzAZNmRFIHzeT14/AeN+ObRw41lk4fVIuDz8mIn6Oe+z1p5TAHKErHzZUsX2sNRDKkt41oFyDiEESIj+ldg4tBavPGjU41l7BvQGKZ+SzotBQ5mLDvjhMJ9FUEf/uu0MZgzZQgevPhoXHzcwXjmiuOlbTZYZjPiSBeWbL8bN+vhEARRXFDMTmlCr7g55Nan1+OTne1GDI2OzLKjI6sGzMbsVIR8yj9Gc+q5vDIxAMw/9mDbWA/W6sJadHTYqsJjGOF02nhrywsLey0GVYfwcXOid9YfLpyGKcPqjGDik8cOtrXo6IxrqMbpE+qxqz2MkQMSdYOokzFBEE6gmJ3ShMRODnn9k934bHcnRENCmUURuFPHDsajq5o4aw6beVMV8nPdwFl0sdPdE8fMu5Zx9XEmHVSDD79M9b9ii+mpGCVYdnSeueJ4/GX5F7j2jLHGsmFMxhhrWXHCQXXl+LKlC984ajiOGz0QcU3DGRMb0toHi8fjwR8vOopbNr/xYNz98ic4YUzh19AhCCJ/RKmoYElCYieHjBpYhc92d0J0AVv1cjpt/GA8cPHRGN+QCsLlLDtBPzyQW2zY7Tbt6jA+3/n1KThjQj0Ov22JsczOFZQYv1zsTDqoFr8QivedOGYQBleHcPTB/U3ZT3Y8ueBYrNvWilPGDra8Nr3hspMPxfgh1Zh+yICc7J8giNKALDulCYmdHDJqYPrdoz0eD04R3DWslacq5IOmCKBTNYQ8e8pQlAd9OH70QLy5aQ+uPG2Mo7HoLiAgEaBsRb/KIFb95DRH+xUZXF2G08bnNh086Pf2ylpEEERpM6g6hN3tYZwyzrkbnigeSOzkELtMI6dUCZadr049CP/3+uc4nWl1AKgDb/WKzXd9Yyre+WI/Zk9yNulXlwXwt+/NgMdjHWdkd3yCIIhC59krjsdbn+3BnMlD8z0UIgeQ2Mkh2WoayaaI+30ejBhQgXW3nGHq+QQAN8weh40727GrLYw3N+0BkBIhg2vKMGdKevE0jYeS24cgiNJncE0ZvnaEfSwjUZyQ2Mkhoti55vTDTC4qJ7CiRvdgVZfJC8r94KRDAQA/fepDQ+wQBEEQRF+G6uzkkPrqMlQy7p/vnjAKk4fVpr0fNmhXFa8jQhYZgiAIgkhAlp0c4vV68Nv/PBIvf7QTx4zqbxT96w1+h32QZk9qwI1zxmPSQemLK4IgCIIoJUjs5JhTxg3OSnT/D046BK9/sgdfO+IgR9t7PB5894RDen1cgiAIgih2PJpTv0gJ09bWhtraWrS2tqKmxrrJJEEQBEEQhYHT+ZtidgiCIAiCKGlI7BAEQRAEUdKUjNhZtGgRDj74YJSVlWH69Ol4++238z0kgiAIgiAKgJIQO4899hiuueYa3HzzzXjvvfcwdepUzJo1C7t27cr30AiCIAiCyDMlEaA8ffp0HH300fjtb38LAIjH4xg+fDiuuOIKXH/99abtw+EwwuGw8XNbWxuGDx9OAcoEQRAEUUT0mQDlSCSC1atXY+bMmcYyr9eLmTNnYsWKFdLvLFy4ELW1tca/4cOHuzVcgiAIgiBcpujFzp49exCLxVBfzzfFrK+vR3Nzs/Q7N9xwA1pbW41/W7dudWOoBEEQBEHkgT5ZVDAUCiEUCuV7GARBEARBuEDRW3YGDhwIn8+HnTt3cst37tyJhoaGPI2KIAiCIIhCoejFTjAYxLRp07B06VJjWTwex9KlS9HY2JjHkREEQRAEUQiUhBvrmmuuwfz583HUUUfhmGOOwa9//Wt0dnbi4osvzvfQCIIgCILIMyUhds4//3zs3r0bN910E5qbm3H44YfjhRdeMAUtEwRBEATR9yiJOju9hRqBEgRBEETx0Wfq7BAEQRAEQVhREm6s3qIbt9ra2vI8EoIgCIIgnKLP23ZOKhI7ANrb2wGAKikTBEEQRBHS3t6O2tpa5XqK2UEiVX379u2orq6Gx+PJ2n71nltbt26lWKAcQ9faHeg6uwNdZ/ega+0OubrOmqahvb0dQ4cOhderjswhyw4SvbSGDRuWs/3X1NTQH5FL0LV2B7rO7kDX2T3oWrtDLq6zlUVHhwKUCYIgCIIoaUjsEARBEARR0pDYySGhUAg333wzNR11AbrW7kDX2R3oOrsHXWt3yPd1pgBlgiAIgiBKGrLsEARBEARR0pDYIQiCIAiipCGxQxAEQRBESUNihyAIgiCIkobETg5ZtGgRDj74YJSVlWH69Ol4++238z2kouL111/H2WefjaFDh8Lj8eCpp57i1muahptuuglDhgxBeXk5Zs6ciU8//ZTbZt++fZg3bx5qampQV1eHSy65BB0dHS6eReGzcOFCHH300aiursbgwYNx7rnnYuPGjdw23d3dWLBgAQYMGICqqirMnTsXO3fu5LZpamrCnDlzUFFRgcGDB+O6665DNBp181QKmvvuuw9Tpkwxiqo1Njbi+eefN9bTNc4Nd9xxBzweD6666ipjGV3r7HDLLbfA4/Fw/8aNG2esL6jrrBE5YfHixVowGNT+/Oc/a+vXr9e+973vaXV1ddrOnTvzPbSi4bnnntP+53/+R/vnP/+pAdCefPJJbv0dd9yh1dbWak899ZS2du1a7atf/ao2atQoraury9jmzDPP1KZOnaqtXLlSe+ONN7TRo0drF1xwgctnUtjMmjVLe+CBB7QPP/xQW7NmjXbWWWdpI0aM0Do6OoxtLr30Um348OHa0qVLtXfffVebMWOGduyxxxrro9GoNmnSJG3mzJna+++/rz333HPawIEDtRtuuCEfp1SQ/Pvf/9aeffZZ7ZNPPtE2btyo/eQnP9ECgYD24YcfappG1zgXvP3229rBBx+sTZkyRbvyyiuN5XSts8PNN9+sTZw4UduxY4fxb/fu3cb6QrrOJHZyxDHHHKMtWLDA+DkWi2lDhw7VFi5cmMdRFS+i2InH41pDQ4P2i1/8wljW0tKihUIh7W9/+5umaZq2YcMGDYD2zjvvGNs8//zzmsfj0b788kvXxl5s7Nq1SwOgLVu2TNO0xHUNBALaE088YWzz0UcfaQC0FStWaJqWEKZer1drbm42trnvvvu0mpoaLRwOu3sCRUS/fv20P/3pT3SNc0B7e7s2ZswYbcmSJdpJJ51kiB261tnj5ptv1qZOnSpdV2jXmdxYOSASiWD16tWYOXOmsczr9WLmzJlYsWJFHkdWOmzevBnNzc3cNa6trcX06dONa7xixQrU1dXhqKOOMraZOXMmvF4vVq1a5fqYi4XW1lYAQP/+/QEAq1evRk9PD3etx40bhxEjRnDXevLkyaivrze2mTVrFtra2rB+/XoXR18cxGIxLF68GJ2dnWhsbKRrnAMWLFiAOXPmcNcUoPs523z66acYOnQoDjnkEMybNw9NTU0ACu86UyPQHLBnzx7EYjHuFwgA9fX1+Pjjj/M0qtKiubkZAKTXWF/X3NyMwYMHc+v9fj/69+9vbEPwxONxXHXVVTjuuOMwadIkAInrGAwGUVdXx20rXmvZ70JfRyRYt24dGhsb0d3djaqqKjz55JOYMGEC1qxZQ9c4iyxevBjvvfce3nnnHdM6up+zx/Tp0/Hggw9i7Nix2LFjB2699VaccMIJ+PDDDwvuOpPYIQjCYMGCBfjwww/x5ptv5nsoJcnYsWOxZs0atLa24u9//zvmz5+PZcuW5XtYJcXWrVtx5ZVXYsmSJSgrK8v3cEqa2bNnG5+nTJmC6dOnY+TIkXj88cdRXl6ex5GZITdWDhg4cCB8Pp8p6nznzp1oaGjI06hKC/06Wl3jhoYG7Nq1i1sfjUaxb98++j1IuPzyy/HMM8/g1VdfxbBhw4zlDQ0NiEQiaGlp4bYXr7Xsd6GvIxIEg0GMHj0a06ZNw8KFCzF16lT85je/oWucRVavXo1du3bhyCOPhN/vh9/vx7Jly3DPPffA7/ejvr6ernWOqKurw2GHHYZNmzYV3D1NYicHBINBTJs2DUuXLjWWxeNxLF26FI2NjXkcWekwatQoNDQ0cNe4ra0Nq1atMq5xY2MjWlpasHr1amObV155BfF4HNOnT3d9zIWKpmm4/PLL8eSTT+KVV17BqFGjuPXTpk1DIBDgrvXGjRvR1NTEXet169Zx4nLJkiWoqanBhAkT3DmRIiQejyMcDtM1ziKnnXYa1q1bhzVr1hj/jjrqKMybN8/4TNc6N3R0dOCzzz7DkCFDCu+ezmq4M2GwePFiLRQKaQ8++KC2YcMG7fvf/75WV1fHRZ0T1rS3t2vvv/++9v7772sAtLvuukt7//33tS1btmialkg9r6ur0/71r39pH3zwgXbOOedIU8+POOIIbdWqVdqbb76pjRkzhlLPBS677DKttrZWe+2117gU0gMHDhjbXHrppdqIESO0V155RXv33Xe1xsZGrbGx0Vivp5CeccYZ2po1a7QXXnhBGzRoEKXqMlx//fXasmXLtM2bN2sffPCBdv3112sej0d76aWXNE2ja5xL2GwsTaNrnS2uvfZa7bXXXtM2b96svfXWW9rMmTO1gQMHart27dI0rbCuM4mdHHLvvfdqI0aM0ILBoHbMMcdoK1euzPeQiopXX31VA2D6N3/+fE3TEunnP/3pT7X6+notFAppp512mrZx40ZuH3v37tUuuOACraqqSqupqdEuvvhirb29PQ9nU7jIrjEA7YEHHjC26erq0n74wx9q/fr10yoqKrSvfe1r2o4dO7j9fPHFF9rs2bO18vJybeDAgdq1116r9fT0uHw2hct3vvMdbeTIkVowGNQGDRqknXbaaYbQ0TS6xrlEFDt0rbPD+eefrw0ZMkQLBoPaQQcdpJ1//vnapk2bjPWFdJ09mqZp2bUVEQRBEARBFA4Us0MQBEEQRElDYocgCIIgiJKGxA5BEARBECUNiR2CIAiCIEoaEjsEQRAEQZQ0JHYIgiAIgihpSOwQBEEQBFHSkNghCIIgCKKkIbFDEETR8sUXX8Dj8WDNmjU5O8a3v/1tnHvuuTnbP0EQuYfEDkEQeePb3/42PB6P6d+ZZ57p6PvDhw/Hjh07MGnSpByPlCCIYsaf7wEQBNG3OfPMM/HAAw9wy0KhkKPv+nw+NDQ05GJYBEGUEGTZIQgir4RCITQ0NHD/+vXrBwDweDy47777MHv2bJSXl+OQQw7B3//+d+O7ohtr//79mDdvHgYNGoTy8nKMGTOGE1Lr1q3DqaeeivLycgwYMADf//730dHRYayPxWK45pprUFdXhwEDBuBHP/oRxPaB8XgcCxcuxKhRo1BeXo6pU6dyYyIIovAgsUMQREHz05/+FHPnzsXatWsxb948fPOb38RHH32k3HbDhg14/vnn8dFHH+G+++7DwIEDAQCdnZ2YNWsW+vXrh3feeQdPPPEEXn75ZVx++eXG93/1q1/hwQcfxJ///Ge8+eab2LdvH5588knuGAsXLsRDDz2E3//+91i/fj2uvvpqfOtb38KyZctydxEIgugdWe+jThAE4ZD58+drPp9Pq6ys5P797//+r6ZpmgZAu/TSS7nvTJ8+Xbvssss0TdO0zZs3awC0999/X9M0TTv77LO1iy++WHqsP/zhD1q/fv20jo4OY9mzzz6reb1erbm5WdM0TRsyZIh25513Gut7enq0YcOGaeecc46maZrW3d2tVVRUaMuXL+f2fckll2gXXHBB5heCIIicQjE7BEHklVNOOQX33Xcft6x///7G58bGRm5dY2OjMvvqsssuw9y5c/Hee+/hjDPOwLnnnotjjz0WAPDRRx9h6tSpqKysNLY/7rjjEI/HsXHjRpSVlWHHjh2YPn26sd7v9+Ooo44yXFmbNm3CgQMHcPrpp3PHjUQiOOKII9I/eYIgXIHEDkEQeaWyshKjR4/Oyr5mz56NLVu24LnnnsOSJUtw2mmnYcGCBfjlL3+Zlf3r8T3PPvssDjroIG6d06BqgiDch2J2CIIoaFauXGn6efz48crtBw0ahPnz5+Phhx/Gr3/9a/zhD38AAIwfPx5r165FZ2ense1bb70Fr9eLsWPHora2FkOGDMGqVauM9dFoFKtXrzZ+njBhAkKhEJqamjB69Gju3/Dhw7N1ygRBZBmy7BAEkVfC4TCam5u5ZX6/3wgsfuKJJ3DUUUfh+OOPxyOPPIK3334b999/v3RfN910E6ZNm4aJEyciHA7jmWeeMYTRvHnzcPPNN2P+/Pm45ZZbsHv3blxxxRW48MILUV9fDwC48sorcccdd2DMmDEYN24c7rrrLrS0tBj7r66uxn//93/j6quvRjwex/HHH4/W1la89dZbqKmpwfz583NwhQiC6C0kdgiCyCsvvPAChgwZwi0bO3YsPv74YwDArbfeisWLF+OHP/whhgwZgr/97W+YMGGCdF/BYBA33HADvvjiC5SXl+OEE07A4sWLAQAVFRV48cUXceWVV+Loo49GRUUF5s6di7vuusv4/rXXXosdO3Zg/vz58Hq9+M53voOvfe1raG1tNba5/fbbMWjQICxcuBCff/456urqcOSRR+InP/lJti8NQRBZwqNpQhEJgiCIAsHj8eDJJ5+kdg0EQfQKitkhCIIgCKKkIbFDEARBEERJQzE7BEEULORlJwgiG5BlhyAIgiCIkobEDkEQBEEQJQ2JHYIgCIIgShoSOwRBEARBlDQkdgiCIAiCKGlI7BAEQRAEUdKQ2CEIgiAIoqQhsUMQBEEQREnz/wEFxyoodbYwNAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    env.seed(500)\n",
        "    torch.manual_seed(500)\n",
        "\n",
        "    num_inputs = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n\n",
        "    print('state size:', num_inputs)\n",
        "    print('action size:', num_actions)\n",
        "\n",
        "    net = TRPO(num_inputs, num_actions)\n",
        "\n",
        "    net.to(device)\n",
        "    net.train()\n",
        "    running_score = 0\n",
        "    steps = 0\n",
        "    loss = 0\n",
        "\n",
        "    # Create lists to hold scores and episodes\n",
        "    scores = []\n",
        "    episodes = []\n",
        "\n",
        "    for e in range(500):\n",
        "        done = False\n",
        "        memory = Memory()\n",
        "\n",
        "        score = 0\n",
        "        state = env.reset()\n",
        "        state = torch.Tensor(state).to(device)\n",
        "        state = state.unsqueeze(0)\n",
        "\n",
        "        while not done:\n",
        "            steps += 1\n",
        "\n",
        "            action = net.get_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            next_state = torch.Tensor(next_state)\n",
        "            next_state = next_state.unsqueeze(0)\n",
        "\n",
        "            mask = 0 if done else 1\n",
        "            reward = reward if not done or score == 499 else -1\n",
        "\n",
        "            action_one_hot = torch.zeros(2)\n",
        "            action_one_hot[action] = 1\n",
        "            memory.push(state, next_state, action_one_hot, reward, mask)\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "        loss = TRPO.train_model(net, memory.sample())\n",
        "\n",
        "        score = score if score == 500.0 else score + 1\n",
        "        running_score = 0.99 * running_score + 0.01 * score\n",
        "\n",
        "        # Append score and episode number to lists\n",
        "        scores.append(score)\n",
        "        episodes.append(e)\n",
        "\n",
        "        if e % log_interval == 0:\n",
        "            print('{} episode | score: {:.2f}'.format(e, running_score))\n",
        "\n",
        "        if running_score > goal_score:\n",
        "            break\n",
        "\n",
        "    # After all episodes, plot the scores\n",
        "    plt.plot(episodes, scores)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.show()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeWLF0sVYTZA"
      },
      "source": [
        "## MC - Try 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYif-OhvYrUj",
        "outputId": "8d383fda-f755-423b-861e-975558e9e69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 3, 64)             128       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3, 64)             4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3, 4)              260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4548 (17.77 KB)\n",
            "Trainable params: 4548 (17.77 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Makes gradient of function loss_fn wrt var_list and\n",
        "# flattens it to have a 1-D vector\n",
        "def flatgrad(loss_fn, var_list):\n",
        "\twith tf.GradientTape() as t:\n",
        "\t\tloss = loss_fn()\n",
        "\tgrads = t.gradient(loss, var_list, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        "\treturn tf.concat([tf.reshape(g, [-1]) for g in grads], axis=0)\n",
        "\n",
        "def nn_model(input_shape, output_shape, convolutional=False):\n",
        "\tmodel = keras.Sequential()\n",
        "\tif convolutional:\n",
        "\t\tmodel.add(layers.Lambda(lambda x: tf.cast(tf.image.resize(tf.image.rgb_to_grayscale(x), size=(32,32)), dtype=tf.float64)/256., input_shape=input_shape))\n",
        "\t\tmodel.add(layers.Conv2D(10, (3, 3), activation='relu'))\n",
        "\t\tmodel.add(layers.MaxPooling2D((3, 3)))\n",
        "\t\tmodel.add(layers.Conv2D(5, (3, 3), activation='relu'))\n",
        "\t\tmodel.add(layers.MaxPooling2D((3, 3)))\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t# else:\n",
        "\tmodel.add(layers.Dense(64, input_shape=input_shape, activation='relu'))\n",
        "\tmodel.add(layers.Dense(64, activation='relu'))\n",
        "\tmodel.add(layers.Dense(output_shape))\n",
        "\treturn model\n",
        "\n",
        "def nn_model2(input_shape, output_shape, convolutional=False):\n",
        "\tmodel = keras.Sequential()\n",
        "\tif convolutional:\n",
        "\t\tmodel.add(layers.Lambda(lambda x: tf.cast(tf.image.resize(tf.image.rgb_to_grayscale(tf.image.crop_to_bounding_box(x, 33,0,160,160)), size=(32,32)), dtype=tf.float64)/256., input_shape=input_shape))\n",
        "\t\tmodel.add(layers.Conv2D(20, (3, 3), activation='relu'))\n",
        "\t\tmodel.add(layers.MaxPooling2D((3, 3)))\n",
        "\t\tmodel.add(layers.Conv2D(20, (3, 3), activation='relu'))\n",
        "\t\tmodel.add(layers.MaxPooling2D((3, 3)))\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t# else:\n",
        "\tmodel.add(layers.Dense(128, input_shape=input_shape, activation='relu'))\n",
        "\tmodel.add(layers.Dense(64, activation='relu'))\n",
        "\tmodel.add(layers.Dense(output_shape))\n",
        "\treturn model\n",
        "\n",
        "\n",
        "def assign_vars(model, theta):\n",
        "\t\t\"\"\"\n",
        "\t\tCreate the process of assigning updated vars\n",
        "\t\t\"\"\"\n",
        "\t\tshapes = [v.shape.as_list() for v in model.trainable_variables]\n",
        "\t\tsize_theta = np.sum([np.prod(shape) for shape in shapes])\n",
        "\t\t# self.assign_weights_op = tf.assign(self.flat_weights, self.flat_wieghts_ph)\n",
        "\t\tstart = 0\n",
        "\t\tfor i, shape in enumerate(shapes):\n",
        "\t\t\tsize = np.prod(shape)\n",
        "\t\t\tparam = tf.reshape(theta[start:start + size], shape)\n",
        "\t\t\tmodel.trainable_variables[i].assign(param)\n",
        "\t\t\tstart += size\n",
        "\t\tassert start == size_theta, \"messy shapes\"\n",
        "\n",
        "def flatvars(model):\n",
        "\treturn tf.concat([tf.reshape(v, [-1]) for v in model.trainable_variables], axis=0)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmodel = nn_model((3,1), 4)\n",
        "\n",
        "\tmodel.summary()\n",
        "\n",
        "\tfv = flatvars(model).numpy()\n",
        "\n",
        "\tassign_vars(model, fv)\n",
        "\n",
        "\tfv_new = flatvars(model).numpy()\n",
        "\n",
        "\tprint((fv == fv_new).all())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89mQEPBnYYLM",
        "outputId": "db75f4d4-97f2-4d1e-d95b-132e7e7275a6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers, losses, models\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import gym\n",
        "import time\n",
        "import copy\n",
        "\n",
        "class TRPO:\n",
        "\tdef __init__(self, env_name, env, policy_model, value_model=None, value_lr=1e-1, gamma=0.99, delta = 0.01,\n",
        "\t\t\t\tcg_damping=0.001, cg_iters=10, residual_tol=1e-5, ent_coeff=0.0, epsilon=0.4,\n",
        "\t\t\t\tbacktrack_coeff=0.6, backtrack_iters=10, render=False, batch_size=4096, n_paths=10, n_threads=2, epsilon_decay=lambda x: x - 5e-3, reward_scaling = 1., correlated_epsilon=False):\n",
        "\t\tself.env_name = env_name\n",
        "\t\tself.N_PATHS = n_paths\n",
        "\t\tself.N_THREADS = n_threads\n",
        "\t\tself.envs = []\n",
        "\t\tself.epsilon_decay = epsilon_decay\n",
        "\t\tassert self.N_PATHS > 0 and self.N_THREADS > 0\n",
        "\t\tfor i in range(self.N_PATHS):\n",
        "\t\t\tself.envs.append(copy.deepcopy(env))\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.cg_iters = cg_iters\n",
        "\t\tself.cg_damping = cg_damping\n",
        "\t\tself.ent_coeff = ent_coeff\n",
        "\t\tself.residual_tol = residual_tol\n",
        "\t\tcurrent_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\t\tself.name = f\"mylogs/TRPO-{self.env_name}-{current_time}\"\n",
        "\t\tself.model = policy_model\n",
        "\t\tself.tmp_model = models.clone_model(self.model)\n",
        "\t\tself.value_model = value_model\n",
        "\t\tif self.value_model:\n",
        "\t\t\tself.value_optimizer = optimizers.Adam(lr=value_lr)\n",
        "\t\t\tself.value_model.compile(self.value_optimizer, \"mse\")\n",
        "\t\t\tself.writer = tf.summary.create_file_writer(self.name)\n",
        "\t\tself.delta = delta\n",
        "\t\tself.epsilon = epsilon\n",
        "\t\tself.backtrack_coeff = backtrack_coeff\n",
        "\t\tself.backtrack_iters = backtrack_iters\n",
        "\t\tself.render = render\n",
        "\t\tself.reward_scaling = reward_scaling\n",
        "\t\tself.correlated_epsilon = correlated_epsilon\n",
        "\t\tif render:\n",
        "\t\t\tos.system(\"touch render\")\n",
        "\t\telif not render and len(glob.glob(\"render\")) > 0:\n",
        "\t\t\tos.system(\"rm render\")\n",
        "\t\tself.BATCH_SIZE = batch_size\n",
        "\tdef close(self):\n",
        "\t\tfor env in self.envs:\n",
        "\t\t\tenv.close()\n",
        "\tdef __call__(self, ob, last_action=None):\n",
        "\t\tob = ob[np.newaxis, :]\n",
        "\t\tlogits = self.model(ob)\n",
        "\t\taction_prob = tf.nn.softmax(logits).numpy().ravel()\n",
        "\t\taction = np.random.choice(range(action_prob.shape[0]), p=action_prob)\n",
        "\t\t# epsilon greedy\n",
        "\t\tif np.random.uniform(0,1) < self.epsilon:\n",
        "\t\t\tif self.correlated_epsilon and np.random.uniform(0,1) < 0.8 and last_action is not None:\n",
        "\t\t\t\taction = last_action\n",
        "\t\t\telse:\n",
        "\t\t\t\taction = np.random.randint(0,self.envs[0].action_space.n)\n",
        "\t\tself.last_action = action\n",
        "\t\treturn action, action_prob\n",
        "\tdef render_episode(self, n=1):\n",
        "\t\tfor i in range(n):\n",
        "\t\t\tob = self.envs[0].reset()\n",
        "\t\t\tdone = False\n",
        "\t\t\taction = None\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\tself.envs[0].render()\n",
        "\t\t\t\taction, _ = self(ob, action)\n",
        "\t\t\t\tob, r, done, info = self.envs[0].step(action)\n",
        "\n",
        "\tdef load_weights(self, path):\n",
        "\t\tself.model.load_weights(path)\n",
        "\n",
        "\tdef sample(self, episode):\n",
        "\t\tobs_all, actions_all, rs_all, action_probs_all, Gs_all = [None]*self.N_PATHS, [None]*self.N_PATHS, [None]*self.N_PATHS, [None]*self.N_PATHS, [None]*self.N_PATHS\n",
        "\t\tmean_total_reward = [None]*self.N_PATHS\n",
        "\t\tmean_entropy = [None]*self.N_PATHS\n",
        "\t\tif len(glob.glob(\"render\")) > 0:\n",
        "\t\t\tself.render = True\n",
        "\t\telse:\n",
        "\t\t\tself.render = False\n",
        "\n",
        "\t\tif self.render:\n",
        "\t\t\tself.render_episode()\n",
        "\n",
        "\t\tdef generate_path(path):\n",
        "\t\t\tentropy = 0\n",
        "\t\t\tobs, actions, rs, action_probs, Gs = [], [], [], [], []\n",
        "\t\t\tob = self.envs[path].reset()\n",
        "\t\t\tdone = False\n",
        "\n",
        "\t\t\tlast_action = None\n",
        "\t\t\twhile not done:\n",
        "\t\t\t\taction, action_prob = self(ob, last_action)\n",
        "\t\t\t\tnew_ob, r, done, info = self.envs[path].step(action)\n",
        "\t\t\t\tlast_action = action\n",
        "\t\t\t\trs.append(r/self.reward_scaling)\n",
        "\t\t\t\tobs.append(ob)\n",
        "\t\t\t\tactions.append(action)\n",
        "\t\t\t\taction_probs.append(action_prob)\n",
        "\t\t\t\tentropy += - tf.reduce_sum(action_prob*tf.math.log(action_prob))\n",
        "\t\t\t\tob = new_ob\n",
        "\t\t\tG = 0\n",
        "\t\t\tfor r in rs[::-1]:\n",
        "\t\t\t\tG = r + self.gamma*G\n",
        "\t\t\t\tGs.insert(0, G)\n",
        "\t\t\tmean_total_reward[path] = sum(rs)\n",
        "\t\t\tentropy = entropy / len(actions)\n",
        "\t\t\tmean_entropy[path] = entropy\n",
        "\t\t\tobs_all[path] = obs\n",
        "\t\t\tactions_all[path] = actions\n",
        "\t\t\trs_all[path] = rs\n",
        "\t\t\taction_probs_all[path] = action_probs\n",
        "\t\t\tGs_all[path] = Gs\n",
        "\n",
        "\t\ti = 0\n",
        "\t\twhile i < self.N_PATHS:\n",
        "\t\t\tj = 0\n",
        "\t\t\tthreads = []\n",
        "\t\t\twhile j < self.N_THREADS and i < self.N_PATHS:\n",
        "\t\t\t\tthread = threading.Thread(target=generate_path, args=(i,))\n",
        "\t\t\t\tthread.start()\n",
        "\t\t\t\tthreads.append(thread)\n",
        "\t\t\t\tj += 1\n",
        "\t\t\t\ti += 1\n",
        "\t\t\tfor thread in threads:\n",
        "\t\t\t\tthread.join()\n",
        "\n",
        "\n",
        "\t\tmean_entropy = np.mean(mean_entropy)\n",
        "\t\tbest_reward = np.max(mean_total_reward)\n",
        "\t\tmean_total_reward = np.mean(mean_total_reward)\n",
        "\t\tGs_all = np.concatenate(Gs_all)\n",
        "\t\tobs_all = np.concatenate(obs_all)\n",
        "\t\trs_all = np.concatenate(rs_all)\n",
        "\t\tactions_all = np.concatenate(actions_all)\n",
        "\t\taction_probs_all = np.concatenate(action_probs_all)\n",
        "\t\treturn obs_all, Gs_all, mean_total_reward, best_reward, actions_all, action_probs_all, mean_entropy\n",
        "\n",
        "\tdef train_step(self, episode, obs_all, Gs_all, actions_all, action_probs_all, total_reward, best_reward, entropy, t0):\n",
        "\t\tdef surrogate_loss(theta=None):\n",
        "\t\t\tif theta is None:\n",
        "\t\t\t\tmodel = self.model\n",
        "\t\t\telse:\n",
        "\t\t\t\tmodel = self.tmp_model\n",
        "\t\t\t\tassign_vars(self.tmp_model, theta)\n",
        "\t\t\tlogits = model(obs)\n",
        "\t\t\taction_prob = tf.nn.softmax(logits)\n",
        "\t\t\taction_prob = tf.reduce_sum(actions_one_hot * action_prob, axis=1)\n",
        "\t\t\told_logits = self.model(obs)\n",
        "\t\t\told_action_prob = tf.nn.softmax(old_logits)\n",
        "\t\t\told_action_prob = tf.reduce_sum(actions_one_hot * old_action_prob, axis=1).numpy() + 1e-8\n",
        "\t\t\tprob_ratio = action_prob / old_action_prob # pi(a|s) / pi_old(a|s)\n",
        "\t\t\tloss = tf.reduce_mean(prob_ratio * advantage) + self.ent_coeff * entropy\n",
        "\t\t\treturn loss\n",
        "\n",
        "\t\tdef kl_fn(theta=None):\n",
        "\t\t\tif theta is None:\n",
        "\t\t\t\tmodel = self.model\n",
        "\t\t\telse:\n",
        "\t\t\t\tmodel = self.tmp_model\n",
        "\t\t\t\tassign_vars(self.tmp_model, theta)\n",
        "\t\t\tlogits = model(obs)\n",
        "\t\t\taction_prob = tf.nn.softmax(logits).numpy() + 1e-8\n",
        "\t\t\told_logits = self.model(obs)\n",
        "\t\t\told_action_prob = tf.nn.softmax(old_logits)\n",
        "\t\t\treturn tf.reduce_mean(tf.reduce_sum(old_action_prob * tf.math.log(old_action_prob / action_prob), axis=1))\n",
        "\n",
        "\t\tdef hessian_vector_product(p):\n",
        "\t\t\tdef hvp_fn():\n",
        "\t\t\t\tkl_grad_vector = flatgrad(kl_fn, self.model.trainable_variables)\n",
        "\t\t\t\tgrad_vector_product = tf.reduce_sum(kl_grad_vector * p)\n",
        "\t\t\t\treturn grad_vector_product\n",
        "\n",
        "\t\t\tfisher_vector_product = flatgrad(hvp_fn, self.model.trainable_variables).numpy()\n",
        "\t\t\treturn fisher_vector_product + (self.cg_damping * p)\n",
        "\n",
        "\t\tdef conjugate_grad(Ax, b):\n",
        "\t\t\t\"\"\"\n",
        "\t\t\tConjugate gradient algorithm\n",
        "\t\t\t(see https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
        "\t\t\t\"\"\"\n",
        "\t\t\tx = np.zeros_like(b)\n",
        "\t\t\tr = b.copy() # Note: should be 'b - Ax(x)', but for x=0, Ax(x)=0. Change if doing warm start.\n",
        "\t\t\tp = r.copy()\n",
        "\t\t\told_p = p.copy()\n",
        "\t\t\tr_dot_old = np.dot(r,r)\n",
        "\t\t\tfor _ in range(self.cg_iters):\n",
        "\t\t\t\tz = Ax(p)\n",
        "\t\t\t\talpha = r_dot_old / (np.dot(p, z) + 1e-8)\n",
        "\t\t\t\told_x = x\n",
        "\t\t\t\tx += alpha * p\n",
        "\t\t\t\tr -= alpha * z\n",
        "\t\t\t\tr_dot_new = np.dot(r,r)\n",
        "\t\t\t\tbeta = r_dot_new / (r_dot_old + 1e-8)\n",
        "\t\t\t\tr_dot_old = r_dot_new\n",
        "\t\t\t\tif r_dot_old < self.residual_tol:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\told_p = p.copy()\n",
        "\t\t\t\tp = r + beta * p\n",
        "\t\t\t\tif np.isnan(x).any():\n",
        "\t\t\t\t\tprint(\"x is nan\")\n",
        "\t\t\t\t\tprint(\"z\", np.isnan(z))\n",
        "\t\t\t\t\tprint(\"old_x\", np.isnan(old_x))\n",
        "\t\t\t\t\tprint(\"kl_fn\", np.isnan(kl_fn()))\n",
        "\t\t\treturn x\n",
        "\n",
        "\t\tdef linesearch(x, fullstep):\n",
        "\t\t\tfval = surrogate_loss(x)\n",
        "\t\t\tfor (_n_backtracks, stepfrac) in enumerate(self.backtrack_coeff**np.arange(self.backtrack_iters)):\n",
        "\t\t\t\txnew = x + stepfrac * fullstep\n",
        "\t\t\t\tnewfval = surrogate_loss(xnew)\n",
        "\t\t\t\tkl_div = kl_fn(xnew)\n",
        "\t\t\t\tif np.isnan(kl_div):\n",
        "\t\t\t\t\tprint(\"kl is nan\")\n",
        "\t\t\t\t\tprint(\"xnew\", np.isnan(xnew))\n",
        "\t\t\t\t\tprint(\"x\", np.isnan(x))\n",
        "\t\t\t\t\tprint(\"stepfrac\", np.isnan(stepfrac))\n",
        "\t\t\t\t\tprint(\"fullstep\",  np.isnan(fullstep))\n",
        "\t\t\t\tif kl_div <= self.delta and newfval >= 0:\n",
        "\t\t\t\t\tprint(\"Linesearch worked at \", _n_backtracks)\n",
        "\t\t\t\t\treturn xnew\n",
        "\t\t\t\tif _n_backtracks == self.backtrack_iters - 1:\n",
        "\t\t\t\t\tprint(\"Linesearch failed.\", kl_div, newfval)\n",
        "\t\t\treturn x\n",
        "\n",
        "\t\tNBATCHES = len(obs_all) // self.BATCH_SIZE\n",
        "\t\tif len(obs_all) < self.BATCH_SIZE:\n",
        "\t\t\tNBATCHES += 1\n",
        "\t\tfor batch_id in range(NBATCHES):\n",
        "\t\t\tobs = obs_all[batch_id*self.BATCH_SIZE: (batch_id + 1)*self.BATCH_SIZE]\n",
        "\t\t\tGs = Gs_all[batch_id*self.BATCH_SIZE: (batch_id + 1)*self.BATCH_SIZE]\n",
        "\t\t\tactions = actions_all[batch_id*self.BATCH_SIZE: (batch_id + 1)*self.BATCH_SIZE]\n",
        "\t\t\taction_probs = action_probs_all[batch_id*self.BATCH_SIZE: (batch_id + 1)*self.BATCH_SIZE]\n",
        "\n",
        "\n",
        "\t\t\tVs = self.value_model(obs).numpy().flatten()\n",
        "\t\t\t# advantage = Gs\n",
        "\t\t\tadvantage = Gs - Vs\n",
        "\t\t\tadvantage = (advantage - advantage.mean())/(advantage.std() + 1e-8)\n",
        "\t\t\tactions_one_hot = tf.one_hot(actions, self.envs[0].action_space.n, dtype=\"float64\")\n",
        "\t\t\tpolicy_loss = surrogate_loss()\n",
        "\t\t\tpolicy_gradient = flatgrad(surrogate_loss, self.model.trainable_variables).numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\t\tstep_direction = conjugate_grad(hessian_vector_product, policy_gradient)\n",
        "\n",
        "\t\t\tshs = .5 * step_direction.dot(hessian_vector_product(step_direction).T)\n",
        "\n",
        "\t\t\tlm = np.sqrt(shs / self.delta) + 1e-8\n",
        "\t\t\tfullstep = step_direction / lm\n",
        "\t\t\tif np.isnan(fullstep).any():\n",
        "\t\t\t\tprint(\"fullstep is nan\")\n",
        "\t\t\t\tprint(\"lm\", lm)\n",
        "\t\t\t\tprint(\"step_direction\", step_direction)\n",
        "\t\t\t\tprint(\"policy_gradient\", policy_gradient)\n",
        "\n",
        "\t\t\toldtheta = flatvars(self.model).numpy()\n",
        "\n",
        "\t\t\ttheta = linesearch(oldtheta, fullstep)\n",
        "\n",
        "\n",
        "\t\t\tif np.isnan(theta).any():\n",
        "\t\t\t\tprint(\"NaN detected. Skipping update...\")\n",
        "\t\t\telse:\n",
        "\t\t\t\tassign_vars(self.model, theta)\n",
        "\n",
        "\t\t\tkl = kl_fn(oldtheta)\n",
        "\n",
        "\t\t\thistory = self.value_model.fit(obs, Gs, epochs=5, verbose=0)\n",
        "\t\t\tvalue_loss = history.history[\"loss\"][-1]\n",
        "\n",
        "\n",
        "\t\t\tprint(f\"Ep {episode}.{batch_id}: Rw_mean {total_reward} - Rw_best {best_reward} - PL {policy_loss} - VL {value_loss} - KL {kl} - epsilon {self.epsilon} - time {time.time() - t0}\")\n",
        "\t\tif self.value_model:\n",
        "\t\t\twriter = self.writer\n",
        "\t\t\twith writer.as_default():\n",
        "\t\t\t\ttf.summary.scalar(\"reward\", total_reward, step=episode)\n",
        "\t\t\t\ttf.summary.scalar(\"best_reward\", best_reward, step=episode)\n",
        "\t\t\t\ttf.summary.scalar(\"value_loss\", value_loss, step=episode)\n",
        "\t\t\t\ttf.summary.scalar(\"policy_loss\", policy_loss, step=episode)\n",
        "\t\tself.epsilon = self.epsilon_decay(self.epsilon)\n",
        "\n",
        "\tdef train(self, episodes):\n",
        "\t\tassert self.value_model is not None\n",
        "\t\tprint(\"Starting training, saving checkpoints and logs to:\", self.name)\n",
        "\t\tfor episode in range(episodes):\n",
        "\t\t\tt0 = time.time()\n",
        "\t\t\tobs, Gs, total_reward, best_reward, actions, action_probs, entropy = self.sample(episode)\n",
        "\t\t\tprint(f\"Sample Time {time.time() - t0}\")\n",
        "\t\t\ttotal_loss = self.train_step(episode, obs, Gs, actions, action_probs, total_reward, best_reward, entropy, t0)\n",
        "\t\t\tif episode % 10 == 0 and episode != 0 and self.value_model:\n",
        "\t\t\t\tself.model.save_weights(f\"{self.name}/{episode}.ckpt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zRvw4DhmS5ts",
        "outputId": "47f1182e-5410-4bdd-ddc6-e5c72dd4b885"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "Exception in thread Thread-5:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "Exception in thread Thread-6:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "Exception in thread Thread-7:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "Exception in thread Thread-8:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Tensorflow 2.13.0\n",
            "Playing in MountainCar-v0\n",
            "Starting training, saving checkpoints and logs to: mylogs/TRPO-MountainCar-v0-Apr09_16-26-40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-9:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "Exception in thread Thread-10:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "Exception in thread Thread-11:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "Exception in thread Thread-12:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "Exception in thread Thread-13:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "Exception in thread Thread-14:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self.run()\n",
            "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 100, in generate_path\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "  File \"C:\\Users\\chery\\AppData\\Local\\Temp\\ipykernel_591056\\4231894702.py\", line 55, in __call__\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n",
            "TypeError: tuple indices must be integers or slices, not tuple\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlaying in\u001b[39m\u001b[38;5;124m\"\u001b[39m, env_name)\n\u001b[0;32m     24\u001b[0m agent \u001b[38;5;241m=\u001b[39m TRPO(env_name, env, policy_model, value_model, render\u001b[38;5;241m=\u001b[39mrender, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m---> 25\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m agent\u001b[38;5;241m.\u001b[39mclose()\n",
            "Cell \u001b[1;32mIn[2], line 297\u001b[0m, in \u001b[0;36mTRPO.train\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m    296\u001b[0m \tt0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 297\u001b[0m \tobs, Gs, total_reward, best_reward, actions, action_probs, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    299\u001b[0m \ttotal_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(episode, obs, Gs, actions, action_probs, total_reward, best_reward, entropy, t0)\n",
            "Cell \u001b[1;32mIn[2], line 136\u001b[0m, in \u001b[0;36mTRPO.sample\u001b[1;34m(self, episode)\u001b[0m\n\u001b[0;32m    132\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m    133\u001b[0m \t\tthread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m--> 136\u001b[0m mean_entropy \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_entropy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m best_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(mean_total_reward)\n\u001b[0;32m    138\u001b[0m mean_total_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(mean_total_reward)\n",
            "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\numpy\\core\\_methods.py:181\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    178\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    179\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
            "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import gym\n",
        "\n",
        "# Define your variables here\n",
        "env_name = \"MountainCar-v0\"\n",
        "episodes = 500  # replace with your number of episodes\n",
        "render = False  # replace with your preference (True or False)\n",
        "\n",
        "print(\"Using Tensorflow\", tf.__version__)\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "config = {\n",
        "\t\"correlated_epsilon\" : True\n",
        "}\n",
        "\n",
        "# Create an instance of the environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "policy_model = nn_model((2,), 3)\n",
        "value_model = nn_model((2,), 1)\n",
        "\n",
        "print(\"Playing in\", env_name)\n",
        "\n",
        "agent = TRPO(env_name, env, policy_model, value_model, render=render, **config)\n",
        "agent.train(episodes)\n",
        "agent.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke6D0hOmfyjr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPEtUavlfy8G"
      },
      "source": [
        "## MC - Try 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "GGZOkOwZfy8H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "gamma = 0.99\n",
        "goal_score = 0\n",
        "log_interval = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "max_kl = 0.01\n",
        "\n",
        "def flat_grad(grads):\n",
        "    grad_flatten = []\n",
        "    for grad in grads:\n",
        "        grad_flatten.append(grad.view(-1))\n",
        "    grad_flatten = torch.cat(grad_flatten)\n",
        "    return grad_flatten\n",
        "\n",
        "def flat_hessian(hessians):\n",
        "    hessians_flatten = []\n",
        "    for hessian in hessians:\n",
        "        hessians_flatten.append(hessian.contiguous().view(-1))\n",
        "    hessians_flatten = torch.cat(hessians_flatten).data\n",
        "    return hessians_flatten\n",
        "\n",
        "def flat_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.data.view(-1))\n",
        "    params_flatten = torch.cat(params)\n",
        "    return params_flatten\n",
        "\n",
        "def update_model(model, new_params):\n",
        "    index = 0\n",
        "    for params in model.parameters():\n",
        "        params_length = len(params.view(-1))\n",
        "        new_param = new_params[index: index + params_length]\n",
        "        new_param = new_param.view(params.size())\n",
        "        params.data.copy_(new_param)\n",
        "        index += params_length\n",
        "\n",
        "def kl_divergence(policy, old_policy):\n",
        "    kl = old_policy * torch.log(old_policy / policy)\n",
        "\n",
        "    kl = kl.sum(1, keepdim=True)\n",
        "    return kl\n",
        "\n",
        "def fisher_vector_product(net, states, p, cg_damp=0.1):\n",
        "    policy = net(states)\n",
        "    old_policy = net(states).detach()\n",
        "    kl = kl_divergence(policy, old_policy)\n",
        "    kl = kl.mean()\n",
        "    kl_grad = torch.autograd.grad(kl, net.parameters(), create_graph=True) # create_graph is True if we need higher order derivative products\n",
        "    kl_grad = flat_grad(kl_grad)\n",
        "\n",
        "    kl_grad_p = (kl_grad * p.detach()).sum()\n",
        "    kl_hessian_p = torch.autograd.grad(kl_grad_p, net.parameters())\n",
        "    kl_hessian_p = flat_hessian(kl_hessian_p)\n",
        "\n",
        "    return kl_hessian_p + cg_damp * p.detach()\n",
        "\n",
        "\n",
        "def conjugate_gradient(net, states, loss_grad, n_step=10, residual_tol=1e-10):\n",
        "    x = torch.zeros(loss_grad.size())\n",
        "    r = loss_grad.clone()\n",
        "    p = loss_grad.clone()\n",
        "    r_dot_r = torch.dot(r, r)\n",
        "\n",
        "    for i in range(n_step):\n",
        "        A_dot_p = fisher_vector_product(net, states, p)\n",
        "        alpha = r_dot_r / torch.dot(p, A_dot_p)\n",
        "        x += alpha * p\n",
        "        r -= alpha * A_dot_p\n",
        "        new_r_dot_r = torch.dot(r,r)\n",
        "        betta = new_r_dot_r / r_dot_r\n",
        "        p = r + betta * p\n",
        "        r_dot_r = new_r_dot_r\n",
        "        if r_dot_r < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "class TRPO(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(TRPO, self).__init__()\n",
        "        self.t = 0\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        self.fc_1 = nn.Linear(num_inputs, 256)\n",
        "        self.fc_2 = nn.Linear(256, num_outputs)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        x = torch.relu(self.fc_1(input))\n",
        "        logits = self.fc_2(x)\n",
        "        logits = logits - logits.max(dim=-1, keepdim=True).values\n",
        "        policy = F.softmax(logits / 0.1 , dim=-1)\n",
        "        if torch.isnan(policy).any():\n",
        "            print(\"Warning: NaN detected in policy output.\")\n",
        "            policy = torch.nan_to_num(policy)  # Replace NaNs with 0, but use cautiously\n",
        "   \n",
        "        return policy\n",
        "\n",
        "    @classmethod\n",
        "    def train_model(cls, net, transitions):\n",
        "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
        "\n",
        "        states = torch.stack(states)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        masks = torch.Tensor(masks)\n",
        "\n",
        "        returns = torch.zeros_like(rewards)\n",
        "\n",
        "        running_return = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
        "            returns[t] = running_return\n",
        "\n",
        "        policy = net(states)\n",
        "        policy = policy.view(-1, net.num_outputs)\n",
        "        policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        old_policy = net(states).detach()\n",
        "        old_policy = old_policy.view(-1, net.num_outputs)\n",
        "        old_policy_action = (old_policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
        "\n",
        "        surrogate_loss_grad = torch.autograd.grad(surrogate_loss, net.parameters())\n",
        "        surrogate_loss_grad = flat_grad(surrogate_loss_grad)\n",
        "\n",
        "        step_dir = conjugate_gradient(net, states, surrogate_loss_grad.data)\n",
        "\n",
        "        params = flat_params(net)\n",
        "        shs = (step_dir * fisher_vector_product(net, states, step_dir)).sum(0, keepdim=True)\n",
        "        step_size = torch.sqrt((2 * max_kl) / shs)[0]\n",
        "        #print('step_size', step_size)\n",
        "        full_step = step_size * step_dir\n",
        "\n",
        "        fraction = 1.0\n",
        "        for _ in range(10):\n",
        "            new_params = params + fraction * full_step\n",
        "            update_model(net, new_params)\n",
        "            policy = net(states)\n",
        "            policy = policy.view(-1, net.num_outputs)\n",
        "            policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "            entropy = -(policy * policy.log()).sum(1, keepdim=True).mean()\n",
        "            beta = .9\n",
        "            surrogate_loss = ((policy_action / old_policy_action) * returns).mean() - beta * entropy\n",
        "\n",
        "            kl = kl_divergence(policy, old_policy)\n",
        "            kl = kl.mean()\n",
        "\n",
        "            if kl < max_kl:\n",
        "                break\n",
        "            fraction = fraction * 0.5\n",
        "\n",
        "        return -surrogate_loss\n",
        "\n",
        "    def get_action(self, input):\n",
        "        policy = self.forward(input)\n",
        "        policy = policy[0].data.numpy()\n",
        "        #print(policy)\n",
        "        # Check if policy contains NaN values\n",
        "        if np.isnan(policy).any():\n",
        "          print(\"Policy contains NaN values:\", policy)\n",
        "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
        "        #print(action)\n",
        "        return action\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque()\n",
        "\n",
        "    def push(self, state, next_state, action, reward, mask):\n",
        "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
        "\n",
        "    def sample(self):\n",
        "        memory = self.memory\n",
        "        return Transition(*zip(*memory))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "5M0Yq6K9fy8I",
        "outputId": "6b5eebc3-2801-495e-c554-8c6c872b3623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000\n",
            "state size: 2\n",
            "action size: 3\n",
            "0 episode | score: -5000.00\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "    env = gym.make('MountainCar-v0')  # Change the environment here\n",
        "    env._max_episode_steps = 5000\n",
        "    max_step = env._max_episode_steps\n",
        "    ep = 2000\n",
        "    print(env._max_episode_steps)\n",
        "    #env.seed(500)\n",
        "    torch.manual_seed(500)\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    num_inputs = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n  # Update the number of actions\n",
        "    print('state size:', num_inputs)\n",
        "    print('action size:', num_actions)\n",
        "\n",
        "    net = TRPO(num_inputs, num_actions)\n",
        "\n",
        "    net.to(device)\n",
        "    net.train()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Create lists to hold scores and episodes\n",
        "    scores = []\n",
        "    episodes = []\n",
        "\n",
        "    for e in range(ep):\n",
        "        done = False\n",
        "        memory = Memory()\n",
        "        running_score = 0\n",
        "        steps = 0\n",
        "        score = 0\n",
        "        state, _ = env.reset()\n",
        "        state = torch.Tensor(state).to(device)\n",
        "        state = state.unsqueeze(0)\n",
        "\n",
        "        while not done and steps < max_step:\n",
        "            steps += 1\n",
        "\n",
        "            action = net.get_action(state)\n",
        "            next_state, reward, done, info, _ = env.step(action)  # No need to wrap action in a list\n",
        "            #print(reward, steps)\n",
        "\n",
        "            next_state = torch.Tensor(next_state)\n",
        "            next_state = next_state.unsqueeze(0)\n",
        "\n",
        "            mask = 0 if done else 1\n",
        "            reward = reward if not done or score == 499 else -1\n",
        "\n",
        "            action_one_hot = torch.zeros(num_actions)  # Update the size of action_one_hot\n",
        "            action_one_hot[action] = 1\n",
        "            memory.push(state, next_state, action_one_hot, reward, mask)\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "        loss = TRPO.train_model(net, memory.sample())\n",
        "\n",
        "        score = score #if score == 500.0 else score + 1\n",
        "        running_score = score #0.99 * running_score + 0.01 * score\n",
        "\n",
        "        # Append score and episode number to lists\n",
        "        scores.append(score)\n",
        "        episodes.append(e)\n",
        "\n",
        "        if e % log_interval == 0:\n",
        "            print('{} episode | score: {:.2f}'.format(e, running_score))\n",
        "\n",
        "        if running_score > goal_score:\n",
        "            break\n",
        "\n",
        "    # After all episodes, plot the scores\n",
        "    plt.plot(episodes, scores)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.show()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "gamma = 0.99\n",
        "goal_score = 200\n",
        "log_interval = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "max_kl = 0.01\n",
        "\n",
        "def flat_grad(grads):\n",
        "    grad_flatten = []\n",
        "    for grad in grads:\n",
        "        grad_flatten.append(grad.view(-1))\n",
        "    grad_flatten = torch.cat(grad_flatten)\n",
        "    return grad_flatten\n",
        "\n",
        "def flat_hessian(hessians):\n",
        "    hessians_flatten = []\n",
        "    for hessian in hessians:\n",
        "        hessians_flatten.append(hessian.contiguous().view(-1))\n",
        "    hessians_flatten = torch.cat(hessians_flatten).data\n",
        "    return hessians_flatten\n",
        "\n",
        "def flat_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.data.view(-1))\n",
        "    params_flatten = torch.cat(params)\n",
        "    return params_flatten\n",
        "\n",
        "def update_model(model, new_params):\n",
        "    index = 0\n",
        "    for params in model.parameters():\n",
        "        params_length = len(params.view(-1))\n",
        "        new_param = new_params[index: index + params_length]\n",
        "        new_param = new_param.view(params.size())\n",
        "        params.data.copy_(new_param)\n",
        "        index += params_length\n",
        "\n",
        "def kl_divergence(policy, old_policy):\n",
        "    kl = old_policy * torch.log(old_policy / policy)\n",
        "\n",
        "    kl = kl.sum(1, keepdim=True)\n",
        "    return kl\n",
        "\n",
        "def fisher_vector_product(net, states, p, cg_damp=0.1):\n",
        "    policy = net(states)\n",
        "    old_policy = net(states).detach()\n",
        "    kl = kl_divergence(policy, old_policy)\n",
        "    kl = kl.mean()\n",
        "    kl_grad = torch.autograd.grad(kl, net.parameters(), create_graph=True) # create_graph is True if we need higher order derivative products\n",
        "    kl_grad = flat_grad(kl_grad)\n",
        "\n",
        "    kl_grad_p = (kl_grad * p.detach()).sum()\n",
        "    kl_hessian_p = torch.autograd.grad(kl_grad_p, net.parameters())\n",
        "    kl_hessian_p = flat_hessian(kl_hessian_p)\n",
        "\n",
        "    return kl_hessian_p + cg_damp * p.detach()\n",
        "\n",
        "\n",
        "def conjugate_gradient(net, states, loss_grad, n_step=10, residual_tol=1e-10):\n",
        "    x = torch.zeros(loss_grad.size())\n",
        "    r = loss_grad.clone()\n",
        "    p = loss_grad.clone()\n",
        "    r_dot_r = torch.dot(r, r)\n",
        "\n",
        "    for i in range(n_step):\n",
        "        A_dot_p = fisher_vector_product(net, states, p)\n",
        "        alpha = r_dot_r / torch.dot(p, A_dot_p)\n",
        "        x += alpha * p\n",
        "        r -= alpha * A_dot_p\n",
        "        new_r_dot_r = torch.dot(r,r)\n",
        "        betta = new_r_dot_r / r_dot_r\n",
        "        p = r + betta * p\n",
        "        r_dot_r = new_r_dot_r\n",
        "        if r_dot_r < residual_tol:\n",
        "            break\n",
        "    return x\n",
        "\n",
        "class TRPO(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super(TRPO, self).__init__()\n",
        "        self.t = 0\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        self.fc_1 = nn.Linear(num_inputs, 256)\n",
        "        self.fc_2 = nn.Linear(256, num_outputs)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = torch.relu(self.fc_1(input))\n",
        "        policy = F.softmax(self.fc_2(x)/10, dim = -1)\n",
        "\n",
        "        return policy\n",
        "\n",
        "    @classmethod\n",
        "    def train_model(cls, net, transitions):\n",
        "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
        "\n",
        "        states = torch.stack(states)\n",
        "        actions = torch.stack(actions)\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        masks = torch.Tensor(masks)\n",
        "\n",
        "        returns = torch.zeros_like(rewards)\n",
        "\n",
        "        running_return = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
        "            returns[t] = running_return\n",
        "\n",
        "        policy = net(states)\n",
        "        policy = policy.view(-1, net.num_outputs)\n",
        "        policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        old_policy = net(states).detach()\n",
        "        old_policy = old_policy.view(-1, net.num_outputs)\n",
        "        old_policy_action = (old_policy * actions.detach()).sum(dim=1)\n",
        "\n",
        "        surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
        "\n",
        "        surrogate_loss_grad = torch.autograd.grad(surrogate_loss, net.parameters())\n",
        "        surrogate_loss_grad = flat_grad(surrogate_loss_grad)\n",
        "\n",
        "        step_dir = conjugate_gradient(net, states, surrogate_loss_grad.data)\n",
        "\n",
        "        params = flat_params(net)\n",
        "        shs = (step_dir * fisher_vector_product(net, states, step_dir)).sum(0, keepdim=True)\n",
        "        step_size = torch.sqrt((2 * max_kl) / shs)[0]\n",
        "        #print('step_size', step_size)\n",
        "        full_step = step_size * step_dir\n",
        "\n",
        "        fraction = 1.0\n",
        "        for _ in range(10):\n",
        "            new_params = params + fraction * full_step\n",
        "            update_model(net, new_params)\n",
        "            policy = net(states)\n",
        "            policy = policy.view(-1, net.num_outputs)\n",
        "            policy_action = (policy * actions.detach()).sum(dim=1)\n",
        "            entropy = -(policy * policy.log()).sum(1, keepdim=True).mean()\n",
        "            beta = .9\n",
        "            surrogate_loss = ((policy_action / old_policy_action) * returns).mean() - beta * entropy\n",
        "\n",
        "            kl = kl_divergence(policy, old_policy)\n",
        "            kl = kl.mean()\n",
        "\n",
        "            if kl < max_kl:\n",
        "                break\n",
        "            fraction = fraction * 0.5\n",
        "\n",
        "        return -surrogate_loss\n",
        "\n",
        "    def get_action(self, input):\n",
        "        policy = self.forward(input)\n",
        "        policy = policy[0].data.numpy()\n",
        "        #print(policy)\n",
        "        # Check if policy contains NaN values\n",
        "        if np.isnan(policy).any():\n",
        "          print(\"Policy contains NaN values:\", policy)\n",
        "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
        "        #print(action)\n",
        "        return action\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
        "\n",
        "class Memory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque()\n",
        "\n",
        "    def push(self, state, next_state, action, reward, mask):\n",
        "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
        "\n",
        "    def sample(self):\n",
        "        memory = self.memory\n",
        "        return Transition(*zip(*memory))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000\n",
            "state size: 2\n",
            "action size: 1\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[96], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[96], line 43\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     42\u001b[0m action \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[1;32m---> 43\u001b[0m next_state, reward, done, info, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# No need to wrap action in a list\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#print(reward, steps)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(next_state)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    216\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    217\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\envs\\classic_control\\continuous_mountain_car.py:146\u001b[0m, in \u001b[0;36mContinuous_MountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    144\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    145\u001b[0m velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 146\u001b[0m force \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_action), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_action)\n\u001b[0;32m    148\u001b[0m velocity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m force \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.0025\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m position)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m velocity \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_speed:\n",
            "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "def main():\n",
        "    env = gym.make('MountainCarContinuous-v0')  # Change the environment here\n",
        "    env._max_episode_steps = 5000\n",
        "    max_step = env._max_episode_steps\n",
        "    ep = 2000\n",
        "    print(env._max_episode_steps)\n",
        "    #env.seed(500)\n",
        "    torch.manual_seed(500)\n",
        "\n",
        "    num_inputs = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n  # Update the number of actions\n",
        "    print('state size:', num_inputs)\n",
        "    print('action size:', num_actions)\n",
        "\n",
        "    net = TRPO(num_inputs, num_actions)\n",
        "\n",
        "    net.to(device)\n",
        "    net.train()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # Create lists to hold scores and episodes\n",
        "    scores = []\n",
        "    episodes = []\n",
        "\n",
        "    for e in range(ep):\n",
        "        done = False\n",
        "        memory = Memory()\n",
        "        running_score = 0\n",
        "        steps = 0\n",
        "        score = 0\n",
        "        state, _ = env.reset()\n",
        "        state = torch.Tensor(state).to(device)\n",
        "        state = state.unsqueeze(0)\n",
        "\n",
        "        while not done and steps < max_step:\n",
        "            steps += 1\n",
        "\n",
        "            action = net.get_action(state)\n",
        "            next_state, reward, done, info, _ = env.step(action)  # No need to wrap action in a list\n",
        "            #print(reward, steps)\n",
        "\n",
        "            next_state = torch.Tensor(next_state)\n",
        "            next_state = next_state.unsqueeze(0)\n",
        "\n",
        "            mask = 0 if done else 1\n",
        "            reward = reward if not done or score == 499 else -1\n",
        "\n",
        "            action_one_hot = torch.zeros(num_actions)  # Update the size of action_one_hot\n",
        "            action_one_hot[action] = 1\n",
        "            memory.push(state, next_state, action_one_hot, reward, mask)\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "        loss = TRPO.train_model(net, memory.sample())\n",
        "\n",
        "        score = score #if score == 500.0 else score + 1\n",
        "        running_score = score #0.99 * running_score + 0.01 * score\n",
        "\n",
        "        # Append score and episode number to lists\n",
        "        scores.append(score)\n",
        "        episodes.append(e)\n",
        "\n",
        "        if e % log_interval == 0:\n",
        "            print('{} episode | score: {:.2f}'.format(e, running_score))\n",
        "\n",
        "        if running_score > goal_score:\n",
        "            break\n",
        "\n",
        "    # After all episodes, plot the scores\n",
        "    plt.plot(episodes, scores)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.show()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
