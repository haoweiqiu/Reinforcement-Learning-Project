{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AgentConfig:\n",
    "    # Learning\n",
    "    gamma = 0.99\n",
    "    plot_every = 10\n",
    "    update_freq = 1\n",
    "    k_epoch = 3\n",
    "    learning_rate = 0.02\n",
    "    lmbda = 0.95\n",
    "    eps_clip = 0.2\n",
    "    v_coef = 1\n",
    "    entropy_coef = 0.01\n",
    "\n",
    "    # Memory\n",
    "    memory_size = 400\n",
    "\n",
    "    train_cartpole = True\n",
    "\n",
    "class MlpPolicy(nn.Module):\n",
    "    def __init__(self, action_size, input_size=4):\n",
    "        super(MlpPolicy, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(self.input_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3_pi = nn.Linear(24, self.action_size)\n",
    "        self.fc3_v = nn.Linear(24, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def pi(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_pi(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def v(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_v(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Agent(AgentConfig):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.action_size = self.env.action_space.n  # 2 for cartpole\n",
    "        if self.train_cartpole:\n",
    "            self.policy_network = MlpPolicy(action_size=self.action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=self.k_epoch,\n",
    "                                                   gamma=0.999)\n",
    "        self.loss = 0\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.memory = {\n",
    "            'state': [], 'action': [], 'reward': [], 'next_state': [], 'action_prob': [], 'terminal': [], 'count': 0,\n",
    "            'advantage': [], 'td_target': torch.FloatTensor([])\n",
    "        }\n",
    "\n",
    "    def new_random_game(self):\n",
    "        self.env.reset()\n",
    "        action = self.env.action_space.sample()\n",
    "        obs, reward, terminal, info = self.env.step(action)\n",
    "        return obs, reward, action, terminal\n",
    "\n",
    "    def train(self):\n",
    "        episode = 0\n",
    "        step = 0\n",
    "        reward_history = []\n",
    "        avg_reward = []\n",
    "        solved = False\n",
    "\n",
    "        # A new episode\n",
    "        while not solved:\n",
    "            start_step = step\n",
    "            episode += 1\n",
    "            episode_length = 0\n",
    "\n",
    "            # Get initial state\n",
    "            state, reward, action, terminal = self.new_random_game()\n",
    "            current_state = state\n",
    "            total_episode_reward = 1\n",
    "\n",
    "            # A step in an episode\n",
    "            while not solved:\n",
    "                step += 1\n",
    "                episode_length += 1\n",
    "\n",
    "                # Choose action\n",
    "                prob_a = self.policy_network.pi(torch.FloatTensor(current_state).to(device))\n",
    "                # print(prob_a)\n",
    "                action = torch.distributions.Categorical(prob_a).sample().item()\n",
    "\n",
    "                # Act\n",
    "                state, reward, terminal, _ = self.env.step(action)\n",
    "                new_state = state\n",
    "\n",
    "                reward = -1 if terminal else reward\n",
    "\n",
    "                self.add_memory(current_state, action, reward/10.0, new_state, terminal, prob_a[action].item())\n",
    "\n",
    "                current_state = new_state\n",
    "                total_episode_reward += reward\n",
    "\n",
    "                if terminal:\n",
    "                    episode_length = step - start_step\n",
    "                    reward_history.append(total_episode_reward)\n",
    "                    avg_reward.append(sum(reward_history[-10:])/10.0)\n",
    "\n",
    "                    self.finish_path(episode_length)\n",
    "\n",
    "                    if episode > 300:\n",
    "                        solved = True\n",
    "\n",
    "                    print('episode: %.2f, total step: %.2f, last_episode length: %.2f, last_episode_reward: %.2f, '\n",
    "                          'loss: %.4f, lr: %.4f' % (episode, step, episode_length, total_episode_reward, self.loss,\n",
    "                                                    self.scheduler.get_lr()[0]))\n",
    "\n",
    "                    self.env.reset()\n",
    "\n",
    "                    break\n",
    "\n",
    "            if episode % self.update_freq == 0:\n",
    "                for _ in range(self.k_epoch):\n",
    "                    self.update_network()\n",
    "\n",
    "            if episode % self.plot_every == 0:\n",
    "                plot_graph(reward_history, avg_reward)\n",
    "\n",
    "        self.env.close()\n",
    "        return reward_history\n",
    "\n",
    "    def update_network(self):\n",
    "        # get ratio\n",
    "        pi = self.policy_network.pi(torch.FloatTensor(self.memory['state']).to(device))\n",
    "        new_probs_a = torch.gather(pi, 1, torch.tensor(self.memory['action']))\n",
    "        old_probs_a = torch.FloatTensor(self.memory['action_prob'])\n",
    "        ratio = torch.exp(torch.log(new_probs_a) - torch.log(old_probs_a))\n",
    "\n",
    "        # surrogate loss\n",
    "        surr1 = ratio * torch.FloatTensor(self.memory['advantage'])\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * torch.FloatTensor(self.memory['advantage'])\n",
    "        pred_v = self.policy_network.v(torch.FloatTensor(self.memory['state']).to(device))\n",
    "        v_loss = 0.5 * (pred_v - self.memory['td_target']).pow(2)  # Huber loss\n",
    "        entropy = torch.distributions.Categorical(pi).entropy()\n",
    "        entropy = torch.tensor([[e] for e in entropy])\n",
    "        self.loss = (-torch.min(surr1, surr2) + self.v_coef * v_loss - self.entropy_coef * entropy).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def add_memory(self, s, a, r, next_s, t, prob):\n",
    "        if self.memory['count'] < self.memory_size:\n",
    "            self.memory['count'] += 1\n",
    "        else:\n",
    "            self.memory['state'] = self.memory['state'][1:]\n",
    "            self.memory['action'] = self.memory['action'][1:]\n",
    "            self.memory['reward'] = self.memory['reward'][1:]\n",
    "            self.memory['next_state'] = self.memory['next_state'][1:]\n",
    "            self.memory['terminal'] = self.memory['terminal'][1:]\n",
    "            self.memory['action_prob'] = self.memory['action_prob'][1:]\n",
    "            self.memory['advantage'] = self.memory['advantage'][1:]\n",
    "            self.memory['td_target'] = self.memory['td_target'][1:]\n",
    "\n",
    "        self.memory['state'].append(s)\n",
    "        self.memory['action'].append([a])\n",
    "        self.memory['reward'].append([r])\n",
    "        self.memory['next_state'].append(next_s)\n",
    "        self.memory['terminal'].append([1 - t])\n",
    "        self.memory['action_prob'].append(prob)\n",
    "\n",
    "    def finish_path(self, length):\n",
    "        state = self.memory['state'][-length:]\n",
    "        reward = self.memory['reward'][-length:]\n",
    "        next_state = self.memory['next_state'][-length:]\n",
    "        terminal = self.memory['terminal'][-length:]\n",
    "\n",
    "        td_target = torch.FloatTensor(reward) + \\\n",
    "                    self.gamma * self.policy_network.v(torch.FloatTensor(next_state)) * torch.FloatTensor(terminal)\n",
    "        delta = td_target - self.policy_network.v(torch.FloatTensor(state))\n",
    "        delta = delta.detach().numpy()\n",
    "\n",
    "        # get advantage\n",
    "        advantages = []\n",
    "        adv = 0.0\n",
    "        for d in delta[::-1]:\n",
    "            adv = self.gamma * self.lmbda * adv + d[0]\n",
    "            advantages.append([adv])\n",
    "        advantages.reverse()\n",
    "\n",
    "        if self.memory['td_target'].shape == torch.Size([1, 0]):\n",
    "            self.memory['td_target'] = td_target.data\n",
    "        else:\n",
    "            self.memory['td_target'] = torch.cat((self.memory['td_target'], td_target.data), dim=0)\n",
    "        self.memory['advantage'] += advantages\n",
    "\n",
    "\n",
    "def plot_graph(reward_history, avg_reward):\n",
    "    df = pd.DataFrame({'x': range(len(reward_history)), 'Reward': reward_history, 'Average': avg_reward})\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    palette = plt.get_cmap('Set1')\n",
    "\n",
    "    plt.plot(np.array(df['x']), np.array(df['Reward']), marker='', color=palette(1), linewidth=0.8, alpha=0.9, label='Reward')\n",
    "\n",
    "    # plt.legend(loc='upper left')\n",
    "    plt.title(\"MountainCar\", fontsize=14)\n",
    "    plt.xlabel(\"episode\", fontsize=12)\n",
    "    plt.ylabel(\"score\", fontsize=12)\n",
    "\n",
    "    plt.savefig('score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    agent = Agent()\n",
    "    reward_output = agent.train()\n",
    "    reward_list.append(reward_output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
