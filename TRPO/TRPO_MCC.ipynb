{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "gamma = 0.99\n",
    "goal_score = 0\n",
    "log_interval = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_kl = 0.01\n",
    "\n",
    "def flat_grad(grads):\n",
    "    grad_flatten = []\n",
    "    for grad in grads:\n",
    "        grad_flatten.append(grad.view(-1))\n",
    "    grad_flatten = torch.cat(grad_flatten)\n",
    "    return grad_flatten\n",
    "\n",
    "def flat_hessian(hessians):\n",
    "    hessians_flatten = []\n",
    "    for hessian in hessians:\n",
    "        hessians_flatten.append(hessian.contiguous().view(-1))\n",
    "    hessians_flatten = torch.cat(hessians_flatten).data\n",
    "    return hessians_flatten\n",
    "\n",
    "def flat_params(model):\n",
    "    params = []\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "    params_flatten = torch.cat(params)\n",
    "    return params_flatten\n",
    "\n",
    "def update_model(model, new_params):\n",
    "    index = 0\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    for params in model.parameters():\n",
    "        params_length = len(params.view(-1))\n",
    "        new_param = new_params[index: index + params_length]\n",
    "        new_param = new_param.view(params.size())\n",
    "        params.data.copy_(new_param)\n",
    "        index += params_length\n",
    "\n",
    "def kl_divergence(policy, old_policy):\n",
    "    kl = old_policy * torch.log(old_policy / policy)\n",
    "\n",
    "    kl = kl.sum(1, keepdim=True)\n",
    "    return kl\n",
    "\n",
    "def fisher_vector_product(net, states, p, cg_damp=0.1):\n",
    "    policy = net(states)\n",
    "    old_policy = net(states).detach()\n",
    "    kl = kl_divergence(policy, old_policy)\n",
    "    kl = kl.mean()\n",
    "    kl_grad = torch.autograd.grad(kl, net.parameters(), create_graph=True) # create_graph is True if we need higher order derivative products\n",
    "    kl_grad = flat_grad(kl_grad)\n",
    "\n",
    "    kl_grad_p = (kl_grad * p.detach()).sum()\n",
    "    kl_hessian_p = torch.autograd.grad(kl_grad_p, net.parameters())\n",
    "    kl_hessian_p = flat_hessian(kl_hessian_p)\n",
    "\n",
    "    return kl_hessian_p + cg_damp * p.detach()\n",
    "\n",
    "\n",
    "def conjugate_gradient(net, states, loss_grad, n_step=10, residual_tol=1e-10):\n",
    "    x = torch.zeros(loss_grad.size())\n",
    "    r = loss_grad.clone()\n",
    "    p = loss_grad.clone()\n",
    "    r_dot_r = torch.dot(r, r)\n",
    "\n",
    "    for i in range(n_step):\n",
    "        A_dot_p = fisher_vector_product(net, states, p)\n",
    "        alpha = r_dot_r / torch.dot(p, A_dot_p)\n",
    "        x += alpha * p\n",
    "        r -= alpha * A_dot_p\n",
    "        new_r_dot_r = torch.dot(r,r)\n",
    "        betta = new_r_dot_r / r_dot_r\n",
    "        p = r + betta * p\n",
    "        r_dot_r = new_r_dot_r\n",
    "        if r_dot_r < residual_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "class TRPO(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(TRPO, self).__init__()\n",
    "        self.t = 0\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.fc_1 = nn.Linear(num_inputs, 512)\n",
    "        self.fc_2 = nn.Linear(512, num_outputs)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    def softmax(self, x, temp = 0.5):\n",
    "        # Subtract the max value from each element in x to prevent overflow\n",
    "        output = []\n",
    "        for i in x:\n",
    "            x_adjusted = i - np.max(x, axis=0)\n",
    "            e_x = np.exp(x_adjusted / temp)\n",
    "            output.append(e_x / e_x.sum(axis=0))\n",
    "        return output\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.relu(self.fc_1(input))\n",
    "        logits = self.fc_2(x) \n",
    "        logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "        policy = F.softmax(logits)\n",
    "        return policy\n",
    "\n",
    "    @classmethod\n",
    "    def train_model(cls, net, transitions):\n",
    "        states, actions, rewards, masks = transitions.state, transitions.action, transitions.reward, transitions.mask\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        masks = torch.Tensor(masks)\n",
    "\n",
    "        returns = torch.zeros_like(rewards)\n",
    "\n",
    "        running_return = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_return = rewards[t] + gamma * running_return * masks[t]\n",
    "            returns[t] = running_return\n",
    "\n",
    "        policy = net(states)\n",
    "        policy = policy.view(-1, net.num_outputs)\n",
    "        policy_action = (policy * actions.detach()).sum(dim=1)\n",
    "\n",
    "        old_policy = net(states).detach()\n",
    "        old_policy = old_policy.view(-1, net.num_outputs)\n",
    "        old_policy_action = (old_policy * actions.detach()).sum(dim=1)\n",
    "\n",
    "        surrogate_loss = ((policy_action / old_policy_action) * returns).mean()\n",
    "\n",
    "        surrogate_loss_grad = torch.autograd.grad(surrogate_loss, net.parameters())\n",
    "        surrogate_loss_grad = flat_grad(surrogate_loss_grad)\n",
    "\n",
    "        step_dir = conjugate_gradient(net, states, surrogate_loss_grad.data)\n",
    "\n",
    "        params = flat_params(net)\n",
    "        shs = (step_dir * fisher_vector_product(net, states, step_dir)).sum(0, keepdim=True)\n",
    "        step_size = torch.sqrt((2 * max_kl) / shs)[0]\n",
    "        full_step = step_size * step_dir\n",
    "\n",
    "        fraction = 1.0\n",
    "        for _ in range(10):\n",
    "            new_params = params + fraction * full_step\n",
    "            update_model(net, new_params)\n",
    "            policy = net(states)\n",
    "            policy = policy.view(-1, net.num_outputs)\n",
    "            policy_action = (policy * actions.detach()).sum(dim=1)\n",
    "            entropy = -(policy * policy.log()).sum(1, keepdim=True).mean()\n",
    "            beta = .9\n",
    "            surrogate_loss = ((policy_action / old_policy_action) * returns).mean() #- beta * entropy\n",
    "\n",
    "            kl = kl_divergence(policy, old_policy)\n",
    "            kl = kl.mean()\n",
    "\n",
    "            if kl < max_kl:\n",
    "                break\n",
    "            fraction = fraction * 0.5\n",
    "\n",
    "        return -surrogate_loss\n",
    "\n",
    "    def get_action(self, input):\n",
    "        policy = self.forward(input)\n",
    "        policy = policy[0].data.numpy()\n",
    "        # Check if policy contains NaN values\n",
    "        if np.isnan(policy).any():\n",
    "          print(\"Policy contains NaN values:\", policy)\n",
    "        action = np.random.choice(self.num_outputs, 1, p=policy)[0]\n",
    "        return action\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'next_state', 'action', 'reward', 'mask'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = deque()\n",
    "\n",
    "    def push(self, state, next_state, action, reward, mask):\n",
    "        self.memory.append(Transition(state, next_state, action, reward, mask))\n",
    "\n",
    "    def sample(self):\n",
    "        memory = self.memory\n",
    "        return Transition(*zip(*memory))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class discretize_action:\n",
    "    def __init__(self, env) -> None:\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def discretize_action_space(self, steps_per_dimension):\n",
    "        \"\"\"\n",
    "        Discretizes the action space.\n",
    "\n",
    "        Parameters:\n",
    "        - low: The lower bound of the action space.\n",
    "        - high: The higher bound of the action space.\n",
    "        - steps_per_dimension: Number of discrete steps per dimension.\n",
    "\n",
    "        Returns:\n",
    "        - A list of discretized actions.\n",
    "        \"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        # Generate a range of values for each dimension\n",
    "        ranges = [np.linspace(low[i], high[i], steps_per_dimension) for i in range(len(low))]\n",
    "        \n",
    "        # Create a meshgrid of all possible combinations\n",
    "        mesh = np.meshgrid(*ranges)\n",
    "        \n",
    "        # Reshape the meshgrid to create a list of actions\n",
    "        self.actions = np.vstack([m.flatten() for m in mesh]).T\n",
    "        \n",
    "        return self.actions\n",
    "    \n",
    "    def allocate_action(self, action):\n",
    "        distances = np.linalg.norm(self.actions - action, axis=1)\n",
    "        nearest_index = np.argmin(distances)\n",
    "        return self.actions[nearest_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "state size: 2\n",
      "action size: 3\n",
      "0 episode | score: -14.70\n",
      "Policy contains NaN values: [nan nan nan]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[73], line 46\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m max_step:\n\u001b[0;32m     44\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 46\u001b[0m     action \u001b[38;5;241m=\u001b[39m AD\u001b[38;5;241m.\u001b[39mallocate_action(\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m     next_state, reward, done, info, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# No need to wrap action in a list\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m#print(reward, steps)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[72], line 179\u001b[0m, in \u001b[0;36mTRPO.get_action\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(policy)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    178\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy contains NaN values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy)\n\u001b[1;32m--> 179\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mmtrand.pyx:954\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "def main():\n",
    "    env = gym.make('MountainCarContinuous-v0')  # Change the environment here\n",
    "    env._max_episode_steps = 200\n",
    "    max_step = env._max_episode_steps\n",
    "    ep = 2000\n",
    "    print(env._max_episode_steps)\n",
    "    #env.seed(500)\n",
    "    torch.manual_seed(500)\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    step_dim = 3\n",
    "    AD = discretize_action(env)\n",
    "    action_list = AD.discretize_action_space(step_dim)\n",
    "\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_actions = step_dim # Update the number of actions\n",
    "    print('state size:', num_inputs)\n",
    "    print('action size:', num_actions)\n",
    "\n",
    "    net = TRPO(num_inputs, num_actions)\n",
    "\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Create lists to hold scores and episodes\n",
    "    scores = []\n",
    "    episodes = []\n",
    "\n",
    "    for e in range(ep):\n",
    "        done = False\n",
    "        memory = Memory()\n",
    "        running_score = 0\n",
    "        steps = 0\n",
    "        score = 0\n",
    "        state, _ = env.reset()\n",
    "        state = torch.Tensor(state).to(device)\n",
    "        state = state.unsqueeze(0)\n",
    "\n",
    "        while not done and steps < max_step:\n",
    "            steps += 1\n",
    "\n",
    "            action = AD.allocate_action(net.get_action(state))\n",
    "            next_state, reward, done, info, _ = env.step(action)  # No need to wrap action in a list\n",
    "            #print(reward, steps)\n",
    "\n",
    "            next_state = torch.Tensor(next_state)\n",
    "            next_state = next_state.unsqueeze(0)\n",
    "\n",
    "            mask = 0 if done else 1\n",
    "            reward = reward if not done or score == 499 else -1\n",
    "\n",
    "            action_one_hot = torch.zeros(num_actions)  # Update the size of action_one_hot\n",
    "            action_one_hot[action] = 1\n",
    "            memory.push(state, next_state, action_one_hot, reward, mask)\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "        TRPO.train_model(net, memory.sample())\n",
    "\n",
    "        score = score #if score == 500.0 else score + 1\n",
    "        running_score = score #0.99 * running_score + 0.01 * score\n",
    "\n",
    "        # Append score and episode number to lists\n",
    "        scores.append(score)\n",
    "        episodes.append(e)\n",
    "\n",
    "        if e % log_interval == 0:\n",
    "            print('{} episode | score: {:.2f}'.format(e, running_score))\n",
    "\n",
    "        if running_score > goal_score:\n",
    "            break\n",
    "\n",
    "    # After all episodes, plot the scores\n",
    "    plt.plot(episodes, scores)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
